{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "84f51b2b",
   "metadata": {},
   "source": [
    "# Reconnaissance faciale via $k$ plus proches voisins ($k$-NN)\n",
    "\n",
    ">## Reconnaissance faciale : Multi-Layer Perceptron"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfb3d054",
   "metadata": {},
   "source": [
    "## Librairies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3b2f2e32",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import cv2\n",
    "from sklearn.neural_network import MLPClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dfbac10",
   "metadata": {},
   "source": [
    "## Algorithme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b8d4b3ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of visages matrix -->  (90, 50, 50, 3)\n"
     ]
    }
   ],
   "source": [
    "# Chargement des données\n",
    "with open('data/visages.pkl', 'rb') as fh:\n",
    "    visages = pickle.load(fh)\n",
    "\n",
    "with open('data/noms.pkl', 'rb') as fh:\n",
    "    noms = pickle.load(fh)\n",
    "\n",
    "print('Shape of visages matrix --> ', visages.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "279bc064",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 10.02129165\n",
      "Iteration 2, loss = 10.01926405\n",
      "Iteration 3, loss = 10.01723768\n",
      "Iteration 4, loss = 10.01521253\n",
      "Iteration 5, loss = 10.01318862\n",
      "Iteration 6, loss = 10.01116596\n",
      "Iteration 7, loss = 10.00914455\n",
      "Iteration 8, loss = 10.00712440\n",
      "Iteration 9, loss = 10.00510552\n",
      "Iteration 10, loss = 10.00308791\n",
      "Iteration 11, loss = 10.00107158\n",
      "Iteration 12, loss = 9.99905654\n",
      "Iteration 13, loss = 9.99704278\n",
      "Iteration 14, loss = 9.99503032\n",
      "Iteration 15, loss = 9.99301916\n",
      "Iteration 16, loss = 9.99100930\n",
      "Iteration 17, loss = 9.98900075\n",
      "Iteration 18, loss = 9.98699351\n",
      "Iteration 19, loss = 9.98498757\n",
      "Iteration 20, loss = 9.98298296\n",
      "Iteration 21, loss = 9.98097965\n",
      "Iteration 22, loss = 9.97897766\n",
      "Iteration 23, loss = 9.97697699\n",
      "Iteration 24, loss = 9.97497763\n",
      "Iteration 25, loss = 9.97297959\n",
      "Iteration 26, loss = 9.97098286\n",
      "Iteration 27, loss = 9.96898745\n",
      "Iteration 28, loss = 9.96699335\n",
      "Iteration 29, loss = 9.96500056\n",
      "Iteration 30, loss = 9.96300908\n",
      "Iteration 31, loss = 9.96101890\n",
      "Iteration 32, loss = 9.95903002\n",
      "Iteration 33, loss = 9.95704244\n",
      "Iteration 34, loss = 9.95505616\n",
      "Iteration 35, loss = 9.95307116\n",
      "Iteration 36, loss = 9.95108744\n",
      "Iteration 37, loss = 9.94910501\n",
      "Iteration 38, loss = 9.94712384\n",
      "Iteration 39, loss = 9.94514395\n",
      "Iteration 40, loss = 9.94316531\n",
      "Iteration 41, loss = 9.94118793\n",
      "Iteration 42, loss = 9.93921179\n",
      "Iteration 43, loss = 9.93723690\n",
      "Iteration 44, loss = 9.93526323\n",
      "Iteration 45, loss = 9.93329079\n",
      "Iteration 46, loss = 9.93131957\n",
      "Iteration 47, loss = 9.92934956\n",
      "Iteration 48, loss = 9.92738074\n",
      "Iteration 49, loss = 9.92541312\n",
      "Iteration 50, loss = 9.92344667\n",
      "Iteration 51, loss = 9.92148157\n",
      "Iteration 52, loss = 9.91951219\n",
      "Iteration 53, loss = 9.91752986\n",
      "Iteration 54, loss = 9.91554845\n",
      "Iteration 55, loss = 9.91356798\n",
      "Iteration 56, loss = 9.91158846\n",
      "Iteration 57, loss = 9.90960990\n",
      "Iteration 58, loss = 9.90763233\n",
      "Iteration 59, loss = 9.90565574\n",
      "Iteration 60, loss = 9.90368015\n",
      "Iteration 61, loss = 9.90170556\n",
      "Iteration 62, loss = 9.89973196\n",
      "Iteration 63, loss = 9.89775937\n",
      "Iteration 64, loss = 9.89578778\n",
      "Iteration 65, loss = 9.89381719\n",
      "Iteration 66, loss = 9.89184759\n",
      "Iteration 67, loss = 9.88987898\n",
      "Iteration 68, loss = 9.88791136\n",
      "Iteration 69, loss = 9.88594471\n",
      "Iteration 70, loss = 9.88397904\n",
      "Iteration 71, loss = 9.88201433\n",
      "Iteration 72, loss = 9.88005057\n",
      "Iteration 73, loss = 9.87808777\n",
      "Iteration 74, loss = 9.87612590\n",
      "Iteration 75, loss = 9.87416496\n",
      "Iteration 76, loss = 9.87220493\n",
      "Iteration 77, loss = 9.87024582\n",
      "Iteration 78, loss = 9.86828761\n",
      "Iteration 79, loss = 9.86633028\n",
      "Iteration 80, loss = 9.86437383\n",
      "Iteration 81, loss = 9.86241825\n",
      "Iteration 82, loss = 9.86046352\n",
      "Iteration 83, loss = 9.85850964\n",
      "Iteration 84, loss = 9.85655659\n",
      "Iteration 85, loss = 9.85460437\n",
      "Iteration 86, loss = 9.85265295\n",
      "Iteration 87, loss = 9.85070234\n",
      "Iteration 88, loss = 9.84875252\n",
      "Iteration 89, loss = 9.84680347\n",
      "Iteration 90, loss = 9.84485519\n",
      "Iteration 91, loss = 9.84290766\n",
      "Iteration 92, loss = 9.84096088\n",
      "Iteration 93, loss = 9.83901483\n",
      "Iteration 94, loss = 9.83706951\n",
      "Iteration 95, loss = 9.83512489\n",
      "Iteration 96, loss = 9.83318098\n",
      "Iteration 97, loss = 9.83123775\n",
      "Iteration 98, loss = 9.82929520\n",
      "Iteration 99, loss = 9.82735332\n",
      "Iteration 100, loss = 9.82541210\n",
      "Iteration 101, loss = 9.82347153\n",
      "Iteration 102, loss = 9.82153159\n",
      "Iteration 103, loss = 9.81959227\n",
      "Iteration 104, loss = 9.81765357\n",
      "Iteration 105, loss = 9.81571548\n",
      "Iteration 106, loss = 9.81377798\n",
      "Iteration 107, loss = 9.81185519\n",
      "Iteration 108, loss = 9.80993480\n",
      "Iteration 109, loss = 9.80801515\n",
      "Iteration 110, loss = 9.80609622\n",
      "Iteration 111, loss = 9.80417798\n",
      "Iteration 112, loss = 9.80226040\n",
      "Iteration 113, loss = 9.80034346\n",
      "Iteration 114, loss = 9.79842714\n",
      "Iteration 115, loss = 9.79651141\n",
      "Iteration 116, loss = 9.79459627\n",
      "Iteration 117, loss = 9.79268168\n",
      "Iteration 118, loss = 9.79076764\n",
      "Iteration 119, loss = 9.78885412\n",
      "Iteration 120, loss = 9.78694112\n",
      "Iteration 121, loss = 9.78502861\n",
      "Iteration 122, loss = 9.78311659\n",
      "Iteration 123, loss = 9.78120504\n",
      "Iteration 124, loss = 9.77929394\n",
      "Iteration 125, loss = 9.77738329\n",
      "Iteration 126, loss = 9.77547308\n",
      "Iteration 127, loss = 9.77356329\n",
      "Iteration 128, loss = 9.77165392\n",
      "Iteration 129, loss = 9.76974494\n",
      "Iteration 130, loss = 9.76783637\n",
      "Iteration 131, loss = 9.76592817\n",
      "Iteration 132, loss = 9.76402036\n",
      "Iteration 133, loss = 9.76211291\n",
      "Iteration 134, loss = 9.76020582\n",
      "Iteration 135, loss = 9.75829908\n",
      "Iteration 136, loss = 9.75639268\n",
      "Iteration 137, loss = 9.75448662\n",
      "Iteration 138, loss = 9.75258088\n",
      "Iteration 139, loss = 9.75067547\n",
      "Iteration 140, loss = 9.74877037\n",
      "Iteration 141, loss = 9.74686558\n",
      "Iteration 142, loss = 9.74496109\n",
      "Iteration 143, loss = 9.74305690\n",
      "Iteration 144, loss = 9.74115299\n",
      "Iteration 145, loss = 9.73924936\n",
      "Iteration 146, loss = 9.73734601\n",
      "Iteration 147, loss = 9.73544293\n",
      "Iteration 148, loss = 9.73354012\n",
      "Iteration 149, loss = 9.73163756\n",
      "Iteration 150, loss = 9.72973526\n",
      "Iteration 151, loss = 9.72783320\n",
      "Iteration 152, loss = 9.72593139\n",
      "Iteration 153, loss = 9.72402982\n",
      "Iteration 154, loss = 9.72212848\n",
      "Iteration 155, loss = 9.72022736\n",
      "Iteration 156, loss = 9.71832648\n",
      "Iteration 157, loss = 9.71642581\n",
      "Iteration 158, loss = 9.71452535\n",
      "Iteration 159, loss = 9.71262511\n",
      "Iteration 160, loss = 9.71072507\n",
      "Iteration 161, loss = 9.70882523\n",
      "Iteration 162, loss = 9.70692559\n",
      "Iteration 163, loss = 9.70502614\n",
      "Iteration 164, loss = 9.70312689\n",
      "Iteration 165, loss = 9.70122782\n",
      "Iteration 166, loss = 9.69932893\n",
      "Iteration 167, loss = 9.69743022\n",
      "Iteration 168, loss = 9.69553168\n",
      "Iteration 169, loss = 9.69363331\n",
      "Iteration 170, loss = 9.69173511\n",
      "Iteration 171, loss = 9.68983708\n",
      "Iteration 172, loss = 9.68793921\n",
      "Iteration 173, loss = 9.68604149\n",
      "Iteration 174, loss = 9.68414393\n",
      "Iteration 175, loss = 9.68224652\n",
      "Iteration 176, loss = 9.68034926\n",
      "Iteration 177, loss = 9.67845215\n",
      "Iteration 178, loss = 9.67655518\n",
      "Iteration 179, loss = 9.67465835\n",
      "Iteration 180, loss = 9.67276165\n",
      "Iteration 181, loss = 9.67086509\n",
      "Iteration 182, loss = 9.66896867\n",
      "Iteration 183, loss = 9.66707237\n",
      "Iteration 184, loss = 9.66517620\n",
      "Iteration 185, loss = 9.66328016\n",
      "Iteration 186, loss = 9.66138423\n",
      "Iteration 187, loss = 9.65948843\n",
      "Iteration 188, loss = 9.65759275\n",
      "Iteration 189, loss = 9.65569718\n",
      "Iteration 190, loss = 9.65380173\n",
      "Iteration 191, loss = 9.65190639\n",
      "Iteration 192, loss = 9.65001116\n",
      "Iteration 193, loss = 9.64811603\n",
      "Iteration 194, loss = 9.64622102\n",
      "Iteration 195, loss = 9.64432610\n",
      "Iteration 196, loss = 9.64243130\n",
      "Iteration 197, loss = 9.64053659\n",
      "Iteration 198, loss = 9.63864198\n",
      "Iteration 199, loss = 9.63674747\n",
      "Iteration 200, loss = 9.63485306\n",
      "Iteration 201, loss = 9.63295874\n",
      "Iteration 202, loss = 9.63106451\n",
      "Iteration 203, loss = 9.62917038\n",
      "Iteration 204, loss = 9.62727634\n",
      "Iteration 205, loss = 9.62538239\n",
      "Iteration 206, loss = 9.62348853\n",
      "Iteration 207, loss = 9.62159476\n",
      "Iteration 208, loss = 9.61970107\n",
      "Iteration 209, loss = 9.61780747\n",
      "Iteration 210, loss = 9.61591395\n",
      "Iteration 211, loss = 9.61402052\n",
      "Iteration 212, loss = 9.61212717\n",
      "Iteration 213, loss = 9.61023390\n",
      "Iteration 214, loss = 9.60834071\n",
      "Iteration 215, loss = 9.60644760\n",
      "Iteration 216, loss = 9.60455458\n",
      "Iteration 217, loss = 9.60266163\n",
      "Iteration 218, loss = 9.60076876\n",
      "Iteration 219, loss = 9.59887596\n",
      "Iteration 220, loss = 9.59698325\n",
      "Iteration 221, loss = 9.59509061\n",
      "Iteration 222, loss = 9.59319805\n",
      "Iteration 223, loss = 9.59130556\n",
      "Iteration 224, loss = 9.58941315\n",
      "Iteration 225, loss = 9.58752082\n",
      "Iteration 226, loss = 9.58562856\n",
      "Iteration 227, loss = 9.58373637\n",
      "Iteration 228, loss = 9.58184426\n",
      "Iteration 229, loss = 9.57995223\n",
      "Iteration 230, loss = 9.57806027\n",
      "Iteration 231, loss = 9.57616838\n",
      "Iteration 232, loss = 9.57427657\n",
      "Iteration 233, loss = 9.57238483\n",
      "Iteration 234, loss = 9.57049317\n",
      "Iteration 235, loss = 9.56860158\n",
      "Iteration 236, loss = 9.56671007\n",
      "Iteration 237, loss = 9.56481863\n",
      "Iteration 238, loss = 9.56292727\n",
      "Iteration 239, loss = 9.56103598\n",
      "Iteration 240, loss = 9.55914477\n",
      "Iteration 241, loss = 9.55725364\n",
      "Iteration 242, loss = 9.55536258\n",
      "Iteration 243, loss = 9.55347160\n",
      "Iteration 244, loss = 9.55157963\n",
      "Iteration 245, loss = 9.54968833\n",
      "Iteration 246, loss = 9.54779959\n",
      "Iteration 247, loss = 9.54591098\n",
      "Iteration 248, loss = 9.54402251\n",
      "Iteration 249, loss = 9.54213416\n",
      "Iteration 250, loss = 9.54024593\n",
      "Iteration 251, loss = 9.53835782\n",
      "Iteration 252, loss = 9.53646983\n",
      "Iteration 253, loss = 9.53458194\n",
      "Iteration 254, loss = 9.53269417\n",
      "Iteration 255, loss = 9.53080651\n",
      "Iteration 256, loss = 9.52891896\n",
      "Iteration 257, loss = 9.52703151\n",
      "Iteration 258, loss = 9.52514416\n",
      "Iteration 259, loss = 9.52325692\n",
      "Iteration 260, loss = 9.52136979\n",
      "Iteration 261, loss = 9.51948276\n",
      "Iteration 262, loss = 9.51759583\n",
      "Iteration 263, loss = 9.51570901\n",
      "Iteration 264, loss = 9.51382229\n",
      "Iteration 265, loss = 9.51193567\n",
      "Iteration 266, loss = 9.51004916\n",
      "Iteration 267, loss = 9.50816276\n",
      "Iteration 268, loss = 9.50627646\n",
      "Iteration 269, loss = 9.50439028\n",
      "Iteration 270, loss = 9.50250420\n",
      "Iteration 271, loss = 9.50061823\n",
      "Iteration 272, loss = 9.49873237\n",
      "Iteration 273, loss = 9.49684663\n",
      "Iteration 274, loss = 9.49496100\n",
      "Iteration 275, loss = 9.49307548\n",
      "Iteration 276, loss = 9.49119009\n",
      "Iteration 277, loss = 9.48930481\n",
      "Iteration 278, loss = 9.48741965\n",
      "Iteration 279, loss = 9.48553462\n",
      "Iteration 280, loss = 9.48364971\n",
      "Iteration 281, loss = 9.48176493\n",
      "Iteration 282, loss = 9.47988028\n",
      "Iteration 283, loss = 9.47799576\n",
      "Iteration 284, loss = 9.47611138\n",
      "Iteration 285, loss = 9.47422713\n",
      "Iteration 286, loss = 9.47234302\n",
      "Iteration 287, loss = 9.47045905\n",
      "Iteration 288, loss = 9.46857522\n",
      "Iteration 289, loss = 9.46669154\n",
      "Iteration 290, loss = 9.46480801\n",
      "Iteration 291, loss = 9.46292463\n",
      "Iteration 292, loss = 9.46104141\n",
      "Iteration 293, loss = 9.45915834\n",
      "Iteration 294, loss = 9.45727544\n",
      "Iteration 295, loss = 9.45539269\n",
      "Iteration 296, loss = 9.45351011\n",
      "Iteration 297, loss = 9.45162771\n",
      "Iteration 298, loss = 9.44974547\n",
      "Iteration 299, loss = 9.44786341\n",
      "Iteration 300, loss = 9.44598153\n",
      "Iteration 301, loss = 9.44409983\n",
      "Iteration 302, loss = 9.44221832\n",
      "Iteration 303, loss = 9.44033700\n",
      "Iteration 304, loss = 9.43845587\n",
      "Iteration 305, loss = 9.43657494\n",
      "Iteration 306, loss = 9.43469436\n",
      "Iteration 307, loss = 9.43281410\n",
      "Iteration 308, loss = 9.43093417\n",
      "Iteration 309, loss = 9.42905456\n",
      "Iteration 310, loss = 9.42717525\n",
      "Iteration 311, loss = 9.42529625\n",
      "Iteration 312, loss = 9.42341755\n",
      "Iteration 313, loss = 9.42153914\n",
      "Iteration 314, loss = 9.41966103\n",
      "Iteration 315, loss = 9.41778321\n",
      "Iteration 316, loss = 9.41569622\n",
      "Iteration 317, loss = 9.41358178\n",
      "Iteration 318, loss = 9.41146762\n",
      "Iteration 319, loss = 9.40935375\n",
      "Iteration 320, loss = 9.40724018\n",
      "Iteration 321, loss = 9.40512691\n",
      "Iteration 322, loss = 9.40301393\n",
      "Iteration 323, loss = 9.40090191\n",
      "Iteration 324, loss = 9.39879239\n",
      "Iteration 325, loss = 9.39668326\n",
      "Iteration 326, loss = 9.39457452\n",
      "Iteration 327, loss = 9.39246617\n",
      "Iteration 328, loss = 9.39035820\n",
      "Iteration 329, loss = 9.38825062\n",
      "Iteration 330, loss = 9.38614341\n",
      "Iteration 331, loss = 9.38403658\n",
      "Iteration 332, loss = 9.38193014\n",
      "Iteration 333, loss = 9.37982407\n",
      "Iteration 334, loss = 9.37771838\n",
      "Iteration 335, loss = 9.37561308\n",
      "Iteration 336, loss = 9.37350816\n",
      "Iteration 337, loss = 9.37140363\n",
      "Iteration 338, loss = 9.36929950\n",
      "Iteration 339, loss = 9.36719576\n",
      "Iteration 340, loss = 9.36509243\n",
      "Iteration 341, loss = 9.36298950\n",
      "Iteration 342, loss = 9.36088698\n",
      "Iteration 343, loss = 9.35878488\n",
      "Iteration 344, loss = 9.35668320\n",
      "Iteration 345, loss = 9.35458196\n",
      "Iteration 346, loss = 9.35248114\n",
      "Iteration 347, loss = 9.35038077\n",
      "Iteration 348, loss = 9.34828085\n",
      "Iteration 349, loss = 9.34618138\n",
      "Iteration 350, loss = 9.34408238\n",
      "Iteration 351, loss = 9.34198385\n",
      "Iteration 352, loss = 9.33988580\n",
      "Iteration 353, loss = 9.33766863\n",
      "Iteration 354, loss = 9.33533451\n",
      "Iteration 355, loss = 9.33300091\n",
      "Iteration 356, loss = 9.33066783\n",
      "Iteration 357, loss = 9.32833527\n",
      "Iteration 358, loss = 9.32600325\n",
      "Iteration 359, loss = 9.32367178\n",
      "Iteration 360, loss = 9.32134086\n",
      "Iteration 361, loss = 9.31901051\n",
      "Iteration 362, loss = 9.31668073\n",
      "Iteration 363, loss = 9.31435153\n",
      "Iteration 364, loss = 9.31202293\n",
      "Iteration 365, loss = 9.30969493\n",
      "Iteration 366, loss = 9.30736755\n",
      "Iteration 367, loss = 9.30504080\n",
      "Iteration 368, loss = 9.30271468\n",
      "Iteration 369, loss = 9.30038921\n",
      "Iteration 370, loss = 9.29806439\n",
      "Iteration 371, loss = 9.29574025\n",
      "Iteration 372, loss = 9.29341678\n",
      "Iteration 373, loss = 9.29109401\n",
      "Iteration 374, loss = 9.28877194\n",
      "Iteration 375, loss = 9.28645058\n",
      "Iteration 376, loss = 9.28412995\n",
      "Iteration 377, loss = 9.28181006\n",
      "Iteration 378, loss = 9.27949092\n",
      "Iteration 379, loss = 9.27717255\n",
      "Iteration 380, loss = 9.27485494\n",
      "Iteration 381, loss = 9.27253813\n",
      "Iteration 382, loss = 9.27022212\n",
      "Iteration 383, loss = 9.26790692\n",
      "Iteration 384, loss = 9.26559255\n",
      "Iteration 385, loss = 9.26327901\n",
      "Iteration 386, loss = 9.26096633\n",
      "Iteration 387, loss = 9.25865452\n",
      "Iteration 388, loss = 9.25634358\n",
      "Iteration 389, loss = 9.25403354\n",
      "Iteration 390, loss = 9.25172440\n",
      "Iteration 391, loss = 9.24941619\n",
      "Iteration 392, loss = 9.24710890\n",
      "Iteration 393, loss = 9.24480257\n",
      "Iteration 394, loss = 9.24249720\n",
      "Iteration 395, loss = 9.24019281\n",
      "Iteration 396, loss = 9.23788941\n",
      "Iteration 397, loss = 9.23558701\n",
      "Iteration 398, loss = 9.23328563\n",
      "Iteration 399, loss = 9.23098529\n",
      "Iteration 400, loss = 9.22868600\n",
      "Iteration 401, loss = 9.22638778\n",
      "Iteration 402, loss = 9.22409064\n",
      "Iteration 403, loss = 9.22179459\n",
      "Iteration 404, loss = 9.21949965\n",
      "Iteration 405, loss = 9.21720584\n",
      "Iteration 406, loss = 9.21491317\n",
      "Iteration 407, loss = 9.21262165\n",
      "Iteration 408, loss = 9.21033131\n",
      "Iteration 409, loss = 9.20804216\n",
      "Iteration 410, loss = 9.20575421\n",
      "Iteration 411, loss = 9.20346748\n",
      "Iteration 412, loss = 9.20118199\n",
      "Iteration 413, loss = 9.19889775\n",
      "Iteration 414, loss = 9.19661478\n",
      "Iteration 415, loss = 9.19433309\n",
      "Iteration 416, loss = 9.19205270\n",
      "Iteration 417, loss = 9.18977363\n",
      "Iteration 418, loss = 9.18749588\n",
      "Iteration 419, loss = 9.18521949\n",
      "Iteration 420, loss = 9.18294447\n",
      "Iteration 421, loss = 9.18067082\n",
      "Iteration 422, loss = 9.17839857\n",
      "Iteration 423, loss = 9.17612774\n",
      "Iteration 424, loss = 9.17385833\n",
      "Iteration 425, loss = 9.17159038\n",
      "Iteration 426, loss = 9.16932388\n",
      "Iteration 427, loss = 9.16705887\n",
      "Iteration 428, loss = 9.16479535\n",
      "Iteration 429, loss = 9.16253334\n",
      "Iteration 430, loss = 9.16027286\n",
      "Iteration 431, loss = 9.15801393\n",
      "Iteration 432, loss = 9.15575655\n",
      "Iteration 433, loss = 9.15350076\n",
      "Iteration 434, loss = 9.15124656\n",
      "Iteration 435, loss = 9.14899397\n",
      "Iteration 436, loss = 9.14674300\n",
      "Iteration 437, loss = 9.14449367\n",
      "Iteration 438, loss = 9.14224601\n",
      "Iteration 439, loss = 9.14000002\n",
      "Iteration 440, loss = 9.13775571\n",
      "Iteration 441, loss = 9.13551311\n",
      "Iteration 442, loss = 9.13327224\n",
      "Iteration 443, loss = 9.13103310\n",
      "Iteration 444, loss = 9.12879571\n",
      "Iteration 445, loss = 9.12656009\n",
      "Iteration 446, loss = 9.12432625\n",
      "Iteration 447, loss = 9.12209422\n",
      "Iteration 448, loss = 9.11986399\n",
      "Iteration 449, loss = 9.11763560\n",
      "Iteration 450, loss = 9.11540904\n",
      "Iteration 451, loss = 9.11318435\n",
      "Iteration 452, loss = 9.11096152\n",
      "Iteration 453, loss = 9.10874059\n",
      "Iteration 454, loss = 9.10652155\n",
      "Iteration 455, loss = 9.10430443\n",
      "Iteration 456, loss = 9.10208924\n",
      "Iteration 457, loss = 9.09987599\n",
      "Iteration 458, loss = 9.09766470\n",
      "Iteration 459, loss = 9.09545538\n",
      "Iteration 460, loss = 9.09324805\n",
      "Iteration 461, loss = 9.09104271\n",
      "Iteration 462, loss = 9.08883938\n",
      "Iteration 463, loss = 9.08663807\n",
      "Iteration 464, loss = 9.08443880\n",
      "Iteration 465, loss = 9.08224158\n",
      "Iteration 466, loss = 9.08004642\n",
      "Iteration 467, loss = 9.07785333\n",
      "Iteration 468, loss = 9.07566232\n",
      "Iteration 469, loss = 9.07347341\n",
      "Iteration 470, loss = 9.07128660\n",
      "Iteration 471, loss = 9.06910192\n",
      "Iteration 472, loss = 9.06691936\n",
      "Iteration 473, loss = 9.06473895\n",
      "Iteration 474, loss = 9.06256069\n",
      "Iteration 475, loss = 9.06038458\n",
      "Iteration 476, loss = 9.05821065\n",
      "Iteration 477, loss = 9.05603891\n",
      "Iteration 478, loss = 9.05386935\n",
      "Iteration 479, loss = 9.05170200\n",
      "Iteration 480, loss = 9.04953686\n",
      "Iteration 481, loss = 9.04737393\n",
      "Iteration 482, loss = 9.04521324\n",
      "Iteration 483, loss = 9.04305479\n",
      "Iteration 484, loss = 9.04089858\n",
      "Iteration 485, loss = 9.03874463\n",
      "Iteration 486, loss = 9.03659294\n",
      "Iteration 487, loss = 9.03444353\n",
      "Iteration 488, loss = 9.03229639\n",
      "Iteration 489, loss = 9.03015155\n",
      "Iteration 490, loss = 9.02800899\n",
      "Iteration 491, loss = 9.02586874\n",
      "Iteration 492, loss = 9.02373080\n",
      "Iteration 493, loss = 9.02159518\n",
      "Iteration 494, loss = 9.01946187\n",
      "Iteration 495, loss = 9.01733090\n",
      "Iteration 496, loss = 9.01520227\n",
      "Iteration 497, loss = 9.01307597\n",
      "Iteration 498, loss = 9.01095203\n",
      "Iteration 499, loss = 9.00883044\n",
      "Iteration 500, loss = 9.00671121\n",
      "Iteration 501, loss = 9.00459434\n",
      "Iteration 502, loss = 9.00247984\n",
      "Iteration 503, loss = 9.00036773\n",
      "Iteration 504, loss = 8.99825799\n",
      "Iteration 505, loss = 8.99615064\n",
      "Iteration 506, loss = 8.99404568\n",
      "Iteration 507, loss = 8.99194312\n",
      "Iteration 508, loss = 8.98984295\n",
      "Iteration 509, loss = 8.98774519\n",
      "Iteration 510, loss = 8.98564985\n",
      "Iteration 511, loss = 8.98355691\n",
      "Iteration 512, loss = 8.98146639\n",
      "Iteration 513, loss = 8.97937830\n",
      "Iteration 514, loss = 8.97729263\n",
      "Iteration 515, loss = 8.97520938\n",
      "Iteration 516, loss = 8.97312858\n",
      "Iteration 517, loss = 8.97105021\n",
      "Iteration 518, loss = 8.96897427\n",
      "Iteration 519, loss = 8.96690079\n",
      "Iteration 520, loss = 8.96482821\n",
      "Iteration 521, loss = 8.96275673\n",
      "Iteration 522, loss = 8.96068491\n",
      "Iteration 523, loss = 8.95842021\n",
      "Iteration 524, loss = 8.95615706\n",
      "Iteration 525, loss = 8.95389557\n",
      "Iteration 526, loss = 8.95163583\n",
      "Iteration 527, loss = 8.94937793\n",
      "Iteration 528, loss = 8.94712197\n",
      "Iteration 529, loss = 8.94486802\n",
      "Iteration 530, loss = 8.94261613\n",
      "Iteration 531, loss = 8.94036638\n",
      "Iteration 532, loss = 8.93811881\n",
      "Iteration 533, loss = 8.93587347\n",
      "Iteration 534, loss = 8.93363042\n",
      "Iteration 535, loss = 8.93138968\n",
      "Iteration 536, loss = 8.92915129\n",
      "Iteration 537, loss = 8.92691529\n",
      "Iteration 538, loss = 8.92468171\n",
      "Iteration 539, loss = 8.92245057\n",
      "Iteration 540, loss = 8.92022190\n",
      "Iteration 541, loss = 8.91799571\n",
      "Iteration 542, loss = 8.91577203\n",
      "Iteration 543, loss = 8.91355088\n",
      "Iteration 544, loss = 8.91133227\n",
      "Iteration 545, loss = 8.90911622\n",
      "Iteration 546, loss = 8.90690274\n",
      "Iteration 547, loss = 8.90469185\n",
      "Iteration 548, loss = 8.90248355\n",
      "Iteration 549, loss = 8.90027786\n",
      "Iteration 550, loss = 8.89807479\n",
      "Iteration 551, loss = 8.89587435\n",
      "Iteration 552, loss = 8.89367654\n",
      "Iteration 553, loss = 8.89148137\n",
      "Iteration 554, loss = 8.88928886\n",
      "Iteration 555, loss = 8.88709900\n",
      "Iteration 556, loss = 8.88491181\n",
      "Iteration 557, loss = 8.88272729\n",
      "Iteration 558, loss = 8.88054545\n",
      "Iteration 559, loss = 8.87836629\n",
      "Iteration 560, loss = 8.87618982\n",
      "Iteration 561, loss = 8.87401605\n",
      "Iteration 562, loss = 8.87184498\n",
      "Iteration 563, loss = 8.86967661\n",
      "Iteration 564, loss = 8.86751095\n",
      "Iteration 565, loss = 8.86534801\n",
      "Iteration 566, loss = 8.86318779\n",
      "Iteration 567, loss = 8.86103029\n",
      "Iteration 568, loss = 8.85887552\n",
      "Iteration 569, loss = 8.85672349\n",
      "Iteration 570, loss = 8.85457498\n",
      "Iteration 571, loss = 8.85242994\n",
      "Iteration 572, loss = 8.85028828\n",
      "Iteration 573, loss = 8.84814995\n",
      "Iteration 574, loss = 8.84601490\n",
      "Iteration 575, loss = 8.84388416\n",
      "Iteration 576, loss = 8.84175815\n",
      "Iteration 577, loss = 8.83963535\n",
      "Iteration 578, loss = 8.83751572\n",
      "Iteration 579, loss = 8.83539922\n",
      "Iteration 580, loss = 8.83328582\n",
      "Iteration 581, loss = 8.83117550\n",
      "Iteration 582, loss = 8.82906822\n",
      "Iteration 583, loss = 8.82696396\n",
      "Iteration 584, loss = 8.82486270\n",
      "Iteration 585, loss = 8.82276444\n",
      "Iteration 586, loss = 8.82066914\n",
      "Iteration 587, loss = 8.81857680\n",
      "Iteration 588, loss = 8.81648740\n",
      "Iteration 589, loss = 8.81440095\n",
      "Iteration 590, loss = 8.81231741\n",
      "Iteration 591, loss = 8.81023680\n",
      "Iteration 592, loss = 8.80815911\n",
      "Iteration 593, loss = 8.80608432\n",
      "Iteration 594, loss = 8.80401244\n",
      "Iteration 595, loss = 8.80194347\n",
      "Iteration 596, loss = 8.79987739\n",
      "Iteration 597, loss = 8.79781421\n",
      "Iteration 598, loss = 8.79575393\n",
      "Iteration 599, loss = 8.79369654\n",
      "Iteration 600, loss = 8.79164206\n",
      "Iteration 601, loss = 8.78959047\n",
      "Iteration 602, loss = 8.78754177\n",
      "Iteration 603, loss = 8.78549598\n",
      "Iteration 604, loss = 8.78345309\n",
      "Iteration 605, loss = 8.78141310\n",
      "Iteration 606, loss = 8.77937601\n",
      "Iteration 607, loss = 8.77734183\n",
      "Iteration 608, loss = 8.77531055\n",
      "Iteration 609, loss = 8.77328218\n",
      "Iteration 610, loss = 8.77125673\n",
      "Iteration 611, loss = 8.76923419\n",
      "Iteration 612, loss = 8.76721457\n",
      "Iteration 613, loss = 8.76519787\n",
      "Iteration 614, loss = 8.76318408\n",
      "Iteration 615, loss = 8.76117322\n",
      "Iteration 616, loss = 8.75916529\n",
      "Iteration 617, loss = 8.75716029\n",
      "Iteration 618, loss = 8.75515822\n",
      "Iteration 619, loss = 8.75315908\n",
      "Iteration 620, loss = 8.75116288\n",
      "Iteration 621, loss = 8.74916961\n",
      "Iteration 622, loss = 8.74717929\n",
      "Iteration 623, loss = 8.74519190\n",
      "Iteration 624, loss = 8.74320746\n",
      "Iteration 625, loss = 8.74122597\n",
      "Iteration 626, loss = 8.73924742\n",
      "Iteration 627, loss = 8.73727182\n",
      "Iteration 628, loss = 8.73513793\n",
      "Iteration 629, loss = 8.73284641\n",
      "Iteration 630, loss = 8.73045266\n",
      "Iteration 631, loss = 8.72806195\n",
      "Iteration 632, loss = 8.72567428\n",
      "Iteration 633, loss = 8.72328967\n",
      "Iteration 634, loss = 8.72090810\n",
      "Iteration 635, loss = 8.71852958\n",
      "Iteration 636, loss = 8.71615411\n",
      "Iteration 637, loss = 8.71378150\n",
      "Iteration 638, loss = 8.71141079\n",
      "Iteration 639, loss = 8.70904307\n",
      "Iteration 640, loss = 8.70667835\n",
      "Iteration 641, loss = 8.70431663\n",
      "Iteration 642, loss = 8.70195792\n",
      "Iteration 643, loss = 8.69960223\n",
      "Iteration 644, loss = 8.69724955\n",
      "Iteration 645, loss = 8.69489989\n",
      "Iteration 646, loss = 8.69255326\n",
      "Iteration 647, loss = 8.69020964\n",
      "Iteration 648, loss = 8.68786905\n",
      "Iteration 649, loss = 8.68553148\n",
      "Iteration 650, loss = 8.68319693\n",
      "Iteration 651, loss = 8.68086540\n",
      "Iteration 652, loss = 8.67853689\n",
      "Iteration 653, loss = 8.67621028\n",
      "Iteration 654, loss = 8.67388560\n",
      "Iteration 655, loss = 8.67156388\n",
      "Iteration 656, loss = 8.66924515\n",
      "Iteration 657, loss = 8.66692939\n",
      "Iteration 658, loss = 8.66461662\n",
      "Iteration 659, loss = 8.66230683\n",
      "Iteration 660, loss = 8.66000002\n",
      "Iteration 661, loss = 8.65769620\n",
      "Iteration 662, loss = 8.65539536\n",
      "Iteration 663, loss = 8.65309749\n",
      "Iteration 664, loss = 8.65080260\n",
      "Iteration 665, loss = 8.64851068\n",
      "Iteration 666, loss = 8.64622172\n",
      "Iteration 667, loss = 8.64393572\n",
      "Iteration 668, loss = 8.64165267\n",
      "Iteration 669, loss = 8.63937256\n",
      "Iteration 670, loss = 8.63709539\n",
      "Iteration 671, loss = 8.63482114\n",
      "Iteration 672, loss = 8.63254981\n",
      "Iteration 673, loss = 8.63028138\n",
      "Iteration 674, loss = 8.62801586\n",
      "Iteration 675, loss = 8.62575322\n",
      "Iteration 676, loss = 8.62349345\n",
      "Iteration 677, loss = 8.62123655\n",
      "Iteration 678, loss = 8.61898249\n",
      "Iteration 679, loss = 8.61673128\n",
      "Iteration 680, loss = 8.61448289\n",
      "Iteration 681, loss = 8.61223731\n",
      "Iteration 682, loss = 8.60999453\n",
      "Iteration 683, loss = 8.60775453\n",
      "Iteration 684, loss = 8.60551730\n",
      "Iteration 685, loss = 8.60328283\n",
      "Iteration 686, loss = 8.60097288\n",
      "Iteration 687, loss = 8.59855373\n",
      "Iteration 688, loss = 8.59613734\n",
      "Iteration 689, loss = 8.59372369\n",
      "Iteration 690, loss = 8.59131277\n",
      "Iteration 691, loss = 8.58890455\n",
      "Iteration 692, loss = 8.58648827\n",
      "Iteration 693, loss = 8.58406250\n",
      "Iteration 694, loss = 8.58163865\n",
      "Iteration 695, loss = 8.57921680\n",
      "Iteration 696, loss = 8.57679698\n",
      "Iteration 697, loss = 8.57437925\n",
      "Iteration 698, loss = 8.57196365\n",
      "Iteration 699, loss = 8.56955020\n",
      "Iteration 700, loss = 8.56713894\n",
      "Iteration 701, loss = 8.56472988\n",
      "Iteration 702, loss = 8.56232305\n",
      "Iteration 703, loss = 8.55991845\n",
      "Iteration 704, loss = 8.55751609\n",
      "Iteration 705, loss = 8.55511599\n",
      "Iteration 706, loss = 8.55271815\n",
      "Iteration 707, loss = 8.55032256\n",
      "Iteration 708, loss = 8.54792922\n",
      "Iteration 709, loss = 8.54553814\n",
      "Iteration 710, loss = 8.54314930\n",
      "Iteration 711, loss = 8.54076271\n",
      "Iteration 712, loss = 8.53837834\n",
      "Iteration 713, loss = 8.53599619\n",
      "Iteration 714, loss = 8.53361625\n",
      "Iteration 715, loss = 8.53123850\n",
      "Iteration 716, loss = 8.52886293\n",
      "Iteration 717, loss = 8.52648953\n",
      "Iteration 718, loss = 8.52411829\n",
      "Iteration 719, loss = 8.52174917\n",
      "Iteration 720, loss = 8.51938218\n",
      "Iteration 721, loss = 8.51701728\n",
      "Iteration 722, loss = 8.51465447\n",
      "Iteration 723, loss = 8.51229372\n",
      "Iteration 724, loss = 8.50993501\n",
      "Iteration 725, loss = 8.50757833\n",
      "Iteration 726, loss = 8.50522365\n",
      "Iteration 727, loss = 8.50287096\n",
      "Iteration 728, loss = 8.50052023\n",
      "Iteration 729, loss = 8.49817145\n",
      "Iteration 730, loss = 8.49582459\n",
      "Iteration 731, loss = 8.49347964\n",
      "Iteration 732, loss = 8.49113656\n",
      "Iteration 733, loss = 8.48879535\n",
      "Iteration 734, loss = 8.48645597\n",
      "Iteration 735, loss = 8.48411841\n",
      "Iteration 736, loss = 8.48178265\n",
      "Iteration 737, loss = 8.47944866\n",
      "Iteration 738, loss = 8.47711642\n",
      "Iteration 739, loss = 8.47478591\n",
      "Iteration 740, loss = 8.47245712\n",
      "Iteration 741, loss = 8.47013001\n",
      "Iteration 742, loss = 8.46780457\n",
      "Iteration 743, loss = 8.46548077\n",
      "Iteration 744, loss = 8.46315860\n",
      "Iteration 745, loss = 8.46083803\n",
      "Iteration 746, loss = 8.45851904\n",
      "Iteration 747, loss = 8.45620160\n",
      "Iteration 748, loss = 8.45388571\n",
      "Iteration 749, loss = 8.45157133\n",
      "Iteration 750, loss = 8.44925845\n",
      "Iteration 751, loss = 8.44694704\n",
      "Iteration 752, loss = 8.44463709\n",
      "Iteration 753, loss = 8.44232857\n",
      "Iteration 754, loss = 8.44002146\n",
      "Iteration 755, loss = 8.43771574\n",
      "Iteration 756, loss = 8.43541140\n",
      "Iteration 757, loss = 8.43310840\n",
      "Iteration 758, loss = 8.43080674\n",
      "Iteration 759, loss = 8.42850638\n",
      "Iteration 760, loss = 8.42620885\n",
      "Iteration 761, loss = 8.42391387\n",
      "Iteration 762, loss = 8.42162009\n",
      "Iteration 763, loss = 8.41932751\n",
      "Iteration 764, loss = 8.41703610\n",
      "Iteration 765, loss = 8.41474582\n",
      "Iteration 766, loss = 8.41245667\n",
      "Iteration 767, loss = 8.41016861\n",
      "Iteration 768, loss = 8.40788163\n",
      "Iteration 769, loss = 8.40559571\n",
      "Iteration 770, loss = 8.40331083\n",
      "Iteration 771, loss = 8.40102696\n",
      "Iteration 772, loss = 8.39874409\n",
      "Iteration 773, loss = 8.39646220\n",
      "Iteration 774, loss = 8.39418128\n",
      "Iteration 775, loss = 8.39190129\n",
      "Iteration 776, loss = 8.38962224\n",
      "Iteration 777, loss = 8.38734409\n",
      "Iteration 778, loss = 8.38506684\n",
      "Iteration 779, loss = 8.38279046\n",
      "Iteration 780, loss = 8.38051494\n",
      "Iteration 781, loss = 8.37824027\n",
      "Iteration 782, loss = 8.37596642\n",
      "Iteration 783, loss = 8.37369339\n",
      "Iteration 784, loss = 8.37142115\n",
      "Iteration 785, loss = 8.36914969\n",
      "Iteration 786, loss = 8.36687901\n",
      "Iteration 787, loss = 8.36460907\n",
      "Iteration 788, loss = 8.36234028\n",
      "Iteration 789, loss = 8.36007259\n",
      "Iteration 790, loss = 8.35780593\n",
      "Iteration 791, loss = 8.35554026\n",
      "Iteration 792, loss = 8.35327555\n",
      "Iteration 793, loss = 8.35101174\n",
      "Iteration 794, loss = 8.34874881\n",
      "Iteration 795, loss = 8.34648671\n",
      "Iteration 796, loss = 8.34422542\n",
      "Iteration 797, loss = 8.34196490\n",
      "Iteration 798, loss = 8.33970513\n",
      "Iteration 799, loss = 8.33744607\n",
      "Iteration 800, loss = 8.33518771\n",
      "Iteration 801, loss = 8.33293003\n",
      "Iteration 802, loss = 8.33067299\n",
      "Iteration 803, loss = 8.32841657\n",
      "Iteration 804, loss = 8.32616076\n",
      "Iteration 805, loss = 8.32390555\n",
      "Iteration 806, loss = 8.32165090\n",
      "Iteration 807, loss = 8.31939680\n",
      "Iteration 808, loss = 8.31714324\n",
      "Iteration 809, loss = 8.31489019\n",
      "Iteration 810, loss = 8.31263766\n",
      "Iteration 811, loss = 8.31038561\n",
      "Iteration 812, loss = 8.30813403\n",
      "Iteration 813, loss = 8.30588292\n",
      "Iteration 814, loss = 8.30363226\n",
      "Iteration 815, loss = 8.30138204\n",
      "Iteration 816, loss = 8.29913224\n",
      "Iteration 817, loss = 8.29688285\n",
      "Iteration 818, loss = 8.29463386\n",
      "Iteration 819, loss = 8.29238526\n",
      "Iteration 820, loss = 8.29013704\n",
      "Iteration 821, loss = 8.28788918\n",
      "Iteration 822, loss = 8.28564169\n",
      "Iteration 823, loss = 8.28336512\n",
      "Iteration 824, loss = 8.28090431\n",
      "Iteration 825, loss = 8.27844381\n",
      "Iteration 826, loss = 8.27598362\n",
      "Iteration 827, loss = 8.27352372\n",
      "Iteration 828, loss = 8.27106411\n",
      "Iteration 829, loss = 8.26860477\n",
      "Iteration 830, loss = 8.26614570\n",
      "Iteration 831, loss = 8.26368690\n",
      "Iteration 832, loss = 8.26122834\n",
      "Iteration 833, loss = 8.25877003\n",
      "Iteration 834, loss = 8.25631195\n",
      "Iteration 835, loss = 8.25385410\n",
      "Iteration 836, loss = 8.25139646\n",
      "Iteration 837, loss = 8.24893904\n",
      "Iteration 838, loss = 8.24648182\n",
      "Iteration 839, loss = 8.24402480\n",
      "Iteration 840, loss = 8.24156796\n",
      "Iteration 841, loss = 8.23911131\n",
      "Iteration 842, loss = 8.23665483\n",
      "Iteration 843, loss = 8.23419851\n",
      "Iteration 844, loss = 8.23174236\n",
      "Iteration 845, loss = 8.22928636\n",
      "Iteration 846, loss = 8.22683051\n",
      "Iteration 847, loss = 8.22437480\n",
      "Iteration 848, loss = 8.22191922\n",
      "Iteration 849, loss = 8.21946377\n",
      "Iteration 850, loss = 8.21700844\n",
      "Iteration 851, loss = 8.21455323\n",
      "Iteration 852, loss = 8.21209813\n",
      "Iteration 853, loss = 8.20964313\n",
      "Iteration 854, loss = 8.20718823\n",
      "Iteration 855, loss = 8.20473342\n",
      "Iteration 856, loss = 8.20227870\n",
      "Iteration 857, loss = 8.19982405\n",
      "Iteration 858, loss = 8.19736949\n",
      "Iteration 859, loss = 8.19491499\n",
      "Iteration 860, loss = 8.19246056\n",
      "Iteration 861, loss = 8.19000619\n",
      "Iteration 862, loss = 8.18755188\n",
      "Iteration 863, loss = 8.18509761\n",
      "Iteration 864, loss = 8.18264339\n",
      "Iteration 865, loss = 8.18018921\n",
      "Iteration 866, loss = 8.17773507\n",
      "Iteration 867, loss = 8.17528095\n",
      "Iteration 868, loss = 8.17282686\n",
      "Iteration 869, loss = 8.17037280\n",
      "Iteration 870, loss = 8.16791875\n",
      "Iteration 871, loss = 8.16546472\n",
      "Iteration 872, loss = 8.16301069\n",
      "Iteration 873, loss = 8.16055667\n",
      "Iteration 874, loss = 8.15810265\n",
      "Iteration 875, loss = 8.15564862\n",
      "Iteration 876, loss = 8.15319458\n",
      "Iteration 877, loss = 8.15074053\n",
      "Iteration 878, loss = 8.14828646\n",
      "Iteration 879, loss = 8.14583238\n",
      "Iteration 880, loss = 8.14337827\n",
      "Iteration 881, loss = 8.14092413\n",
      "Iteration 882, loss = 8.13846997\n",
      "Iteration 883, loss = 8.13601577\n",
      "Iteration 884, loss = 8.13356153\n",
      "Iteration 885, loss = 8.13110726\n",
      "Iteration 886, loss = 8.12865294\n",
      "Iteration 887, loss = 8.12619857\n",
      "Iteration 888, loss = 8.12374415\n",
      "Iteration 889, loss = 8.12128969\n",
      "Iteration 890, loss = 8.11883516\n",
      "Iteration 891, loss = 8.11638058\n",
      "Iteration 892, loss = 8.11392593\n",
      "Iteration 893, loss = 8.11147122\n",
      "Iteration 894, loss = 8.10901645\n",
      "Iteration 895, loss = 8.10656161\n",
      "Iteration 896, loss = 8.10410669\n",
      "Iteration 897, loss = 8.10165170\n",
      "Iteration 898, loss = 8.09919663\n",
      "Iteration 899, loss = 8.09674147\n",
      "Iteration 900, loss = 8.09428624\n",
      "Iteration 901, loss = 8.09183091\n",
      "Iteration 902, loss = 8.08937550\n",
      "Iteration 903, loss = 8.08692000\n",
      "Iteration 904, loss = 8.08446440\n",
      "Iteration 905, loss = 8.08200870\n",
      "Iteration 906, loss = 8.07955291\n",
      "Iteration 907, loss = 8.07709701\n",
      "Iteration 908, loss = 8.07464101\n",
      "Iteration 909, loss = 8.07218491\n",
      "Iteration 910, loss = 8.06972870\n",
      "Iteration 911, loss = 8.06727238\n",
      "Iteration 912, loss = 8.06481594\n",
      "Iteration 913, loss = 8.06235940\n",
      "Iteration 914, loss = 8.05990274\n",
      "Iteration 915, loss = 8.05744596\n",
      "Iteration 916, loss = 8.05498906\n",
      "Iteration 917, loss = 8.05253204\n",
      "Iteration 918, loss = 8.05007490\n",
      "Iteration 919, loss = 8.04761763\n",
      "Iteration 920, loss = 8.04516024\n",
      "Iteration 921, loss = 8.04270271\n",
      "Iteration 922, loss = 8.04024506\n",
      "Iteration 923, loss = 8.03778728\n",
      "Iteration 924, loss = 8.03532937\n",
      "Iteration 925, loss = 8.03287132\n",
      "Iteration 926, loss = 8.03041313\n",
      "Iteration 927, loss = 8.02795481\n",
      "Iteration 928, loss = 8.02549635\n",
      "Iteration 929, loss = 8.02303775\n",
      "Iteration 930, loss = 8.02057900\n",
      "Iteration 931, loss = 8.01812012\n",
      "Iteration 932, loss = 8.01566109\n",
      "Iteration 933, loss = 8.01320191\n",
      "Iteration 934, loss = 8.01074259\n",
      "Iteration 935, loss = 8.00828312\n",
      "Iteration 936, loss = 8.00582350\n",
      "Iteration 937, loss = 8.00336374\n",
      "Iteration 938, loss = 8.00090382\n",
      "Iteration 939, loss = 7.99844374\n",
      "Iteration 940, loss = 7.99598352\n",
      "Iteration 941, loss = 7.99352314\n",
      "Iteration 942, loss = 7.99106260\n",
      "Iteration 943, loss = 7.98859345\n",
      "Iteration 944, loss = 7.98611754\n",
      "Iteration 945, loss = 7.98364085\n",
      "Iteration 946, loss = 7.98116343\n",
      "Iteration 947, loss = 7.97868535\n",
      "Iteration 948, loss = 7.97620666\n",
      "Iteration 949, loss = 7.97372740\n",
      "Iteration 950, loss = 7.97124761\n",
      "Iteration 951, loss = 7.96876620\n",
      "Iteration 952, loss = 7.96611027\n",
      "Iteration 953, loss = 7.96343140\n",
      "Iteration 954, loss = 7.96075127\n",
      "Iteration 955, loss = 7.95807000\n",
      "Iteration 956, loss = 7.95538767\n",
      "Iteration 957, loss = 7.95270439\n",
      "Iteration 958, loss = 7.95002023\n",
      "Iteration 959, loss = 7.94733525\n",
      "Iteration 960, loss = 7.94464954\n",
      "Iteration 961, loss = 7.94196314\n",
      "Iteration 962, loss = 7.93927611\n",
      "Iteration 963, loss = 7.93658850\n",
      "Iteration 964, loss = 7.93390034\n",
      "Iteration 965, loss = 7.93121168\n",
      "Iteration 966, loss = 7.92852254\n",
      "Iteration 967, loss = 7.92583297\n",
      "Iteration 968, loss = 7.92314298\n",
      "Iteration 969, loss = 7.92045261\n",
      "Iteration 970, loss = 7.91776187\n",
      "Iteration 971, loss = 7.91507079\n",
      "Iteration 972, loss = 7.91237939\n",
      "Iteration 973, loss = 7.90968767\n",
      "Iteration 974, loss = 7.90699565\n",
      "Iteration 975, loss = 7.90430336\n",
      "Iteration 976, loss = 7.90161079\n",
      "Iteration 977, loss = 7.89891797\n",
      "Iteration 978, loss = 7.89622489\n",
      "Iteration 979, loss = 7.89353157\n",
      "Iteration 980, loss = 7.89083802\n",
      "Iteration 981, loss = 7.88814425\n",
      "Iteration 982, loss = 7.88545025\n",
      "Iteration 983, loss = 7.88275604\n",
      "Iteration 984, loss = 7.88006163\n",
      "Iteration 985, loss = 7.87736701\n",
      "Iteration 986, loss = 7.87467219\n",
      "Iteration 987, loss = 7.87197717\n",
      "Iteration 988, loss = 7.86928197\n",
      "Iteration 989, loss = 7.86658657\n",
      "Iteration 990, loss = 7.86389099\n",
      "Iteration 991, loss = 7.86119523\n",
      "Iteration 992, loss = 7.85849930\n",
      "Iteration 993, loss = 7.85580318\n",
      "Iteration 994, loss = 7.85310689\n",
      "Iteration 995, loss = 7.85041043\n",
      "Iteration 996, loss = 7.84771380\n",
      "Iteration 997, loss = 7.84501700\n",
      "Iteration 998, loss = 7.84232003\n",
      "Iteration 999, loss = 7.83962291\n",
      "Iteration 1000, loss = 7.83692562\n",
      "Iteration 1001, loss = 7.83422817\n",
      "Iteration 1002, loss = 7.83153056\n",
      "Iteration 1003, loss = 7.82883279\n",
      "Iteration 1004, loss = 7.82613488\n",
      "Iteration 1005, loss = 7.82343681\n",
      "Iteration 1006, loss = 7.82073858\n",
      "Iteration 1007, loss = 7.81804021\n",
      "Iteration 1008, loss = 7.81534170\n",
      "Iteration 1009, loss = 7.81264304\n",
      "Iteration 1010, loss = 7.80994423\n",
      "Iteration 1011, loss = 7.80724529\n",
      "Iteration 1012, loss = 7.80454621\n",
      "Iteration 1013, loss = 7.80184699\n",
      "Iteration 1014, loss = 7.79914764\n",
      "Iteration 1015, loss = 7.79644815\n",
      "Iteration 1016, loss = 7.79374854\n",
      "Iteration 1017, loss = 7.79104880\n",
      "Iteration 1018, loss = 7.78834894\n",
      "Iteration 1019, loss = 7.78564896\n",
      "Iteration 1020, loss = 7.78294886\n",
      "Iteration 1021, loss = 7.78024864\n",
      "Iteration 1022, loss = 7.77754832\n",
      "Iteration 1023, loss = 7.77484788\n",
      "Iteration 1024, loss = 7.77214735\n",
      "Iteration 1025, loss = 7.76944671\n",
      "Iteration 1026, loss = 7.76674598\n",
      "Iteration 1027, loss = 7.76404515\n",
      "Iteration 1028, loss = 7.76134424\n",
      "Iteration 1029, loss = 7.75864324\n",
      "Iteration 1030, loss = 7.75594217\n",
      "Iteration 1031, loss = 7.75324102\n",
      "Iteration 1032, loss = 7.75053980\n",
      "Iteration 1033, loss = 7.74783852\n",
      "Iteration 1034, loss = 7.74513719\n",
      "Iteration 1035, loss = 7.74243580\n",
      "Iteration 1036, loss = 7.73973437\n",
      "Iteration 1037, loss = 7.73703290\n",
      "Iteration 1038, loss = 7.73433140\n",
      "Iteration 1039, loss = 7.73162987\n",
      "Iteration 1040, loss = 7.72892833\n",
      "Iteration 1041, loss = 7.72622678\n",
      "Iteration 1042, loss = 7.72352523\n",
      "Iteration 1043, loss = 7.72082369\n",
      "Iteration 1044, loss = 7.71812217\n",
      "Iteration 1045, loss = 7.71542067\n",
      "Iteration 1046, loss = 7.71271922\n",
      "Iteration 1047, loss = 7.71001781\n",
      "Iteration 1048, loss = 7.70731646\n",
      "Iteration 1049, loss = 7.70461518\n",
      "Iteration 1050, loss = 7.70191398\n",
      "Iteration 1051, loss = 7.69921288\n",
      "Iteration 1052, loss = 7.69651189\n",
      "Iteration 1053, loss = 7.69381102\n",
      "Iteration 1054, loss = 7.69111029\n",
      "Iteration 1055, loss = 7.68840971\n",
      "Iteration 1056, loss = 7.68570930\n",
      "Iteration 1057, loss = 7.68300908\n",
      "Iteration 1058, loss = 7.68030906\n",
      "Iteration 1059, loss = 7.67760927\n",
      "Iteration 1060, loss = 7.67490971\n",
      "Iteration 1061, loss = 7.67221041\n",
      "Iteration 1062, loss = 7.66951140\n",
      "Iteration 1063, loss = 7.66681269\n",
      "Iteration 1064, loss = 7.66411431\n",
      "Iteration 1065, loss = 7.66141629\n",
      "Iteration 1066, loss = 7.65871863\n",
      "Iteration 1067, loss = 7.65602139\n",
      "Iteration 1068, loss = 7.65332457\n",
      "Iteration 1069, loss = 7.65062821\n",
      "Iteration 1070, loss = 7.64793235\n",
      "Iteration 1071, loss = 7.64523700\n",
      "Iteration 1072, loss = 7.64254221\n",
      "Iteration 1073, loss = 7.63984801\n",
      "Iteration 1074, loss = 7.63715444\n",
      "Iteration 1075, loss = 7.63446153\n",
      "Iteration 1076, loss = 7.63176932\n",
      "Iteration 1077, loss = 7.62907785\n",
      "Iteration 1078, loss = 7.62638717\n",
      "Iteration 1079, loss = 7.62369732\n",
      "Iteration 1080, loss = 7.62100835\n",
      "Iteration 1081, loss = 7.61832031\n",
      "Iteration 1082, loss = 7.61563324\n",
      "Iteration 1083, loss = 7.61294720\n",
      "Iteration 1084, loss = 7.61026225\n",
      "Iteration 1085, loss = 7.60757843\n",
      "Iteration 1086, loss = 7.60489582\n",
      "Iteration 1087, loss = 7.60221447\n",
      "Iteration 1088, loss = 7.59953444\n",
      "Iteration 1089, loss = 7.59685581\n",
      "Iteration 1090, loss = 7.59417865\n",
      "Iteration 1091, loss = 7.59150301\n",
      "Iteration 1092, loss = 7.58882899\n",
      "Iteration 1093, loss = 7.58615666\n",
      "Iteration 1094, loss = 7.58348609\n",
      "Iteration 1095, loss = 7.58081737\n",
      "Iteration 1096, loss = 7.57815059\n",
      "Iteration 1097, loss = 7.57548584\n",
      "Iteration 1098, loss = 7.57282320\n",
      "Iteration 1099, loss = 7.57016278\n",
      "Iteration 1100, loss = 7.56750467\n",
      "Iteration 1101, loss = 7.56484897\n",
      "Iteration 1102, loss = 7.56219578\n",
      "Iteration 1103, loss = 7.55954522\n",
      "Iteration 1104, loss = 7.55689740\n",
      "Iteration 1105, loss = 7.55425242\n",
      "Iteration 1106, loss = 7.55161040\n",
      "Iteration 1107, loss = 7.54897147\n",
      "Iteration 1108, loss = 7.54633574\n",
      "Iteration 1109, loss = 7.54370335\n",
      "Iteration 1110, loss = 7.54107440\n",
      "Iteration 1111, loss = 7.53844905\n",
      "Iteration 1112, loss = 7.53582741\n",
      "Iteration 1113, loss = 7.53320963\n",
      "Iteration 1114, loss = 7.53059342\n",
      "Iteration 1115, loss = 7.52796320\n",
      "Iteration 1116, loss = 7.52533581\n",
      "Iteration 1117, loss = 7.52271154\n",
      "Iteration 1118, loss = 7.52009067\n",
      "Iteration 1119, loss = 7.51747346\n",
      "Iteration 1120, loss = 7.51486017\n",
      "Iteration 1121, loss = 7.51225104\n",
      "Iteration 1122, loss = 7.50964630\n",
      "Iteration 1123, loss = 7.50704618\n",
      "Iteration 1124, loss = 7.50445090\n",
      "Iteration 1125, loss = 7.50186066\n",
      "Iteration 1126, loss = 7.49926480\n",
      "Iteration 1127, loss = 7.49666965\n",
      "Iteration 1128, loss = 7.49407946\n",
      "Iteration 1129, loss = 7.49149448\n",
      "Iteration 1130, loss = 7.48891495\n",
      "Iteration 1131, loss = 7.48634108\n",
      "Iteration 1132, loss = 7.48377308\n",
      "Iteration 1133, loss = 7.48121113\n",
      "Iteration 1134, loss = 7.47865543\n",
      "Iteration 1135, loss = 7.47610615\n",
      "Iteration 1136, loss = 7.47356342\n",
      "Iteration 1137, loss = 7.47102741\n",
      "Iteration 1138, loss = 7.46849825\n",
      "Iteration 1139, loss = 7.46597606\n",
      "Iteration 1140, loss = 7.46346095\n",
      "Iteration 1141, loss = 7.46095302\n",
      "Iteration 1142, loss = 7.45845237\n",
      "Iteration 1143, loss = 7.45595908\n",
      "Iteration 1144, loss = 7.45347321\n",
      "Iteration 1145, loss = 7.45099482\n",
      "Iteration 1146, loss = 7.44852396\n",
      "Iteration 1147, loss = 7.44606067\n",
      "Iteration 1148, loss = 7.44360498\n",
      "Iteration 1149, loss = 7.44115690\n",
      "Iteration 1150, loss = 7.43871643\n",
      "Iteration 1151, loss = 7.43628358\n",
      "Iteration 1152, loss = 7.43385833\n",
      "Iteration 1153, loss = 7.43144067\n",
      "Iteration 1154, loss = 7.42903056\n",
      "Iteration 1155, loss = 7.42662796\n",
      "Iteration 1156, loss = 7.42423282\n",
      "Iteration 1157, loss = 7.42184510\n",
      "Iteration 1158, loss = 7.41946472\n",
      "Iteration 1159, loss = 7.41709161\n",
      "Iteration 1160, loss = 7.41472570\n",
      "Iteration 1161, loss = 7.41236691\n",
      "Iteration 1162, loss = 7.41001514\n",
      "Iteration 1163, loss = 7.40767030\n",
      "Iteration 1164, loss = 7.40533229\n",
      "Iteration 1165, loss = 7.40300100\n",
      "Iteration 1166, loss = 7.40067632\n",
      "Iteration 1167, loss = 7.39835444\n",
      "Iteration 1168, loss = 7.39602769\n",
      "Iteration 1169, loss = 7.39370725\n",
      "Iteration 1170, loss = 7.39139302\n",
      "Iteration 1171, loss = 7.38908487\n",
      "Iteration 1172, loss = 7.38678269\n",
      "Iteration 1173, loss = 7.38448635\n",
      "Iteration 1174, loss = 7.38219572\n",
      "Iteration 1175, loss = 7.37991068\n",
      "Iteration 1176, loss = 7.37763108\n",
      "Iteration 1177, loss = 7.37535681\n",
      "Iteration 1178, loss = 7.37308773\n",
      "Iteration 1179, loss = 7.37082371\n",
      "Iteration 1180, loss = 7.36856461\n",
      "Iteration 1181, loss = 7.36631031\n",
      "Iteration 1182, loss = 7.36406067\n",
      "Iteration 1183, loss = 7.36181557\n",
      "Iteration 1184, loss = 7.35957487\n",
      "Iteration 1185, loss = 7.35733845\n",
      "Iteration 1186, loss = 7.35510619\n",
      "Iteration 1187, loss = 7.35287795\n",
      "Iteration 1188, loss = 7.35065363\n",
      "Iteration 1189, loss = 7.34843310\n",
      "Iteration 1190, loss = 7.34621623\n",
      "Iteration 1191, loss = 7.34400293\n",
      "Iteration 1192, loss = 7.34179307\n",
      "Iteration 1193, loss = 7.33958655\n",
      "Iteration 1194, loss = 7.33738326\n",
      "Iteration 1195, loss = 7.33518310\n",
      "Iteration 1196, loss = 7.33298595\n",
      "Iteration 1197, loss = 7.33079173\n",
      "Iteration 1198, loss = 7.32860033\n",
      "Iteration 1199, loss = 7.32641167\n",
      "Iteration 1200, loss = 7.32422565\n",
      "Iteration 1201, loss = 7.32204218\n",
      "Iteration 1202, loss = 7.31986118\n",
      "Iteration 1203, loss = 7.31768256\n",
      "Iteration 1204, loss = 7.31550624\n",
      "Iteration 1205, loss = 7.31333215\n",
      "Iteration 1206, loss = 7.31116021\n",
      "Iteration 1207, loss = 7.30899035\n",
      "Iteration 1208, loss = 7.30682249\n",
      "Iteration 1209, loss = 7.30465657\n",
      "Iteration 1210, loss = 7.30249253\n",
      "Iteration 1211, loss = 7.30033029\n",
      "Iteration 1212, loss = 7.29816981\n",
      "Iteration 1213, loss = 7.29601101\n",
      "Iteration 1214, loss = 7.29385385\n",
      "Iteration 1215, loss = 7.29169827\n",
      "Iteration 1216, loss = 7.28954422\n",
      "Iteration 1217, loss = 7.28739164\n",
      "Iteration 1218, loss = 7.28524050\n",
      "Iteration 1219, loss = 7.28309073\n",
      "Iteration 1220, loss = 7.28094231\n",
      "Iteration 1221, loss = 7.27879519\n",
      "Iteration 1222, loss = 7.27664932\n",
      "Iteration 1223, loss = 7.27450467\n",
      "Iteration 1224, loss = 7.27236120\n",
      "Iteration 1225, loss = 7.27021887\n",
      "Iteration 1226, loss = 7.26807766\n",
      "Iteration 1227, loss = 7.26593753\n",
      "Iteration 1228, loss = 7.26379844\n",
      "Iteration 1229, loss = 7.26166037\n",
      "Iteration 1230, loss = 7.25952329\n",
      "Iteration 1231, loss = 7.25725935\n",
      "Iteration 1232, loss = 7.25492095\n",
      "Iteration 1233, loss = 7.25258346\n",
      "Iteration 1234, loss = 7.25024688\n",
      "Iteration 1235, loss = 7.24791116\n",
      "Iteration 1236, loss = 7.24557630\n",
      "Iteration 1237, loss = 7.24324226\n",
      "Iteration 1238, loss = 7.24090904\n",
      "Iteration 1239, loss = 7.23857662\n",
      "Iteration 1240, loss = 7.23624497\n",
      "Iteration 1241, loss = 7.23391408\n",
      "Iteration 1242, loss = 7.23158394\n",
      "Iteration 1243, loss = 7.22925453\n",
      "Iteration 1244, loss = 7.22692583\n",
      "Iteration 1245, loss = 7.22459784\n",
      "Iteration 1246, loss = 7.22227054\n",
      "Iteration 1247, loss = 7.21994393\n",
      "Iteration 1248, loss = 7.21761799\n",
      "Iteration 1249, loss = 7.21529271\n",
      "Iteration 1250, loss = 7.21296808\n",
      "Iteration 1251, loss = 7.21064410\n",
      "Iteration 1252, loss = 7.20832075\n",
      "Iteration 1253, loss = 7.20599804\n",
      "Iteration 1254, loss = 7.20367595\n",
      "Iteration 1255, loss = 7.20135448\n",
      "Iteration 1256, loss = 7.19903363\n",
      "Iteration 1257, loss = 7.19671338\n",
      "Iteration 1258, loss = 7.19439374\n",
      "Iteration 1259, loss = 7.19207471\n",
      "Iteration 1260, loss = 7.18975627\n",
      "Iteration 1261, loss = 7.18743843\n",
      "Iteration 1262, loss = 7.18512118\n",
      "Iteration 1263, loss = 7.18280452\n",
      "Iteration 1264, loss = 7.18048846\n",
      "Iteration 1265, loss = 7.17817311\n",
      "Iteration 1266, loss = 7.17585847\n",
      "Iteration 1267, loss = 7.17354451\n",
      "Iteration 1268, loss = 7.17123123\n",
      "Iteration 1269, loss = 7.16891863\n",
      "Iteration 1270, loss = 7.16660669\n",
      "Iteration 1271, loss = 7.16429541\n",
      "Iteration 1272, loss = 7.16198478\n",
      "Iteration 1273, loss = 7.15967479\n",
      "Iteration 1274, loss = 7.15736546\n",
      "Iteration 1275, loss = 7.15505676\n",
      "Iteration 1276, loss = 7.15274870\n",
      "Iteration 1277, loss = 7.15044128\n",
      "Iteration 1278, loss = 7.14813450\n",
      "Iteration 1279, loss = 7.14582836\n",
      "Iteration 1280, loss = 7.14352285\n",
      "Iteration 1281, loss = 7.14121799\n",
      "Iteration 1282, loss = 7.13891376\n",
      "Iteration 1283, loss = 7.13661017\n",
      "Iteration 1284, loss = 7.13430723\n",
      "Iteration 1285, loss = 7.13200494\n",
      "Iteration 1286, loss = 7.12970329\n",
      "Iteration 1287, loss = 7.12740229\n",
      "Iteration 1288, loss = 7.12506118\n",
      "Iteration 1289, loss = 7.12256704\n",
      "Iteration 1290, loss = 7.12007355\n",
      "Iteration 1291, loss = 7.11758072\n",
      "Iteration 1292, loss = 7.11508855\n",
      "Iteration 1293, loss = 7.11259705\n",
      "Iteration 1294, loss = 7.11010622\n",
      "Iteration 1295, loss = 7.10761607\n",
      "Iteration 1296, loss = 7.10512660\n",
      "Iteration 1297, loss = 7.10263782\n",
      "Iteration 1298, loss = 7.10014973\n",
      "Iteration 1299, loss = 7.09766233\n",
      "Iteration 1300, loss = 7.09517564\n",
      "Iteration 1301, loss = 7.09268965\n",
      "Iteration 1302, loss = 7.09020437\n",
      "Iteration 1303, loss = 7.08771982\n",
      "Iteration 1304, loss = 7.08523598\n",
      "Iteration 1305, loss = 7.08275287\n",
      "Iteration 1306, loss = 7.08027050\n",
      "Iteration 1307, loss = 7.07778886\n",
      "Iteration 1308, loss = 7.07530797\n",
      "Iteration 1309, loss = 7.07282783\n",
      "Iteration 1310, loss = 7.07034844\n",
      "Iteration 1311, loss = 7.06786981\n",
      "Iteration 1312, loss = 7.06539195\n",
      "Iteration 1313, loss = 7.06291486\n",
      "Iteration 1314, loss = 7.06043854\n",
      "Iteration 1315, loss = 7.05796301\n",
      "Iteration 1316, loss = 7.05548826\n",
      "Iteration 1317, loss = 7.05301431\n",
      "Iteration 1318, loss = 7.05054115\n",
      "Iteration 1319, loss = 7.04806879\n",
      "Iteration 1320, loss = 7.04559725\n",
      "Iteration 1321, loss = 7.04312651\n",
      "Iteration 1322, loss = 7.04065659\n",
      "Iteration 1323, loss = 7.03818750\n",
      "Iteration 1324, loss = 7.03571922\n",
      "Iteration 1325, loss = 7.03325179\n",
      "Iteration 1326, loss = 7.03078518\n",
      "Iteration 1327, loss = 7.02831942\n",
      "Iteration 1328, loss = 7.02585450\n",
      "Iteration 1329, loss = 7.02339043\n",
      "Iteration 1330, loss = 7.02092721\n",
      "Iteration 1331, loss = 7.01846485\n",
      "Iteration 1332, loss = 7.01600336\n",
      "Iteration 1333, loss = 7.01354272\n",
      "Iteration 1334, loss = 7.01108296\n",
      "Iteration 1335, loss = 7.00862406\n",
      "Iteration 1336, loss = 7.00616605\n",
      "Iteration 1337, loss = 7.00370891\n",
      "Iteration 1338, loss = 7.00125265\n",
      "Iteration 1339, loss = 6.99879910\n",
      "Iteration 1340, loss = 6.99634794\n",
      "Iteration 1341, loss = 6.99389781\n",
      "Iteration 1342, loss = 6.99144869\n",
      "Iteration 1343, loss = 6.98900058\n",
      "Iteration 1344, loss = 6.98655347\n",
      "Iteration 1345, loss = 6.98410735\n",
      "Iteration 1346, loss = 6.98166222\n",
      "Iteration 1347, loss = 6.97921806\n",
      "Iteration 1348, loss = 6.97677487\n",
      "Iteration 1349, loss = 6.97433265\n",
      "Iteration 1350, loss = 6.97189139\n",
      "Iteration 1351, loss = 6.96945109\n",
      "Iteration 1352, loss = 6.96701174\n",
      "Iteration 1353, loss = 6.96457334\n",
      "Iteration 1354, loss = 6.96213590\n",
      "Iteration 1355, loss = 6.95969939\n",
      "Iteration 1356, loss = 6.95726383\n",
      "Iteration 1357, loss = 6.95482921\n",
      "Iteration 1358, loss = 6.95239552\n",
      "Iteration 1359, loss = 6.94996277\n",
      "Iteration 1360, loss = 6.94753095\n",
      "Iteration 1361, loss = 6.94510007\n",
      "Iteration 1362, loss = 6.94267011\n",
      "Iteration 1363, loss = 6.94024108\n",
      "Iteration 1364, loss = 6.93781297\n",
      "Iteration 1365, loss = 6.93530287\n",
      "Iteration 1366, loss = 6.93228408\n",
      "Iteration 1367, loss = 6.92926619\n",
      "Iteration 1368, loss = 6.92624921\n",
      "Iteration 1369, loss = 6.92323313\n",
      "Iteration 1370, loss = 6.92021795\n",
      "Iteration 1371, loss = 6.91720367\n",
      "Iteration 1372, loss = 6.91419029\n",
      "Iteration 1373, loss = 6.91117780\n",
      "Iteration 1374, loss = 6.90816620\n",
      "Iteration 1375, loss = 6.90515549\n",
      "Iteration 1376, loss = 6.90214566\n",
      "Iteration 1377, loss = 6.89913671\n",
      "Iteration 1378, loss = 6.89612864\n",
      "Iteration 1379, loss = 6.89312145\n",
      "Iteration 1380, loss = 6.89011513\n",
      "Iteration 1381, loss = 6.88710967\n",
      "Iteration 1382, loss = 6.88410508\n",
      "Iteration 1383, loss = 6.88110135\n",
      "Iteration 1384, loss = 6.87809848\n",
      "Iteration 1385, loss = 6.87509646\n",
      "Iteration 1386, loss = 6.87209529\n",
      "Iteration 1387, loss = 6.86909496\n",
      "Iteration 1388, loss = 6.86609547\n",
      "Iteration 1389, loss = 6.86309682\n",
      "Iteration 1390, loss = 6.86009900\n",
      "Iteration 1391, loss = 6.85710200\n",
      "Iteration 1392, loss = 6.85410582\n",
      "Iteration 1393, loss = 6.85111047\n",
      "Iteration 1394, loss = 6.84811592\n",
      "Iteration 1395, loss = 6.84512218\n",
      "Iteration 1396, loss = 6.84212923\n",
      "Iteration 1397, loss = 6.83913709\n",
      "Iteration 1398, loss = 6.83614573\n",
      "Iteration 1399, loss = 6.83315516\n",
      "Iteration 1400, loss = 6.83016537\n",
      "Iteration 1401, loss = 6.82717635\n",
      "Iteration 1402, loss = 6.82418809\n",
      "Iteration 1403, loss = 6.82120060\n",
      "Iteration 1404, loss = 6.81821386\n",
      "Iteration 1405, loss = 6.81522788\n",
      "Iteration 1406, loss = 6.81223459\n",
      "Iteration 1407, loss = 6.80923110\n",
      "Iteration 1408, loss = 6.80622776\n",
      "Iteration 1409, loss = 6.80322465\n",
      "Iteration 1410, loss = 6.80022179\n",
      "Iteration 1411, loss = 6.79721925\n",
      "Iteration 1412, loss = 6.79421705\n",
      "Iteration 1413, loss = 6.79121524\n",
      "Iteration 1414, loss = 6.78821383\n",
      "Iteration 1415, loss = 6.78485322\n",
      "Iteration 1416, loss = 6.78126927\n",
      "Iteration 1417, loss = 6.77768576\n",
      "Iteration 1418, loss = 6.77410273\n",
      "Iteration 1419, loss = 6.77052018\n",
      "Iteration 1420, loss = 6.76693812\n",
      "Iteration 1421, loss = 6.76335656\n",
      "Iteration 1422, loss = 6.75977552\n",
      "Iteration 1423, loss = 6.75619499\n",
      "Iteration 1424, loss = 6.75261499\n",
      "Iteration 1425, loss = 6.74903551\n",
      "Iteration 1426, loss = 6.74545656\n",
      "Iteration 1427, loss = 6.74187814\n",
      "Iteration 1428, loss = 6.73830025\n",
      "Iteration 1429, loss = 6.73472289\n",
      "Iteration 1430, loss = 6.73114605\n",
      "Iteration 1431, loss = 6.72756974\n",
      "Iteration 1432, loss = 6.72398950\n",
      "Iteration 1433, loss = 6.72039798\n",
      "Iteration 1434, loss = 6.71680594\n",
      "Iteration 1435, loss = 6.71321347\n",
      "Iteration 1436, loss = 6.70962068\n",
      "Iteration 1437, loss = 6.70602764\n",
      "Iteration 1438, loss = 6.70243443\n",
      "Iteration 1439, loss = 6.69884110\n",
      "Iteration 1440, loss = 6.69524771\n",
      "Iteration 1441, loss = 6.69165432\n",
      "Iteration 1442, loss = 6.68806097\n",
      "Iteration 1443, loss = 6.68446770\n",
      "Iteration 1444, loss = 6.68087454\n",
      "Iteration 1445, loss = 6.67728152\n",
      "Iteration 1446, loss = 6.67368868\n",
      "Iteration 1447, loss = 6.67009603\n",
      "Iteration 1448, loss = 6.66650359\n",
      "Iteration 1449, loss = 6.66291138\n",
      "Iteration 1450, loss = 6.65931942\n",
      "Iteration 1451, loss = 6.65572771\n",
      "Iteration 1452, loss = 6.65213628\n",
      "Iteration 1453, loss = 6.64854512\n",
      "Iteration 1454, loss = 6.64495425\n",
      "Iteration 1455, loss = 6.64136367\n",
      "Iteration 1456, loss = 6.63777339\n",
      "Iteration 1457, loss = 6.63418340\n",
      "Iteration 1458, loss = 6.63059372\n",
      "Iteration 1459, loss = 6.62700434\n",
      "Iteration 1460, loss = 6.62341527\n",
      "Iteration 1461, loss = 6.61982650\n",
      "Iteration 1462, loss = 6.61623803\n",
      "Iteration 1463, loss = 6.61264987\n",
      "Iteration 1464, loss = 6.60906200\n",
      "Iteration 1465, loss = 6.60547443\n",
      "Iteration 1466, loss = 6.60188716\n",
      "Iteration 1467, loss = 6.59830019\n",
      "Iteration 1468, loss = 6.59471350\n",
      "Iteration 1469, loss = 6.59112709\n",
      "Iteration 1470, loss = 6.58754097\n",
      "Iteration 1471, loss = 6.58395513\n",
      "Iteration 1472, loss = 6.58036956\n",
      "Iteration 1473, loss = 6.57678426\n",
      "Iteration 1474, loss = 6.57319922\n",
      "Iteration 1475, loss = 6.56961445\n",
      "Iteration 1476, loss = 6.56602993\n",
      "Iteration 1477, loss = 6.56244566\n",
      "Iteration 1478, loss = 6.55886164\n",
      "Iteration 1479, loss = 6.55527786\n",
      "Iteration 1480, loss = 6.55169432\n",
      "Iteration 1481, loss = 6.54811100\n",
      "Iteration 1482, loss = 6.54452792\n",
      "Iteration 1483, loss = 6.54094506\n",
      "Iteration 1484, loss = 6.53736241\n",
      "Iteration 1485, loss = 6.53377997\n",
      "Iteration 1486, loss = 6.53019775\n",
      "Iteration 1487, loss = 6.52661572\n",
      "Iteration 1488, loss = 6.52303390\n",
      "Iteration 1489, loss = 6.51945226\n",
      "Iteration 1490, loss = 6.51587082\n",
      "Iteration 1491, loss = 6.51228956\n",
      "Iteration 1492, loss = 6.50870848\n",
      "Iteration 1493, loss = 6.50512757\n",
      "Iteration 1494, loss = 6.50154684\n",
      "Iteration 1495, loss = 6.49796627\n",
      "Iteration 1496, loss = 6.49438586\n",
      "Iteration 1497, loss = 6.49080561\n",
      "Iteration 1498, loss = 6.48722551\n",
      "Iteration 1499, loss = 6.48364556\n",
      "Iteration 1500, loss = 6.48006576\n",
      "Iteration 1501, loss = 6.47648610\n",
      "Iteration 1502, loss = 6.47290658\n",
      "Iteration 1503, loss = 6.46932719\n",
      "Iteration 1504, loss = 6.46574793\n",
      "Iteration 1505, loss = 6.46216880\n",
      "Iteration 1506, loss = 6.45858979\n",
      "Iteration 1507, loss = 6.45501090\n",
      "Iteration 1508, loss = 6.45143213\n",
      "Iteration 1509, loss = 6.44785347\n",
      "Iteration 1510, loss = 6.44427492\n",
      "Iteration 1511, loss = 6.44069647\n",
      "Iteration 1512, loss = 6.43711813\n",
      "Iteration 1513, loss = 6.43353989\n",
      "Iteration 1514, loss = 6.42996176\n",
      "Iteration 1515, loss = 6.42638371\n",
      "Iteration 1516, loss = 6.42280576\n",
      "Iteration 1517, loss = 6.41922790\n",
      "Iteration 1518, loss = 6.41565013\n",
      "Iteration 1519, loss = 6.41207244\n",
      "Iteration 1520, loss = 6.40849484\n",
      "Iteration 1521, loss = 6.40491732\n",
      "Iteration 1522, loss = 6.40133987\n",
      "Iteration 1523, loss = 6.39776251\n",
      "Iteration 1524, loss = 6.39418522\n",
      "Iteration 1525, loss = 6.39060801\n",
      "Iteration 1526, loss = 6.38703086\n",
      "Iteration 1527, loss = 6.38345379\n",
      "Iteration 1528, loss = 6.37987679\n",
      "Iteration 1529, loss = 6.37629985\n",
      "Iteration 1530, loss = 6.37272299\n",
      "Iteration 1531, loss = 6.36914619\n",
      "Iteration 1532, loss = 6.36556945\n",
      "Iteration 1533, loss = 6.36199278\n",
      "Iteration 1534, loss = 6.35841616\n",
      "Iteration 1535, loss = 6.35483962\n",
      "Iteration 1536, loss = 6.35126313\n",
      "Iteration 1537, loss = 6.34768670\n",
      "Iteration 1538, loss = 6.34411034\n",
      "Iteration 1539, loss = 6.34053403\n",
      "Iteration 1540, loss = 6.33695779\n",
      "Iteration 1541, loss = 6.33338161\n",
      "Iteration 1542, loss = 6.32980548\n",
      "Iteration 1543, loss = 6.32622942\n",
      "Iteration 1544, loss = 6.32265341\n",
      "Iteration 1545, loss = 6.31907747\n",
      "Iteration 1546, loss = 6.31550159\n",
      "Iteration 1547, loss = 6.31192577\n",
      "Iteration 1548, loss = 6.30835001\n",
      "Iteration 1549, loss = 6.30477431\n",
      "Iteration 1550, loss = 6.30119868\n",
      "Iteration 1551, loss = 6.29762311\n",
      "Iteration 1552, loss = 6.29404761\n",
      "Iteration 1553, loss = 6.29047218\n",
      "Iteration 1554, loss = 6.28689682\n",
      "Iteration 1555, loss = 6.28332152\n",
      "Iteration 1556, loss = 6.27974630\n",
      "Iteration 1557, loss = 6.27617115\n",
      "Iteration 1558, loss = 6.27259607\n",
      "Iteration 1559, loss = 6.26902107\n",
      "Iteration 1560, loss = 6.26544615\n",
      "Iteration 1561, loss = 6.26187131\n",
      "Iteration 1562, loss = 6.25829656\n",
      "Iteration 1563, loss = 6.25472189\n",
      "Iteration 1564, loss = 6.25114730\n",
      "Iteration 1565, loss = 6.24757281\n",
      "Iteration 1566, loss = 6.24399841\n",
      "Iteration 1567, loss = 6.24042411\n",
      "Iteration 1568, loss = 6.23684990\n",
      "Iteration 1569, loss = 6.23327580\n",
      "Iteration 1570, loss = 6.22970181\n",
      "Iteration 1571, loss = 6.22612792\n",
      "Iteration 1572, loss = 6.22255414\n",
      "Iteration 1573, loss = 6.21898048\n",
      "Iteration 1574, loss = 6.21540694\n",
      "Iteration 1575, loss = 6.21183352\n",
      "Iteration 1576, loss = 6.20826023\n",
      "Iteration 1577, loss = 6.20468707\n",
      "Iteration 1578, loss = 6.20111405\n",
      "Iteration 1579, loss = 6.19754116\n",
      "Iteration 1580, loss = 6.19396842\n",
      "Iteration 1581, loss = 6.19039583\n",
      "Iteration 1582, loss = 6.18682339\n",
      "Iteration 1583, loss = 6.18325111\n",
      "Iteration 1584, loss = 6.17967900\n",
      "Iteration 1585, loss = 6.17610705\n",
      "Iteration 1586, loss = 6.17253528\n",
      "Iteration 1587, loss = 6.16896369\n",
      "Iteration 1588, loss = 6.16539228\n",
      "Iteration 1589, loss = 6.16182106\n",
      "Iteration 1590, loss = 6.15825004\n",
      "Iteration 1591, loss = 6.15467923\n",
      "Iteration 1592, loss = 6.15110862\n",
      "Iteration 1593, loss = 6.14753822\n",
      "Iteration 1594, loss = 6.14396805\n",
      "Iteration 1595, loss = 6.14039811\n",
      "Iteration 1596, loss = 6.13682840\n",
      "Iteration 1597, loss = 6.13325894\n",
      "Iteration 1598, loss = 6.12968972\n",
      "Iteration 1599, loss = 6.12612076\n",
      "Iteration 1600, loss = 6.12254883\n",
      "Iteration 1601, loss = 6.11897425\n",
      "Iteration 1602, loss = 6.11539736\n",
      "Iteration 1603, loss = 6.11181841\n",
      "Iteration 1604, loss = 6.10823766\n",
      "Iteration 1605, loss = 6.10465535\n",
      "Iteration 1606, loss = 6.10107167\n",
      "Iteration 1607, loss = 6.09748681\n",
      "Iteration 1608, loss = 6.09390094\n",
      "Iteration 1609, loss = 6.09031421\n",
      "Iteration 1610, loss = 6.08672675\n",
      "Iteration 1611, loss = 6.08313870\n",
      "Iteration 1612, loss = 6.07955016\n",
      "Iteration 1613, loss = 6.07596124\n",
      "Iteration 1614, loss = 6.07237203\n",
      "Iteration 1615, loss = 6.06878262\n",
      "Iteration 1616, loss = 6.06519308\n",
      "Iteration 1617, loss = 6.06160349\n",
      "Iteration 1618, loss = 6.05801392\n",
      "Iteration 1619, loss = 6.05442441\n",
      "Iteration 1620, loss = 6.05083503\n",
      "Iteration 1621, loss = 6.04724583\n",
      "Iteration 1622, loss = 6.04354309\n",
      "Iteration 1623, loss = 6.03928475\n",
      "Iteration 1624, loss = 6.03502661\n",
      "Iteration 1625, loss = 6.03076872\n",
      "Iteration 1626, loss = 6.02651112\n",
      "Iteration 1627, loss = 6.02225385\n",
      "Iteration 1628, loss = 6.01799697\n",
      "Iteration 1629, loss = 6.01374051\n",
      "Iteration 1630, loss = 6.00948450\n",
      "Iteration 1631, loss = 6.00522898\n",
      "Iteration 1632, loss = 6.00097398\n",
      "Iteration 1633, loss = 5.99671954\n",
      "Iteration 1634, loss = 5.99246568\n",
      "Iteration 1635, loss = 5.98821243\n",
      "Iteration 1636, loss = 5.98395982\n",
      "Iteration 1637, loss = 5.97970788\n",
      "Iteration 1638, loss = 5.97545663\n",
      "Iteration 1639, loss = 5.97120609\n",
      "Iteration 1640, loss = 5.96695629\n",
      "Iteration 1641, loss = 5.96270726\n",
      "Iteration 1642, loss = 5.95845902\n",
      "Iteration 1643, loss = 5.95421158\n",
      "Iteration 1644, loss = 5.94996497\n",
      "Iteration 1645, loss = 5.94571922\n",
      "Iteration 1646, loss = 5.94147436\n",
      "Iteration 1647, loss = 5.93723039\n",
      "Iteration 1648, loss = 5.93298733\n",
      "Iteration 1649, loss = 5.92874522\n",
      "Iteration 1650, loss = 5.92450409\n",
      "Iteration 1651, loss = 5.92026392\n",
      "Iteration 1652, loss = 5.91602478\n",
      "Iteration 1653, loss = 5.91178667\n",
      "Iteration 1654, loss = 5.90754960\n",
      "Iteration 1655, loss = 5.90331363\n",
      "Iteration 1656, loss = 5.89907875\n",
      "Iteration 1657, loss = 5.89484499\n",
      "Iteration 1658, loss = 5.89061239\n",
      "Iteration 1659, loss = 5.88638096\n",
      "Iteration 1660, loss = 5.88215072\n",
      "Iteration 1661, loss = 5.87792170\n",
      "Iteration 1662, loss = 5.87369392\n",
      "Iteration 1663, loss = 5.86946743\n",
      "Iteration 1664, loss = 5.86524222\n",
      "Iteration 1665, loss = 5.86101833\n",
      "Iteration 1666, loss = 5.85679580\n",
      "Iteration 1667, loss = 5.85257465\n",
      "Iteration 1668, loss = 5.84835493\n",
      "Iteration 1669, loss = 5.84413660\n",
      "Iteration 1670, loss = 5.83991977\n",
      "Iteration 1671, loss = 5.83570440\n",
      "Iteration 1672, loss = 5.83149056\n",
      "Iteration 1673, loss = 5.82727830\n",
      "Iteration 1674, loss = 5.82306759\n",
      "Iteration 1675, loss = 5.81885853\n",
      "Iteration 1676, loss = 5.81465109\n",
      "Iteration 1677, loss = 5.81044533\n",
      "Iteration 1678, loss = 5.80624133\n",
      "Iteration 1679, loss = 5.80203905\n",
      "Iteration 1680, loss = 5.79783854\n",
      "Iteration 1681, loss = 5.79363990\n",
      "Iteration 1682, loss = 5.78944311\n",
      "Iteration 1683, loss = 5.78524820\n",
      "Iteration 1684, loss = 5.78105526\n",
      "Iteration 1685, loss = 5.77686426\n",
      "Iteration 1686, loss = 5.77267533\n",
      "Iteration 1687, loss = 5.76848848\n",
      "Iteration 1688, loss = 5.76430369\n",
      "Iteration 1689, loss = 5.76012108\n",
      "Iteration 1690, loss = 5.75594067\n",
      "Iteration 1691, loss = 5.75176252\n",
      "Iteration 1692, loss = 5.74758668\n",
      "Iteration 1693, loss = 5.74341324\n",
      "Iteration 1694, loss = 5.73924212\n",
      "Iteration 1695, loss = 5.73507351\n",
      "Iteration 1696, loss = 5.73090731\n",
      "Iteration 1697, loss = 5.72674382\n",
      "Iteration 1698, loss = 5.72258286\n",
      "Iteration 1699, loss = 5.71842466\n",
      "Iteration 1700, loss = 5.71426908\n",
      "Iteration 1701, loss = 5.71011637\n",
      "Iteration 1702, loss = 5.70596661\n",
      "Iteration 1703, loss = 5.70181980\n",
      "Iteration 1704, loss = 5.69767586\n",
      "Iteration 1705, loss = 5.69353515\n",
      "Iteration 1706, loss = 5.68939754\n",
      "Iteration 1707, loss = 5.68526308\n",
      "Iteration 1708, loss = 5.68113202\n",
      "Iteration 1709, loss = 5.67700438\n",
      "Iteration 1710, loss = 5.67288019\n",
      "Iteration 1711, loss = 5.66875955\n",
      "Iteration 1712, loss = 5.66464261\n",
      "Iteration 1713, loss = 5.66052936\n",
      "Iteration 1714, loss = 5.65642008\n",
      "Iteration 1715, loss = 5.65231464\n",
      "Iteration 1716, loss = 5.64821321\n",
      "Iteration 1717, loss = 5.64411599\n",
      "Iteration 1718, loss = 5.64002275\n",
      "Iteration 1719, loss = 5.63593406\n",
      "Iteration 1720, loss = 5.63185007\n",
      "Iteration 1721, loss = 5.62777040\n",
      "Iteration 1722, loss = 5.62369555\n",
      "Iteration 1723, loss = 5.61962520\n",
      "Iteration 1724, loss = 5.61556000\n",
      "Iteration 1725, loss = 5.61150002\n",
      "Iteration 1726, loss = 5.60744507\n",
      "Iteration 1727, loss = 5.60339567\n",
      "Iteration 1728, loss = 5.59935168\n",
      "Iteration 1729, loss = 5.59531297\n",
      "Iteration 1730, loss = 5.59128028\n",
      "Iteration 1731, loss = 5.58725360\n",
      "Iteration 1732, loss = 5.58323296\n",
      "Iteration 1733, loss = 5.57921840\n",
      "Iteration 1734, loss = 5.57521052\n",
      "Iteration 1735, loss = 5.57120897\n",
      "Iteration 1736, loss = 5.56721394\n",
      "Iteration 1737, loss = 5.56322624\n",
      "Iteration 1738, loss = 5.55924503\n",
      "Iteration 1739, loss = 5.55527124\n",
      "Iteration 1740, loss = 5.55130535\n",
      "Iteration 1741, loss = 5.54734651\n",
      "Iteration 1742, loss = 5.54339524\n",
      "Iteration 1743, loss = 5.53945213\n",
      "Iteration 1744, loss = 5.53551713\n",
      "Iteration 1745, loss = 5.53159101\n",
      "Iteration 1746, loss = 5.52767298\n",
      "Iteration 1747, loss = 5.52376301\n",
      "Iteration 1748, loss = 5.51986202\n",
      "Iteration 1749, loss = 5.51597015\n",
      "Iteration 1750, loss = 5.51208757\n",
      "Iteration 1751, loss = 5.50821450\n",
      "Iteration 1752, loss = 5.50435121\n",
      "Iteration 1753, loss = 5.50049685\n",
      "Iteration 1754, loss = 5.49665289\n",
      "Iteration 1755, loss = 5.49281852\n",
      "Iteration 1756, loss = 5.48899547\n",
      "Iteration 1757, loss = 5.48518161\n",
      "Iteration 1758, loss = 5.48137880\n",
      "Iteration 1759, loss = 5.47758625\n",
      "Iteration 1760, loss = 5.47380615\n",
      "Iteration 1761, loss = 5.47003454\n",
      "Iteration 1762, loss = 5.46627536\n",
      "Iteration 1763, loss = 5.46252788\n",
      "Iteration 1764, loss = 5.45879127\n",
      "Iteration 1765, loss = 5.45506454\n",
      "Iteration 1766, loss = 5.45135067\n",
      "Iteration 1767, loss = 5.44764892\n",
      "Iteration 1768, loss = 5.44395842\n",
      "Iteration 1769, loss = 5.44028048\n",
      "Iteration 1770, loss = 5.43661186\n",
      "Iteration 1771, loss = 5.43295632\n",
      "Iteration 1772, loss = 5.42931307\n",
      "Iteration 1773, loss = 5.42568395\n",
      "Iteration 1774, loss = 5.42206253\n",
      "Iteration 1775, loss = 5.41845624\n",
      "Iteration 1776, loss = 5.41486141\n",
      "Iteration 1777, loss = 5.41127696\n",
      "Iteration 1778, loss = 5.40770508\n",
      "Iteration 1779, loss = 5.40414839\n",
      "Iteration 1780, loss = 5.40059893\n",
      "Iteration 1781, loss = 5.39706276\n",
      "Iteration 1782, loss = 5.39353906\n",
      "Iteration 1783, loss = 5.39002688\n",
      "Iteration 1784, loss = 5.38652519\n",
      "Iteration 1785, loss = 5.38303733\n",
      "Iteration 1786, loss = 5.37955784\n",
      "Iteration 1787, loss = 5.37609020\n",
      "Iteration 1788, loss = 5.37263857\n",
      "Iteration 1789, loss = 5.36919184\n",
      "Iteration 1790, loss = 5.36575928\n",
      "Iteration 1791, loss = 5.36234024\n",
      "Iteration 1792, loss = 5.35892801\n",
      "Iteration 1793, loss = 5.35552728\n",
      "Iteration 1794, loss = 5.35213714\n",
      "Iteration 1795, loss = 5.34875656\n",
      "Iteration 1796, loss = 5.34538440\n",
      "Iteration 1797, loss = 5.34201942\n",
      "Iteration 1798, loss = 5.33867555\n",
      "Iteration 1799, loss = 5.33532928\n",
      "Iteration 1800, loss = 5.33200291\n",
      "Iteration 1801, loss = 5.32867949\n",
      "Iteration 1802, loss = 5.32536617\n",
      "Iteration 1803, loss = 5.32206203\n",
      "Iteration 1804, loss = 5.31876611\n",
      "Iteration 1805, loss = 5.31547731\n",
      "Iteration 1806, loss = 5.31219447\n",
      "Iteration 1807, loss = 5.30892720\n",
      "Iteration 1808, loss = 5.30566464\n",
      "Iteration 1809, loss = 5.30241704\n",
      "Iteration 1810, loss = 5.29917190\n",
      "Iteration 1811, loss = 5.29592749\n",
      "Iteration 1812, loss = 5.29270844\n",
      "Iteration 1813, loss = 5.28947455\n",
      "Iteration 1814, loss = 5.28626508\n",
      "Iteration 1815, loss = 5.28305139\n",
      "Iteration 1816, loss = 5.27986206\n",
      "Iteration 1817, loss = 5.27666607\n",
      "Iteration 1818, loss = 5.27347755\n",
      "Iteration 1819, loss = 5.27029443\n",
      "Iteration 1820, loss = 5.26711597\n",
      "Iteration 1821, loss = 5.26394129\n",
      "Iteration 1822, loss = 5.26078893\n",
      "Iteration 1823, loss = 5.25761963\n",
      "Iteration 1824, loss = 5.25447204\n",
      "Iteration 1825, loss = 5.25130363\n",
      "Iteration 1826, loss = 5.24815596\n",
      "Iteration 1827, loss = 5.24503016\n",
      "Iteration 1828, loss = 5.24187875\n",
      "Iteration 1829, loss = 5.23874814\n",
      "Iteration 1830, loss = 5.23561303\n",
      "Iteration 1831, loss = 5.23249895\n",
      "Iteration 1832, loss = 5.22937854\n",
      "Iteration 1833, loss = 5.22627931\n",
      "Iteration 1834, loss = 5.22317166\n",
      "Iteration 1835, loss = 5.22005326\n",
      "Iteration 1836, loss = 5.21695475\n",
      "Iteration 1837, loss = 5.21384282\n",
      "Iteration 1838, loss = 5.21075040\n",
      "Iteration 1839, loss = 5.20767859\n",
      "Iteration 1840, loss = 5.20459001\n",
      "Iteration 1841, loss = 5.20148148\n",
      "Iteration 1842, loss = 5.19843263\n",
      "Iteration 1843, loss = 5.19536300\n",
      "Iteration 1844, loss = 5.19226889\n",
      "Iteration 1845, loss = 5.18919291\n",
      "Iteration 1846, loss = 5.18613609\n",
      "Iteration 1847, loss = 5.18304944\n",
      "Iteration 1848, loss = 5.18003251\n",
      "Iteration 1849, loss = 5.17693017\n",
      "Iteration 1850, loss = 5.17389972\n",
      "Iteration 1851, loss = 5.17083208\n",
      "Iteration 1852, loss = 5.16778230\n",
      "Iteration 1853, loss = 5.16475146\n",
      "Iteration 1854, loss = 5.16174078\n",
      "Iteration 1855, loss = 5.15868371\n",
      "Iteration 1856, loss = 5.15564416\n",
      "Iteration 1857, loss = 5.15262323\n",
      "Iteration 1858, loss = 5.14954629\n",
      "Iteration 1859, loss = 5.14656320\n",
      "Iteration 1860, loss = 5.14352064\n",
      "Iteration 1861, loss = 5.14049546\n",
      "Iteration 1862, loss = 5.13748875\n",
      "Iteration 1863, loss = 5.13441035\n",
      "Iteration 1864, loss = 5.13144064\n",
      "Iteration 1865, loss = 5.12839461\n",
      "Iteration 1866, loss = 5.12536460\n",
      "Iteration 1867, loss = 5.12235160\n",
      "Iteration 1868, loss = 5.11935674\n",
      "Iteration 1869, loss = 5.11638127\n",
      "Iteration 1870, loss = 5.11330778\n",
      "Iteration 1871, loss = 5.11037027\n",
      "Iteration 1872, loss = 5.10732810\n",
      "Iteration 1873, loss = 5.10430095\n",
      "Iteration 1874, loss = 5.10128974\n",
      "Iteration 1875, loss = 5.09829550\n",
      "Iteration 1876, loss = 5.09531941\n",
      "Iteration 1877, loss = 5.09236276\n",
      "Iteration 1878, loss = 5.08926718\n",
      "Iteration 1879, loss = 5.08634688\n",
      "Iteration 1880, loss = 5.08327818\n",
      "Iteration 1881, loss = 5.08039933\n",
      "Iteration 1882, loss = 5.07736130\n",
      "Iteration 1883, loss = 5.07433711\n",
      "Iteration 1884, loss = 5.07132761\n",
      "Iteration 1885, loss = 5.06833373\n",
      "Iteration 1886, loss = 5.06535653\n",
      "Iteration 1887, loss = 5.06239719\n",
      "Iteration 1888, loss = 5.05945702\n",
      "Iteration 1889, loss = 5.05653752\n",
      "Iteration 1890, loss = 5.05339070\n",
      "Iteration 1891, loss = 5.05050613\n",
      "Iteration 1892, loss = 5.04737920\n",
      "Iteration 1893, loss = 5.04453423\n",
      "Iteration 1894, loss = 5.04142975\n",
      "Iteration 1895, loss = 5.03863016\n",
      "Iteration 1896, loss = 5.03555135\n",
      "Iteration 1897, loss = 5.03248237\n",
      "Iteration 1898, loss = 5.02975540\n",
      "Iteration 1899, loss = 5.02671780\n",
      "Iteration 1900, loss = 5.02369228\n",
      "Iteration 1901, loss = 5.02067956\n",
      "Iteration 1902, loss = 5.01768041\n",
      "Iteration 1903, loss = 5.01469569\n",
      "Iteration 1904, loss = 5.01172637\n",
      "Iteration 1905, loss = 5.00877350\n",
      "Iteration 1906, loss = 5.00583830\n",
      "Iteration 1907, loss = 5.00292209\n",
      "Iteration 1908, loss = 5.00002639\n",
      "Iteration 1909, loss = 4.99715293\n",
      "Iteration 1910, loss = 4.99430366\n",
      "Iteration 1911, loss = 4.99093872\n",
      "Iteration 1912, loss = 4.98811711\n",
      "Iteration 1913, loss = 4.98532454\n",
      "Iteration 1914, loss = 4.98256403\n",
      "Iteration 1915, loss = 4.97920401\n",
      "Iteration 1916, loss = 4.97648033\n",
      "Iteration 1917, loss = 4.97379643\n",
      "Iteration 1918, loss = 4.97044019\n",
      "Iteration 1919, loss = 4.96780233\n",
      "Iteration 1920, loss = 4.96444867\n",
      "Iteration 1921, loss = 4.96186292\n",
      "Iteration 1922, loss = 4.95851191\n",
      "Iteration 1923, loss = 4.95598566\n",
      "Iteration 1924, loss = 4.95263736\n",
      "Iteration 1925, loss = 4.95017980\n",
      "Iteration 1926, loss = 4.94683429\n",
      "Iteration 1927, loss = 4.94445700\n",
      "Iteration 1928, loss = 4.94111437\n",
      "Iteration 1929, loss = 4.93777321\n",
      "Iteration 1930, loss = 4.93549256\n",
      "Iteration 1931, loss = 4.93215444\n",
      "Iteration 1932, loss = 4.92998855\n",
      "Iteration 1933, loss = 4.92665358\n",
      "Iteration 1934, loss = 4.92332023\n",
      "Iteration 1935, loss = 4.92129724\n",
      "Iteration 1936, loss = 4.91796724\n",
      "Iteration 1937, loss = 4.91463897\n",
      "Iteration 1938, loss = 4.91131246\n",
      "Iteration 1939, loss = 4.90947145\n",
      "Iteration 1940, loss = 4.90614860\n",
      "Iteration 1941, loss = 4.90282764\n",
      "Iteration 1942, loss = 4.89950861\n",
      "Iteration 1943, loss = 4.89790436\n",
      "Iteration 1944, loss = 4.89458935\n",
      "Iteration 1945, loss = 4.89127642\n",
      "Iteration 1946, loss = 4.88796562\n",
      "Iteration 1947, loss = 4.88668281\n",
      "Iteration 1948, loss = 4.88337644\n",
      "Iteration 1949, loss = 4.88007236\n",
      "Iteration 1950, loss = 4.87677065\n",
      "Iteration 1951, loss = 4.87347135\n",
      "Iteration 1952, loss = 4.87017453\n",
      "Iteration 1953, loss = 4.86935963\n",
      "Iteration 1954, loss = 4.86606797\n",
      "Iteration 1955, loss = 4.86277899\n",
      "Iteration 1956, loss = 4.85949275\n",
      "Iteration 1957, loss = 4.85620934\n",
      "Iteration 1958, loss = 4.85292882\n",
      "Iteration 1959, loss = 4.84965127\n",
      "Iteration 1960, loss = 4.84957324\n",
      "Iteration 1961, loss = 4.84630187\n",
      "Iteration 1962, loss = 4.84303371\n",
      "Iteration 1963, loss = 4.83976884\n",
      "Iteration 1964, loss = 4.83650735\n",
      "Iteration 1965, loss = 4.83324933\n",
      "Iteration 1966, loss = 4.82999485\n",
      "Iteration 1967, loss = 4.82674402\n",
      "Iteration 1968, loss = 4.82349693\n",
      "Iteration 1969, loss = 4.82475883\n",
      "Iteration 1970, loss = 4.82151949\n",
      "Iteration 1971, loss = 4.81828417\n",
      "Iteration 1972, loss = 4.81505297\n",
      "Iteration 1973, loss = 4.81182599\n",
      "Iteration 1974, loss = 4.80860333\n",
      "Iteration 1975, loss = 4.80538510\n",
      "Iteration 1976, loss = 4.80217139\n",
      "Iteration 1977, loss = 4.79896232\n",
      "Iteration 1978, loss = 4.79575798\n",
      "Iteration 1979, loss = 4.79255850\n",
      "Iteration 1980, loss = 4.78936397\n",
      "Iteration 1981, loss = 4.78617450\n",
      "Iteration 1982, loss = 4.78299021\n",
      "Iteration 1983, loss = 4.78751283\n",
      "Iteration 1984, loss = 4.78433922\n",
      "Iteration 1985, loss = 4.78117112\n",
      "Iteration 1986, loss = 4.77800864\n",
      "Iteration 1987, loss = 4.77485188\n",
      "Iteration 1988, loss = 4.77170098\n",
      "Iteration 1989, loss = 4.76855603\n",
      "Iteration 1990, loss = 4.76541715\n",
      "Iteration 1991, loss = 4.76228445\n",
      "Iteration 1992, loss = 4.75915805\n",
      "Iteration 1993, loss = 4.75603805\n",
      "Iteration 1994, loss = 4.75292456\n",
      "Iteration 1995, loss = 4.74981771\n",
      "Iteration 1996, loss = 4.74671759\n",
      "Iteration 1997, loss = 4.74362432\n",
      "Iteration 1998, loss = 4.74053799\n",
      "Iteration 1999, loss = 4.73745873\n",
      "Iteration 2000, loss = 4.73438663\n",
      "Iteration 2001, loss = 4.73132181\n",
      "Iteration 2002, loss = 4.72826435\n",
      "Iteration 2003, loss = 4.72521437\n",
      "Iteration 2004, loss = 4.72217843\n",
      "Iteration 2005, loss = 4.71916544\n",
      "Iteration 2006, loss = 4.71616169\n",
      "Iteration 2007, loss = 4.71316712\n",
      "Iteration 2008, loss = 4.71018169\n",
      "Iteration 2009, loss = 4.70720537\n",
      "Iteration 2010, loss = 4.70423812\n",
      "Iteration 2011, loss = 4.70127994\n",
      "Iteration 2012, loss = 4.69833082\n",
      "Iteration 2013, loss = 4.69539076\n",
      "Iteration 2014, loss = 4.69245977\n",
      "Iteration 2015, loss = 4.68953786\n",
      "Iteration 2016, loss = 4.68662505\n",
      "Iteration 2017, loss = 4.68372137\n",
      "Iteration 2018, loss = 4.68082683\n",
      "Iteration 2019, loss = 4.67794146\n",
      "Iteration 2020, loss = 4.67506530\n",
      "Iteration 2021, loss = 4.67219838\n",
      "Iteration 2022, loss = 4.66934073\n",
      "Iteration 2023, loss = 4.66649239\n",
      "Iteration 2024, loss = 4.66365340\n",
      "Iteration 2025, loss = 4.66082380\n",
      "Iteration 2026, loss = 4.65800362\n",
      "Iteration 2027, loss = 4.65519291\n",
      "Iteration 2028, loss = 4.65239170\n",
      "Iteration 2029, loss = 4.64960004\n",
      "Iteration 2030, loss = 4.64681796\n",
      "Iteration 2031, loss = 4.64404551\n",
      "Iteration 2032, loss = 4.64128273\n",
      "Iteration 2033, loss = 4.63852966\n",
      "Iteration 2034, loss = 4.63578634\n",
      "Iteration 2035, loss = 4.63305281\n",
      "Iteration 2036, loss = 4.63032911\n",
      "Iteration 2037, loss = 4.62761529\n",
      "Iteration 2038, loss = 4.62491137\n",
      "Iteration 2039, loss = 4.62221740\n",
      "Iteration 2040, loss = 4.61953343\n",
      "Iteration 2041, loss = 4.61685948\n",
      "Iteration 2042, loss = 4.61419559\n",
      "Iteration 2043, loss = 4.61154180\n",
      "Iteration 2044, loss = 4.60889816\n",
      "Iteration 2045, loss = 4.60626469\n",
      "Iteration 2046, loss = 4.60364144\n",
      "Iteration 2047, loss = 4.60102844\n",
      "Iteration 2048, loss = 4.59842573\n",
      "Iteration 2049, loss = 4.59583334\n",
      "Iteration 2050, loss = 4.59325130\n",
      "Iteration 2051, loss = 4.59067964\n",
      "Iteration 2052, loss = 4.58811840\n",
      "Iteration 2053, loss = 4.58556761\n",
      "Iteration 2054, loss = 4.58302729\n",
      "Iteration 2055, loss = 4.58049748\n",
      "Iteration 2056, loss = 4.57797819\n",
      "Iteration 2057, loss = 4.57546946\n",
      "Iteration 2058, loss = 4.57297131\n",
      "Iteration 2059, loss = 4.57048375\n",
      "Iteration 2060, loss = 4.56800682\n",
      "Iteration 2061, loss = 4.56554052\n",
      "Iteration 2062, loss = 4.56308488\n",
      "Iteration 2063, loss = 4.56063990\n",
      "Iteration 2064, loss = 4.55820561\n",
      "Iteration 2065, loss = 4.55578201\n",
      "Iteration 2066, loss = 4.55336910\n",
      "Iteration 2067, loss = 4.55096690\n",
      "Iteration 2068, loss = 4.54857541\n",
      "Iteration 2069, loss = 4.54619463\n",
      "Iteration 2070, loss = 4.54382455\n",
      "Iteration 2071, loss = 4.54146518\n",
      "Iteration 2072, loss = 4.53911650\n",
      "Iteration 2073, loss = 4.53677851\n",
      "Iteration 2074, loss = 4.53445119\n",
      "Iteration 2075, loss = 4.53213454\n",
      "Iteration 2076, loss = 4.52982852\n",
      "Iteration 2077, loss = 4.52753313\n",
      "Iteration 2078, loss = 4.52524833\n",
      "Iteration 2079, loss = 4.52297411\n",
      "Iteration 2080, loss = 4.52071044\n",
      "Iteration 2081, loss = 4.51845728\n",
      "Iteration 2082, loss = 4.51621460\n",
      "Iteration 2083, loss = 4.51398236\n",
      "Iteration 2084, loss = 4.51176053\n",
      "Iteration 2085, loss = 4.50954907\n",
      "Iteration 2086, loss = 4.50734792\n",
      "Iteration 2087, loss = 4.50515706\n",
      "Iteration 2088, loss = 4.50297642\n",
      "Iteration 2089, loss = 4.50080595\n",
      "Iteration 2090, loss = 4.49864562\n",
      "Iteration 2091, loss = 4.49649535\n",
      "Iteration 2092, loss = 4.49435511\n",
      "Iteration 2093, loss = 4.49222481\n",
      "Iteration 2094, loss = 4.49010442\n",
      "Iteration 2095, loss = 4.48799387\n",
      "Iteration 2096, loss = 4.48589309\n",
      "Iteration 2097, loss = 4.48380202\n",
      "Iteration 2098, loss = 4.48172060\n",
      "Iteration 2099, loss = 4.47964877\n",
      "Iteration 2100, loss = 4.47758644\n",
      "Iteration 2101, loss = 4.47553357\n",
      "Iteration 2102, loss = 4.47349007\n",
      "Iteration 2103, loss = 4.47145588\n",
      "Iteration 2104, loss = 4.46943093\n",
      "Iteration 2105, loss = 4.46741515\n",
      "Iteration 2106, loss = 4.46540847\n",
      "Iteration 2107, loss = 4.46341082\n",
      "Iteration 2108, loss = 4.46142213\n",
      "Iteration 2109, loss = 4.45944233\n",
      "Iteration 2110, loss = 4.45747134\n",
      "Iteration 2111, loss = 4.45550911\n",
      "Iteration 2112, loss = 4.45355554\n",
      "Iteration 2113, loss = 4.45161059\n",
      "Iteration 2114, loss = 4.44967418\n",
      "Iteration 2115, loss = 4.44774623\n",
      "Iteration 2116, loss = 4.44582668\n",
      "Iteration 2117, loss = 4.44391547\n",
      "Iteration 2118, loss = 4.44201252\n",
      "Iteration 2119, loss = 4.44011778\n",
      "Iteration 2120, loss = 4.43823116\n",
      "Iteration 2121, loss = 4.43635262\n",
      "Iteration 2122, loss = 4.43448208\n",
      "Iteration 2123, loss = 4.43261947\n",
      "Iteration 2124, loss = 4.43076475\n",
      "Iteration 2125, loss = 4.42891785\n",
      "Iteration 2126, loss = 4.42707870\n",
      "Iteration 2127, loss = 4.42524725\n",
      "Iteration 2128, loss = 4.42342344\n",
      "Iteration 2129, loss = 4.42160720\n",
      "Iteration 2130, loss = 4.41979849\n",
      "Iteration 2131, loss = 4.41799725\n",
      "Iteration 2132, loss = 4.41620342\n",
      "Iteration 2133, loss = 4.41441695\n",
      "Iteration 2134, loss = 4.41263804\n",
      "Iteration 2135, loss = 4.41086766\n",
      "Iteration 2136, loss = 4.40910452\n",
      "Iteration 2137, loss = 4.40734855\n",
      "Iteration 2138, loss = 4.40559971\n",
      "Iteration 2139, loss = 4.40385791\n",
      "Iteration 2140, loss = 4.40212312\n",
      "Iteration 2141, loss = 4.40039528\n",
      "Iteration 2142, loss = 4.39867432\n",
      "Iteration 2143, loss = 4.39696021\n",
      "Iteration 2144, loss = 4.39525290\n",
      "Iteration 2145, loss = 4.39355233\n",
      "Iteration 2146, loss = 4.39185846\n",
      "Iteration 2147, loss = 4.39017124\n",
      "Iteration 2148, loss = 4.38849064\n",
      "Iteration 2149, loss = 4.38681661\n",
      "Iteration 2150, loss = 4.38514910\n",
      "Iteration 2151, loss = 4.38348808\n",
      "Iteration 2152, loss = 4.38183351\n",
      "Iteration 2153, loss = 4.38018535\n",
      "Iteration 2154, loss = 4.37854356\n",
      "Iteration 2155, loss = 4.37690811\n",
      "Iteration 2156, loss = 4.37527895\n",
      "Iteration 2157, loss = 4.37365605\n",
      "Iteration 2158, loss = 4.37203937\n",
      "Iteration 2159, loss = 4.37042889\n",
      "Iteration 2160, loss = 4.36882456\n",
      "Iteration 2161, loss = 4.36722634\n",
      "Iteration 2162, loss = 4.36563421\n",
      "Iteration 2163, loss = 4.36404813\n",
      "Iteration 2164, loss = 4.36246807\n",
      "Iteration 2165, loss = 4.36089399\n",
      "Iteration 2166, loss = 4.35932585\n",
      "Iteration 2167, loss = 4.35776364\n",
      "Iteration 2168, loss = 4.35620730\n",
      "Iteration 2169, loss = 4.35465681\n",
      "Iteration 2170, loss = 4.35311213\n",
      "Iteration 2171, loss = 4.35157324\n",
      "Iteration 2172, loss = 4.35004009\n",
      "Iteration 2173, loss = 4.34851266\n",
      "Iteration 2174, loss = 4.34699091\n",
      "Iteration 2175, loss = 4.34547480\n",
      "Iteration 2176, loss = 4.34396223\n",
      "Iteration 2177, loss = 4.34242690\n",
      "Iteration 2178, loss = 4.34089556\n",
      "Iteration 2179, loss = 4.33936845\n",
      "Iteration 2180, loss = 4.33784578\n",
      "Iteration 2181, loss = 4.33632774\n",
      "Iteration 2182, loss = 4.33481448\n",
      "Iteration 2183, loss = 4.33330614\n",
      "Iteration 2184, loss = 4.33180285\n",
      "Iteration 2185, loss = 4.33030472\n",
      "Iteration 2186, loss = 4.32881183\n",
      "Iteration 2187, loss = 4.32732427\n",
      "Iteration 2188, loss = 4.32584208\n",
      "Iteration 2189, loss = 4.32436534\n",
      "Iteration 2190, loss = 4.32289407\n",
      "Iteration 2191, loss = 4.32142831\n",
      "Iteration 2192, loss = 4.31996809\n",
      "Iteration 2193, loss = 4.31851342\n",
      "Iteration 2194, loss = 4.31706432\n",
      "Iteration 2195, loss = 4.31562078\n",
      "Iteration 2196, loss = 4.31418280\n",
      "Iteration 2197, loss = 4.31275038\n",
      "Iteration 2198, loss = 4.31132351\n",
      "Iteration 2199, loss = 4.30990216\n",
      "Iteration 2200, loss = 4.30848633\n",
      "Iteration 2201, loss = 4.30707598\n",
      "Iteration 2202, loss = 4.30567108\n",
      "Iteration 2203, loss = 4.30427162\n",
      "Iteration 2204, loss = 4.30287755\n",
      "Iteration 2205, loss = 4.30148884\n",
      "Iteration 2206, loss = 4.29996300\n",
      "Iteration 2207, loss = 4.29839819\n",
      "Iteration 2208, loss = 4.29683893\n",
      "Iteration 2209, loss = 4.29528519\n",
      "Iteration 2210, loss = 4.29373692\n",
      "Iteration 2211, loss = 4.29219408\n",
      "Iteration 2212, loss = 4.29065662\n",
      "Iteration 2213, loss = 4.28912449\n",
      "Iteration 2214, loss = 4.28759765\n",
      "Iteration 2215, loss = 4.28607033\n",
      "Iteration 2216, loss = 4.28454618\n",
      "Iteration 2217, loss = 4.28302624\n",
      "Iteration 2218, loss = 4.28151055\n",
      "Iteration 2219, loss = 4.27999917\n",
      "Iteration 2220, loss = 4.27849214\n",
      "Iteration 2221, loss = 4.27698948\n",
      "Iteration 2222, loss = 4.27549121\n",
      "Iteration 2223, loss = 4.27399734\n",
      "Iteration 2224, loss = 4.27250788\n",
      "Iteration 2225, loss = 4.27102283\n",
      "Iteration 2226, loss = 4.26954219\n",
      "Iteration 2227, loss = 4.26806594\n",
      "Iteration 2228, loss = 4.26659343\n",
      "Iteration 2229, loss = 4.26512452\n",
      "Iteration 2230, loss = 4.26365992\n",
      "Iteration 2231, loss = 4.26219960\n",
      "Iteration 2232, loss = 4.26074355\n",
      "Iteration 2233, loss = 4.25929176\n",
      "Iteration 2234, loss = 4.25784419\n",
      "Iteration 2235, loss = 4.25640083\n",
      "Iteration 2236, loss = 4.25496164\n",
      "Iteration 2237, loss = 4.25352658\n",
      "Iteration 2238, loss = 4.25208721\n",
      "Iteration 2239, loss = 4.25065035\n",
      "Iteration 2240, loss = 4.24921656\n",
      "Iteration 2241, loss = 4.24778592\n",
      "Iteration 2242, loss = 4.24635848\n",
      "Iteration 2243, loss = 4.24493430\n",
      "Iteration 2244, loss = 4.24351340\n",
      "Iteration 2245, loss = 4.24209583\n",
      "Iteration 2246, loss = 4.24068161\n",
      "Iteration 2247, loss = 4.23927075\n",
      "Iteration 2248, loss = 4.23786326\n",
      "Iteration 2249, loss = 4.23645914\n",
      "Iteration 2250, loss = 4.23505840\n",
      "Iteration 2251, loss = 4.23366104\n",
      "Iteration 2252, loss = 4.23226703\n",
      "Iteration 2253, loss = 4.23087637\n",
      "Iteration 2254, loss = 4.22948905\n",
      "Iteration 2255, loss = 4.22810505\n",
      "Iteration 2256, loss = 4.22672435\n",
      "Iteration 2257, loss = 4.22534692\n",
      "Iteration 2258, loss = 4.22397275\n",
      "Iteration 2259, loss = 4.22260181\n",
      "Iteration 2260, loss = 4.22123407\n",
      "Iteration 2261, loss = 4.21986950\n",
      "Iteration 2262, loss = 4.21850808\n",
      "Iteration 2263, loss = 4.21714978\n",
      "Iteration 2264, loss = 4.21579457\n",
      "Iteration 2265, loss = 4.21444242\n",
      "Iteration 2266, loss = 4.21309330\n",
      "Iteration 2267, loss = 4.21174718\n",
      "Iteration 2268, loss = 4.21040402\n",
      "Iteration 2269, loss = 4.20906380\n",
      "Iteration 2270, loss = 4.20772649\n",
      "Iteration 2271, loss = 4.20639205\n",
      "Iteration 2272, loss = 4.20506045\n",
      "Iteration 2273, loss = 4.20372622\n",
      "Iteration 2274, loss = 4.20239382\n",
      "Iteration 2275, loss = 4.20106379\n",
      "Iteration 2276, loss = 4.19973617\n",
      "Iteration 2277, loss = 4.19841097\n",
      "Iteration 2278, loss = 4.19708820\n",
      "Iteration 2279, loss = 4.19576787\n",
      "Iteration 2280, loss = 4.19445000\n",
      "Iteration 2281, loss = 4.19313458\n",
      "Iteration 2282, loss = 4.19182161\n",
      "Iteration 2283, loss = 4.19051109\n",
      "Iteration 2284, loss = 4.18920302\n",
      "Iteration 2285, loss = 4.18789738\n",
      "Iteration 2286, loss = 4.18659417\n",
      "Iteration 2287, loss = 4.18529338\n",
      "Iteration 2288, loss = 4.18399499\n",
      "Iteration 2289, loss = 4.18269900\n",
      "Iteration 2290, loss = 4.18140537\n",
      "Iteration 2291, loss = 4.18011411\n",
      "Iteration 2292, loss = 4.17882519\n",
      "Iteration 2293, loss = 4.17753859\n",
      "Iteration 2294, loss = 4.17625431\n",
      "Iteration 2295, loss = 4.17497231\n",
      "Iteration 2296, loss = 4.17369258\n",
      "Iteration 2297, loss = 4.17241510\n",
      "Iteration 2298, loss = 4.17113985\n",
      "Iteration 2299, loss = 4.16986681\n",
      "Iteration 2300, loss = 4.16859597\n",
      "Iteration 2301, loss = 4.16732729\n",
      "Iteration 2302, loss = 4.16606077\n",
      "Iteration 2303, loss = 4.16479637\n",
      "Iteration 2304, loss = 4.16353409\n",
      "Iteration 2305, loss = 4.16227390\n",
      "Iteration 2306, loss = 4.16101577\n",
      "Iteration 2307, loss = 4.15975970\n",
      "Iteration 2308, loss = 4.15850566\n",
      "Iteration 2309, loss = 4.15725362\n",
      "Iteration 2310, loss = 4.15600358\n",
      "Iteration 2311, loss = 4.15475551\n",
      "Iteration 2312, loss = 4.15350938\n",
      "Iteration 2313, loss = 4.15226519\n",
      "Iteration 2314, loss = 4.15102291\n",
      "Iteration 2315, loss = 4.14978253\n",
      "Iteration 2316, loss = 4.14854402\n",
      "Iteration 2317, loss = 4.14730736\n",
      "Iteration 2318, loss = 4.14607254\n",
      "Iteration 2319, loss = 4.14483954\n",
      "Iteration 2320, loss = 4.14360834\n",
      "Iteration 2321, loss = 4.14237893\n",
      "Iteration 2322, loss = 4.14115127\n",
      "Iteration 2323, loss = 4.13992537\n",
      "Iteration 2324, loss = 4.13870119\n",
      "Iteration 2325, loss = 4.13747872\n",
      "Iteration 2326, loss = 4.13625795\n",
      "Iteration 2327, loss = 4.13496213\n",
      "Iteration 2328, loss = 4.13363493\n",
      "Iteration 2329, loss = 4.13230954\n",
      "Iteration 2330, loss = 4.13098594\n",
      "Iteration 2331, loss = 4.12966411\n",
      "Iteration 2332, loss = 4.12834404\n",
      "Iteration 2333, loss = 4.12702572\n",
      "Iteration 2334, loss = 4.12570912\n",
      "Iteration 2335, loss = 4.12439423\n",
      "Iteration 2336, loss = 4.12308103\n",
      "Iteration 2337, loss = 4.12176951\n",
      "Iteration 2338, loss = 4.12045965\n",
      "Iteration 2339, loss = 4.11915143\n",
      "Iteration 2340, loss = 4.11784485\n",
      "Iteration 2341, loss = 4.11653988\n",
      "Iteration 2342, loss = 4.11523651\n",
      "Iteration 2343, loss = 4.11393472\n",
      "Iteration 2344, loss = 4.11263451\n",
      "Iteration 2345, loss = 4.11133585\n",
      "Iteration 2346, loss = 4.11003874\n",
      "Iteration 2347, loss = 4.10874315\n",
      "Iteration 2348, loss = 4.10744907\n",
      "Iteration 2349, loss = 4.10615650\n",
      "Iteration 2350, loss = 4.10486541\n",
      "Iteration 2351, loss = 4.10357580\n",
      "Iteration 2352, loss = 4.10228764\n",
      "Iteration 2353, loss = 4.10100093\n",
      "Iteration 2354, loss = 4.09971565\n",
      "Iteration 2355, loss = 4.09843179\n",
      "Iteration 2356, loss = 4.09714934\n",
      "Iteration 2357, loss = 4.09586829\n",
      "Iteration 2358, loss = 4.09458862\n",
      "Iteration 2359, loss = 4.09331031\n",
      "Iteration 2360, loss = 4.09203337\n",
      "Iteration 2361, loss = 4.09075777\n",
      "Iteration 2362, loss = 4.08948350\n",
      "Iteration 2363, loss = 4.08821056\n",
      "Iteration 2364, loss = 4.08693892\n",
      "Iteration 2365, loss = 4.08566859\n",
      "Iteration 2366, loss = 4.08439954\n",
      "Iteration 2367, loss = 4.08313176\n",
      "Iteration 2368, loss = 4.08186526\n",
      "Iteration 2369, loss = 4.08060000\n",
      "Iteration 2370, loss = 4.07933599\n",
      "Iteration 2371, loss = 4.07807321\n",
      "Iteration 2372, loss = 4.07681164\n",
      "Iteration 2373, loss = 4.07555129\n",
      "Iteration 2374, loss = 4.07429214\n",
      "Iteration 2375, loss = 4.07303418\n",
      "Iteration 2376, loss = 4.07177740\n",
      "Iteration 2377, loss = 4.07052178\n",
      "Iteration 2378, loss = 4.06926733\n",
      "Iteration 2379, loss = 4.06801402\n",
      "Iteration 2380, loss = 4.06676185\n",
      "Iteration 2381, loss = 4.06551082\n",
      "Iteration 2382, loss = 4.06426090\n",
      "Iteration 2383, loss = 4.06301209\n",
      "Iteration 2384, loss = 4.06176438\n",
      "Iteration 2385, loss = 4.06051776\n",
      "Iteration 2386, loss = 4.05927222\n",
      "Iteration 2387, loss = 4.05802775\n",
      "Iteration 2388, loss = 4.05678435\n",
      "Iteration 2389, loss = 4.05554200\n",
      "Iteration 2390, loss = 4.05430070\n",
      "Iteration 2391, loss = 4.05306043\n",
      "Iteration 2392, loss = 4.05182119\n",
      "Iteration 2393, loss = 4.05058297\n",
      "Iteration 2394, loss = 4.04934576\n",
      "Iteration 2395, loss = 4.04810955\n",
      "Iteration 2396, loss = 4.04687434\n",
      "Iteration 2397, loss = 4.04564011\n",
      "Iteration 2398, loss = 4.04440685\n",
      "Iteration 2399, loss = 4.04317457\n",
      "Iteration 2400, loss = 4.04194324\n",
      "Iteration 2401, loss = 4.04071287\n",
      "Iteration 2402, loss = 4.03948344\n",
      "Iteration 2403, loss = 4.03825495\n",
      "Iteration 2404, loss = 4.03702738\n",
      "Iteration 2405, loss = 4.03580074\n",
      "Iteration 2406, loss = 4.03457501\n",
      "Iteration 2407, loss = 4.03335018\n",
      "Iteration 2408, loss = 4.03212626\n",
      "Iteration 2409, loss = 4.03090322\n",
      "Iteration 2410, loss = 4.02968107\n",
      "Iteration 2411, loss = 4.02845979\n",
      "Iteration 2412, loss = 4.02723938\n",
      "Iteration 2413, loss = 4.02601983\n",
      "Iteration 2414, loss = 4.02480114\n",
      "Iteration 2415, loss = 4.02358329\n",
      "Iteration 2416, loss = 4.02236629\n",
      "Iteration 2417, loss = 4.02115015\n",
      "Iteration 2418, loss = 4.01993483\n",
      "Iteration 2419, loss = 4.01872034\n",
      "Iteration 2420, loss = 4.01750667\n",
      "Iteration 2421, loss = 4.01629381\n",
      "Iteration 2422, loss = 4.01508174\n",
      "Iteration 2423, loss = 4.01387048\n",
      "Iteration 2424, loss = 4.01266000\n",
      "Iteration 2425, loss = 4.01145030\n",
      "Iteration 2426, loss = 4.01024138\n",
      "Iteration 2427, loss = 4.00903323\n",
      "Iteration 2428, loss = 4.00782584\n",
      "Iteration 2429, loss = 4.00661921\n",
      "Iteration 2430, loss = 4.00541333\n",
      "Iteration 2431, loss = 4.00420819\n",
      "Iteration 2432, loss = 4.00300379\n",
      "Iteration 2433, loss = 4.00180013\n",
      "Iteration 2434, loss = 4.00059719\n",
      "Iteration 2435, loss = 3.99939497\n",
      "Iteration 2436, loss = 3.99819347\n",
      "Iteration 2437, loss = 3.99699267\n",
      "Iteration 2438, loss = 3.99579258\n",
      "Iteration 2439, loss = 3.99459318\n",
      "Iteration 2440, loss = 3.99339448\n",
      "Iteration 2441, loss = 3.99219646\n",
      "Iteration 2442, loss = 3.99099913\n",
      "Iteration 2443, loss = 3.98980261\n",
      "Iteration 2444, loss = 3.98864822\n",
      "Iteration 2445, loss = 3.98749606\n",
      "Iteration 2446, loss = 3.98634596\n",
      "Iteration 2447, loss = 3.98519778\n",
      "Iteration 2448, loss = 3.98405136\n",
      "Iteration 2449, loss = 3.98290660\n",
      "Iteration 2450, loss = 3.98176339\n",
      "Iteration 2451, loss = 3.98062161\n",
      "Iteration 2452, loss = 3.97948118\n",
      "Iteration 2453, loss = 3.97834203\n",
      "Iteration 2454, loss = 3.97720407\n",
      "Iteration 2455, loss = 3.97606723\n",
      "Iteration 2456, loss = 3.97493146\n",
      "Iteration 2457, loss = 3.97379670\n",
      "Iteration 2458, loss = 3.97266291\n",
      "Iteration 2459, loss = 3.97153002\n",
      "Iteration 2460, loss = 3.97039801\n",
      "Iteration 2461, loss = 3.96926683\n",
      "Iteration 2462, loss = 3.96813646\n",
      "Iteration 2463, loss = 3.96700685\n",
      "Iteration 2464, loss = 3.96587798\n",
      "Iteration 2465, loss = 3.96474983\n",
      "Iteration 2466, loss = 3.96362236\n",
      "Iteration 2467, loss = 3.96249556\n",
      "Iteration 2468, loss = 3.96136941\n",
      "Iteration 2469, loss = 3.96024388\n",
      "Iteration 2470, loss = 3.95911897\n",
      "Iteration 2471, loss = 3.95799464\n",
      "Iteration 2472, loss = 3.95687090\n",
      "Iteration 2473, loss = 3.95574772\n",
      "Iteration 2474, loss = 3.95462510\n",
      "Iteration 2475, loss = 3.95350302\n",
      "Iteration 2476, loss = 3.95238160\n",
      "Iteration 2477, loss = 3.95126179\n",
      "Iteration 2478, loss = 3.95014259\n",
      "Iteration 2479, loss = 3.94902396\n",
      "Iteration 2480, loss = 3.94790589\n",
      "Iteration 2481, loss = 3.94678838\n",
      "Iteration 2482, loss = 3.94567139\n",
      "Iteration 2483, loss = 3.94455493\n",
      "Iteration 2484, loss = 3.94343897\n",
      "Iteration 2485, loss = 3.94232351\n",
      "Iteration 2486, loss = 3.94120853\n",
      "Iteration 2487, loss = 3.94009403\n",
      "Iteration 2488, loss = 3.93897999\n",
      "Iteration 2489, loss = 3.93786641\n",
      "Iteration 2490, loss = 3.93675328\n",
      "Iteration 2491, loss = 3.93564059\n",
      "Iteration 2492, loss = 3.93452834\n",
      "Iteration 2493, loss = 3.93341651\n",
      "Iteration 2494, loss = 3.93230510\n",
      "Iteration 2495, loss = 3.93119410\n",
      "Iteration 2496, loss = 3.93008352\n",
      "Iteration 2497, loss = 3.92897333\n",
      "Iteration 2498, loss = 3.92786355\n",
      "Iteration 2499, loss = 3.92675415\n",
      "Iteration 2500, loss = 3.92564514\n",
      "Iteration 2501, loss = 3.92453652\n",
      "Iteration 2502, loss = 3.92342827\n",
      "Iteration 2503, loss = 3.92232039\n",
      "Iteration 2504, loss = 3.92121289\n",
      "Iteration 2505, loss = 3.92010574\n",
      "Iteration 2506, loss = 3.91899896\n",
      "Iteration 2507, loss = 3.91789254\n",
      "Iteration 2508, loss = 3.91678646\n",
      "Iteration 2509, loss = 3.91568074\n",
      "Iteration 2510, loss = 3.91457536\n",
      "Iteration 2511, loss = 3.91347033\n",
      "Iteration 2512, loss = 3.91236563\n",
      "Iteration 2513, loss = 3.91126126\n",
      "Iteration 2514, loss = 3.91015723\n",
      "Iteration 2515, loss = 3.90905353\n",
      "Iteration 2516, loss = 3.90795015\n",
      "Iteration 2517, loss = 3.90684709\n",
      "Iteration 2518, loss = 3.90574435\n",
      "Iteration 2519, loss = 3.90464192\n",
      "Iteration 2520, loss = 3.90353981\n",
      "Iteration 2521, loss = 3.90243801\n",
      "Iteration 2522, loss = 3.90133651\n",
      "Iteration 2523, loss = 3.90023532\n",
      "Iteration 2524, loss = 3.89913443\n",
      "Iteration 2525, loss = 3.89803384\n",
      "Iteration 2526, loss = 3.89693354\n",
      "Iteration 2527, loss = 3.89583353\n",
      "Iteration 2528, loss = 3.89473381\n",
      "Iteration 2529, loss = 3.89363438\n",
      "Iteration 2530, loss = 3.89253524\n",
      "Iteration 2531, loss = 3.89143638\n",
      "Iteration 2532, loss = 3.89033779\n",
      "Iteration 2533, loss = 3.88923948\n",
      "Iteration 2534, loss = 3.88814145\n",
      "Iteration 2535, loss = 3.88704368\n",
      "Iteration 2536, loss = 3.88594619\n",
      "Iteration 2537, loss = 3.88484896\n",
      "Iteration 2538, loss = 3.88375200\n",
      "Iteration 2539, loss = 3.88265530\n",
      "Iteration 2540, loss = 3.88155885\n",
      "Iteration 2541, loss = 3.88046267\n",
      "Iteration 2542, loss = 3.87936673\n",
      "Iteration 2543, loss = 3.87827106\n",
      "Iteration 2544, loss = 3.87717563\n",
      "Iteration 2545, loss = 3.87608044\n",
      "Iteration 2546, loss = 3.87498551\n",
      "Iteration 2547, loss = 3.87389081\n",
      "Iteration 2548, loss = 3.87279636\n",
      "Iteration 2549, loss = 3.87170215\n",
      "Iteration 2550, loss = 3.87060817\n",
      "Iteration 2551, loss = 3.86951443\n",
      "Iteration 2552, loss = 3.86842092\n",
      "Iteration 2553, loss = 3.86732764\n",
      "Iteration 2554, loss = 3.86623459\n",
      "Iteration 2555, loss = 3.86514177\n",
      "Iteration 2556, loss = 3.86404917\n",
      "Iteration 2557, loss = 3.86295679\n",
      "Iteration 2558, loss = 3.86186463\n",
      "Iteration 2559, loss = 3.86077269\n",
      "Iteration 2560, loss = 3.85968097\n",
      "Iteration 2561, loss = 3.85858946\n",
      "Iteration 2562, loss = 3.85749816\n",
      "Iteration 2563, loss = 3.85640707\n",
      "Iteration 2564, loss = 3.85531620\n",
      "Iteration 2565, loss = 3.85422553\n",
      "Iteration 2566, loss = 3.85313506\n",
      "Iteration 2567, loss = 3.85204480\n",
      "Iteration 2568, loss = 3.85095474\n",
      "Iteration 2569, loss = 3.84981842\n",
      "Iteration 2570, loss = 3.84864082\n",
      "Iteration 2571, loss = 3.84746109\n",
      "Iteration 2572, loss = 3.84628113\n",
      "Iteration 2573, loss = 3.84510098\n",
      "Iteration 2574, loss = 3.84392068\n",
      "Iteration 2575, loss = 3.84274028\n",
      "Iteration 2576, loss = 3.84155981\n",
      "Iteration 2577, loss = 3.84037928\n",
      "Iteration 2578, loss = 3.83919874\n",
      "Iteration 2579, loss = 3.83801821\n",
      "Iteration 2580, loss = 3.83683770\n",
      "Iteration 2581, loss = 3.83565723\n",
      "Iteration 2582, loss = 3.83447683\n",
      "Iteration 2583, loss = 3.83329650\n",
      "Iteration 2584, loss = 3.83211626\n",
      "Iteration 2585, loss = 3.83093612\n",
      "Iteration 2586, loss = 3.82975609\n",
      "Iteration 2587, loss = 3.82857618\n",
      "Iteration 2588, loss = 3.82739641\n",
      "Iteration 2589, loss = 3.82621676\n",
      "Iteration 2590, loss = 3.82503726\n",
      "Iteration 2591, loss = 3.82385790\n",
      "Iteration 2592, loss = 3.82268508\n",
      "Iteration 2593, loss = 3.82151314\n",
      "Iteration 2594, loss = 3.82034178\n",
      "Iteration 2595, loss = 3.81917098\n",
      "Iteration 2596, loss = 3.81800070\n",
      "Iteration 2597, loss = 3.81683091\n",
      "Iteration 2598, loss = 3.81566159\n",
      "Iteration 2599, loss = 3.81449271\n",
      "Iteration 2600, loss = 3.81332425\n",
      "Iteration 2601, loss = 3.81215620\n",
      "Iteration 2602, loss = 3.81098852\n",
      "Iteration 2603, loss = 3.80982122\n",
      "Iteration 2604, loss = 3.80865427\n",
      "Iteration 2605, loss = 3.80748766\n",
      "Iteration 2606, loss = 3.80632137\n",
      "Iteration 2607, loss = 3.80515541\n",
      "Iteration 2608, loss = 3.80398975\n",
      "Iteration 2609, loss = 3.80282438\n",
      "Iteration 2610, loss = 3.80165930\n",
      "Iteration 2611, loss = 3.80049450\n",
      "Iteration 2612, loss = 3.79932997\n",
      "Iteration 2613, loss = 3.79816571\n",
      "Iteration 2614, loss = 3.79700170\n",
      "Iteration 2615, loss = 3.79583795\n",
      "Iteration 2616, loss = 3.79467444\n",
      "Iteration 2617, loss = 3.79351117\n",
      "Iteration 2618, loss = 3.79234814\n",
      "Iteration 2619, loss = 3.79118535\n",
      "Iteration 2620, loss = 3.79002277\n",
      "Iteration 2621, loss = 3.78886043\n",
      "Iteration 2622, loss = 3.78769830\n",
      "Iteration 2623, loss = 3.78653638\n",
      "Iteration 2624, loss = 3.78537468\n",
      "Iteration 2625, loss = 3.78421319\n",
      "Iteration 2626, loss = 3.78305190\n",
      "Iteration 2627, loss = 3.78189082\n",
      "Iteration 2628, loss = 3.78072994\n",
      "Iteration 2629, loss = 3.77956925\n",
      "Iteration 2630, loss = 3.77840876\n",
      "Iteration 2631, loss = 3.77724847\n",
      "Iteration 2632, loss = 3.77608836\n",
      "Iteration 2633, loss = 3.77492844\n",
      "Iteration 2634, loss = 3.77376871\n",
      "Iteration 2635, loss = 3.77260915\n",
      "Iteration 2636, loss = 3.77144978\n",
      "Iteration 2637, loss = 3.77029059\n",
      "Iteration 2638, loss = 3.76913158\n",
      "Iteration 2639, loss = 3.76797274\n",
      "Iteration 2640, loss = 3.76681408\n",
      "Iteration 2641, loss = 3.76565558\n",
      "Iteration 2642, loss = 3.76449726\n",
      "Iteration 2643, loss = 3.76333910\n",
      "Iteration 2644, loss = 3.76218111\n",
      "Iteration 2645, loss = 3.76102329\n",
      "Iteration 2646, loss = 3.75986563\n",
      "Iteration 2647, loss = 3.75870813\n",
      "Iteration 2648, loss = 3.75755079\n",
      "Iteration 2649, loss = 3.75639360\n",
      "Iteration 2650, loss = 3.75523658\n",
      "Iteration 2651, loss = 3.75407971\n",
      "Iteration 2652, loss = 3.75292299\n",
      "Iteration 2653, loss = 3.75176642\n",
      "Iteration 2654, loss = 3.75061001\n",
      "Iteration 2655, loss = 3.74945374\n",
      "Iteration 2656, loss = 3.74829763\n",
      "Iteration 2657, loss = 3.74714166\n",
      "Iteration 2658, loss = 3.74598583\n",
      "Iteration 2659, loss = 3.74483015\n",
      "Iteration 2660, loss = 3.74367461\n",
      "Iteration 2661, loss = 3.74251921\n",
      "Iteration 2662, loss = 3.74136396\n",
      "Iteration 2663, loss = 3.74020884\n",
      "Iteration 2664, loss = 3.73905386\n",
      "Iteration 2665, loss = 3.73789901\n",
      "Iteration 2666, loss = 3.73674430\n",
      "Iteration 2667, loss = 3.73558973\n",
      "Iteration 2668, loss = 3.73443529\n",
      "Iteration 2669, loss = 3.73328098\n",
      "Iteration 2670, loss = 3.73212680\n",
      "Iteration 2671, loss = 3.73097275\n",
      "Iteration 2672, loss = 3.72981882\n",
      "Iteration 2673, loss = 3.72866503\n",
      "Iteration 2674, loss = 3.72751136\n",
      "Iteration 2675, loss = 3.72635782\n",
      "Iteration 2676, loss = 3.72520440\n",
      "Iteration 2677, loss = 3.72405110\n",
      "Iteration 2678, loss = 3.72289793\n",
      "Iteration 2679, loss = 3.72174487\n",
      "Iteration 2680, loss = 3.72059194\n",
      "Iteration 2681, loss = 3.71943913\n",
      "Iteration 2682, loss = 3.71828643\n",
      "Iteration 2683, loss = 3.71713385\n",
      "Iteration 2684, loss = 3.71598139\n",
      "Iteration 2685, loss = 3.71482905\n",
      "Iteration 2686, loss = 3.71367681\n",
      "Iteration 2687, loss = 3.71252470\n",
      "Iteration 2688, loss = 3.71137269\n",
      "Iteration 2689, loss = 3.71022080\n",
      "Iteration 2690, loss = 3.70906902\n",
      "Iteration 2691, loss = 3.70791735\n",
      "Iteration 2692, loss = 3.70676578\n",
      "Iteration 2693, loss = 3.70561433\n",
      "Iteration 2694, loss = 3.70446298\n",
      "Iteration 2695, loss = 3.70331175\n",
      "Iteration 2696, loss = 3.70216062\n",
      "Iteration 2697, loss = 3.70100959\n",
      "Iteration 2698, loss = 3.69985867\n",
      "Iteration 2699, loss = 3.69870785\n",
      "Iteration 2700, loss = 3.69755714\n",
      "Iteration 2701, loss = 3.69640653\n",
      "Iteration 2702, loss = 3.69525602\n",
      "Iteration 2703, loss = 3.69410562\n",
      "Iteration 2704, loss = 3.69295531\n",
      "Iteration 2705, loss = 3.69180511\n",
      "Iteration 2706, loss = 3.69065500\n",
      "Iteration 2707, loss = 3.68950500\n",
      "Iteration 2708, loss = 3.68835509\n",
      "Iteration 2709, loss = 3.68720528\n",
      "Iteration 2710, loss = 3.68605557\n",
      "Iteration 2711, loss = 3.68490596\n",
      "Iteration 2712, loss = 3.68375644\n",
      "Iteration 2713, loss = 3.68260702\n",
      "Iteration 2714, loss = 3.68145769\n",
      "Iteration 2715, loss = 3.68030846\n",
      "Iteration 2716, loss = 3.67915932\n",
      "Iteration 2717, loss = 3.67801028\n",
      "Iteration 2718, loss = 3.67686133\n",
      "Iteration 2719, loss = 3.67571247\n",
      "Iteration 2720, loss = 3.67456371\n",
      "Iteration 2721, loss = 3.67341504\n",
      "Iteration 2722, loss = 3.67226646\n",
      "Iteration 2723, loss = 3.67111797\n",
      "Iteration 2724, loss = 3.66996957\n",
      "Iteration 2725, loss = 3.66882127\n",
      "Iteration 2726, loss = 3.66767305\n",
      "Iteration 2727, loss = 3.66652493\n",
      "Iteration 2728, loss = 3.66537689\n",
      "Iteration 2729, loss = 3.66422894\n",
      "Iteration 2730, loss = 3.66308109\n",
      "Iteration 2731, loss = 3.66193332\n",
      "Iteration 2732, loss = 3.66078564\n",
      "Iteration 2733, loss = 3.65963805\n",
      "Iteration 2734, loss = 3.65849055\n",
      "Iteration 2735, loss = 3.65734313\n",
      "Iteration 2736, loss = 3.65619580\n",
      "Iteration 2737, loss = 3.65504856\n",
      "Iteration 2738, loss = 3.65390141\n",
      "Iteration 2739, loss = 3.65275435\n",
      "Iteration 2740, loss = 3.65160737\n",
      "Iteration 2741, loss = 3.65046048\n",
      "Iteration 2742, loss = 3.64931368\n",
      "Iteration 2743, loss = 3.64816696\n",
      "Iteration 2744, loss = 3.64702033\n",
      "Iteration 2745, loss = 3.64587378\n",
      "Iteration 2746, loss = 3.64472733\n",
      "Iteration 2747, loss = 3.64358096\n",
      "Iteration 2748, loss = 3.64243467\n",
      "Iteration 2749, loss = 3.64128847\n",
      "Iteration 2750, loss = 3.64014236\n",
      "Iteration 2751, loss = 3.63899633\n",
      "Iteration 2752, loss = 3.63785040\n",
      "Iteration 2753, loss = 3.63670454\n",
      "Iteration 2754, loss = 3.63555877\n",
      "Iteration 2755, loss = 3.63441309\n",
      "Iteration 2756, loss = 3.63327441\n",
      "Iteration 2757, loss = 3.63213853\n",
      "Iteration 2758, loss = 3.63100309\n",
      "Iteration 2759, loss = 3.62986804\n",
      "Iteration 2760, loss = 3.62873336\n",
      "Iteration 2761, loss = 3.62759901\n",
      "Iteration 2762, loss = 3.62646497\n",
      "Iteration 2763, loss = 3.62533122\n",
      "Iteration 2764, loss = 3.62419773\n",
      "Iteration 2765, loss = 3.62306448\n",
      "Iteration 2766, loss = 3.62193147\n",
      "Iteration 2767, loss = 3.62079866\n",
      "Iteration 2768, loss = 3.61966606\n",
      "Iteration 2769, loss = 3.61853364\n",
      "Iteration 2770, loss = 3.61740140\n",
      "Iteration 2771, loss = 3.61626932\n",
      "Iteration 2772, loss = 3.61513740\n",
      "Iteration 2773, loss = 3.61400563\n",
      "Iteration 2774, loss = 3.61287399\n",
      "Iteration 2775, loss = 3.61174249\n",
      "Iteration 2776, loss = 3.61061111\n",
      "Iteration 2777, loss = 3.60947985\n",
      "Iteration 2778, loss = 3.60834871\n",
      "Iteration 2779, loss = 3.60721768\n",
      "Iteration 2780, loss = 3.60608676\n",
      "Iteration 2781, loss = 3.60495594\n",
      "Iteration 2782, loss = 3.60382522\n",
      "Iteration 2783, loss = 3.60269460\n",
      "Iteration 2784, loss = 3.60156407\n",
      "Iteration 2785, loss = 3.60043364\n",
      "Iteration 2786, loss = 3.59930330\n",
      "Iteration 2787, loss = 3.59817304\n",
      "Iteration 2788, loss = 3.59704287\n",
      "Iteration 2789, loss = 3.59591279\n",
      "Iteration 2790, loss = 3.59478280\n",
      "Iteration 2791, loss = 3.59365289\n",
      "Iteration 2792, loss = 3.59252306\n",
      "Iteration 2793, loss = 3.59139331\n",
      "Iteration 2794, loss = 3.59026365\n",
      "Iteration 2795, loss = 3.58913406\n",
      "Iteration 2796, loss = 3.58800456\n",
      "Iteration 2797, loss = 3.58687514\n",
      "Iteration 2798, loss = 3.58574579\n",
      "Iteration 2799, loss = 3.58461653\n",
      "Iteration 2800, loss = 3.58348735\n",
      "Iteration 2801, loss = 3.58235825\n",
      "Iteration 2802, loss = 3.58122923\n",
      "Iteration 2803, loss = 3.58010029\n",
      "Iteration 2804, loss = 3.57897143\n",
      "Iteration 2805, loss = 3.57784266\n",
      "Iteration 2806, loss = 3.57671396\n",
      "Iteration 2807, loss = 3.57558535\n",
      "Iteration 2808, loss = 3.57445682\n",
      "Iteration 2809, loss = 3.57332837\n",
      "Iteration 2810, loss = 3.57220000\n",
      "Iteration 2811, loss = 3.57107172\n",
      "Iteration 2812, loss = 3.56994353\n",
      "Iteration 2813, loss = 3.56881542\n",
      "Iteration 2814, loss = 3.56768739\n",
      "Iteration 2815, loss = 3.56655946\n",
      "Iteration 2816, loss = 3.56543161\n",
      "Iteration 2817, loss = 3.56430384\n",
      "Iteration 2818, loss = 3.56317617\n",
      "Iteration 2819, loss = 3.56204859\n",
      "Iteration 2820, loss = 3.56092109\n",
      "Iteration 2821, loss = 3.55979369\n",
      "Iteration 2822, loss = 3.55866638\n",
      "Iteration 2823, loss = 3.55753916\n",
      "Iteration 2824, loss = 3.55641204\n",
      "Iteration 2825, loss = 3.55528501\n",
      "Iteration 2826, loss = 3.55415808\n",
      "Iteration 2827, loss = 3.55303125\n",
      "Iteration 2828, loss = 3.55190451\n",
      "Iteration 2829, loss = 3.55077787\n",
      "Iteration 2830, loss = 3.54965134\n",
      "Iteration 2831, loss = 3.54852490\n",
      "Iteration 2832, loss = 3.54739856\n",
      "Iteration 2833, loss = 3.54627233\n",
      "Iteration 2834, loss = 3.54514620\n",
      "Iteration 2835, loss = 3.54402018\n",
      "Iteration 2836, loss = 3.54289427\n",
      "Iteration 2837, loss = 3.54176846\n",
      "Iteration 2838, loss = 3.54064276\n",
      "Iteration 2839, loss = 3.53951717\n",
      "Iteration 2840, loss = 3.53839169\n",
      "Iteration 2841, loss = 3.53726632\n",
      "Iteration 2842, loss = 3.53614107\n",
      "Iteration 2843, loss = 3.53501593\n",
      "Iteration 2844, loss = 3.53389091\n",
      "Iteration 2845, loss = 3.53276601\n",
      "Iteration 2846, loss = 3.53164122\n",
      "Iteration 2847, loss = 3.53051656\n",
      "Iteration 2848, loss = 3.52939202\n",
      "Iteration 2849, loss = 3.52826759\n",
      "Iteration 2850, loss = 3.52714330\n",
      "Iteration 2851, loss = 3.52601913\n",
      "Iteration 2852, loss = 3.52489508\n",
      "Iteration 2853, loss = 3.52377117\n",
      "Iteration 2854, loss = 3.52264738\n",
      "Iteration 2855, loss = 3.52152372\n",
      "Iteration 2856, loss = 3.52040020\n",
      "Iteration 2857, loss = 3.51927681\n",
      "Iteration 2858, loss = 3.51815356\n",
      "Iteration 2859, loss = 3.51703044\n",
      "Iteration 2860, loss = 3.51590791\n",
      "Iteration 2861, loss = 3.51480357\n",
      "Iteration 2862, loss = 3.51369960\n",
      "Iteration 2863, loss = 3.51259599\n",
      "Iteration 2864, loss = 3.51149270\n",
      "Iteration 2865, loss = 3.51038972\n",
      "Iteration 2866, loss = 3.50928703\n",
      "Iteration 2867, loss = 3.50818462\n",
      "Iteration 2868, loss = 3.50708246\n",
      "Iteration 2869, loss = 3.50598054\n",
      "Iteration 2870, loss = 3.50487886\n",
      "Iteration 2871, loss = 3.50377741\n",
      "Iteration 2872, loss = 3.50267617\n",
      "Iteration 2873, loss = 3.50157513\n",
      "Iteration 2874, loss = 3.50047429\n",
      "Iteration 2875, loss = 3.49937365\n",
      "Iteration 2876, loss = 3.49827319\n",
      "Iteration 2877, loss = 3.49717292\n",
      "Iteration 2878, loss = 3.49607282\n",
      "Iteration 2879, loss = 3.49497290\n",
      "Iteration 2880, loss = 3.49387315\n",
      "Iteration 2881, loss = 3.49277357\n",
      "Iteration 2882, loss = 3.49167416\n",
      "Iteration 2883, loss = 3.49057491\n",
      "Iteration 2884, loss = 3.48947583\n",
      "Iteration 2885, loss = 3.48837691\n",
      "Iteration 2886, loss = 3.48727816\n",
      "Iteration 2887, loss = 3.48617956\n",
      "Iteration 2888, loss = 3.48508113\n",
      "Iteration 2889, loss = 3.48398287\n",
      "Iteration 2890, loss = 3.48288476\n",
      "Iteration 2891, loss = 3.48178682\n",
      "Iteration 2892, loss = 3.48068904\n",
      "Iteration 2893, loss = 3.47959143\n",
      "Iteration 2894, loss = 3.47849399\n",
      "Iteration 2895, loss = 3.47739671\n",
      "Iteration 2896, loss = 3.47629960\n",
      "Iteration 2897, loss = 3.47520265\n",
      "Iteration 2898, loss = 3.47410588\n",
      "Iteration 2899, loss = 3.47300929\n",
      "Iteration 2900, loss = 3.47191286\n",
      "Iteration 2901, loss = 3.47081662\n",
      "Iteration 2902, loss = 3.46972055\n",
      "Iteration 2903, loss = 3.46862465\n",
      "Iteration 2904, loss = 3.46752894\n",
      "Iteration 2905, loss = 3.46643342\n",
      "Iteration 2906, loss = 3.46533808\n",
      "Iteration 2907, loss = 3.46424292\n",
      "Iteration 2908, loss = 3.46314796\n",
      "Iteration 2909, loss = 3.46205318\n",
      "Iteration 2910, loss = 3.46095861\n",
      "Iteration 2911, loss = 3.45986422\n",
      "Iteration 2912, loss = 3.45877003\n",
      "Iteration 2913, loss = 3.45767605\n",
      "Iteration 2914, loss = 3.45658227\n",
      "Iteration 2915, loss = 3.45548869\n",
      "Iteration 2916, loss = 3.45439532\n",
      "Iteration 2917, loss = 3.45330215\n",
      "Iteration 2918, loss = 3.45220920\n",
      "Iteration 2919, loss = 3.45111647\n",
      "Iteration 2920, loss = 3.45002395\n",
      "Iteration 2921, loss = 3.44893165\n",
      "Iteration 2922, loss = 3.44783957\n",
      "Iteration 2923, loss = 3.44674772\n",
      "Iteration 2924, loss = 3.44565609\n",
      "Iteration 2925, loss = 3.44456469\n",
      "Iteration 2926, loss = 3.44347352\n",
      "Iteration 2927, loss = 3.44238259\n",
      "Iteration 2928, loss = 3.44129189\n",
      "Iteration 2929, loss = 3.44020143\n",
      "Iteration 2930, loss = 3.43911122\n",
      "Iteration 2931, loss = 3.43802125\n",
      "Iteration 2932, loss = 3.43693152\n",
      "Iteration 2933, loss = 3.43584204\n",
      "Iteration 2934, loss = 3.43475282\n",
      "Iteration 2935, loss = 3.43366385\n",
      "Iteration 2936, loss = 3.43257514\n",
      "Iteration 2937, loss = 3.43148669\n",
      "Iteration 2938, loss = 3.43039850\n",
      "Iteration 2939, loss = 3.42931057\n",
      "Iteration 2940, loss = 3.42822291\n",
      "Iteration 2941, loss = 3.42713553\n",
      "Iteration 2942, loss = 3.42604841\n",
      "Iteration 2943, loss = 3.42496157\n",
      "Iteration 2944, loss = 3.42387501\n",
      "Iteration 2945, loss = 3.42278873\n",
      "Iteration 2946, loss = 3.42170274\n",
      "Iteration 2947, loss = 3.42061703\n",
      "Iteration 2948, loss = 3.41953161\n",
      "Iteration 2949, loss = 3.41844648\n",
      "Iteration 2950, loss = 3.41736164\n",
      "Iteration 2951, loss = 3.41627710\n",
      "Iteration 2952, loss = 3.41519286\n",
      "Iteration 2953, loss = 3.41410893\n",
      "Iteration 2954, loss = 3.41302529\n",
      "Iteration 2955, loss = 3.41194197\n",
      "Iteration 2956, loss = 3.41085895\n",
      "Iteration 2957, loss = 3.40977625\n",
      "Iteration 2958, loss = 3.40869387\n",
      "Iteration 2959, loss = 3.40761180\n",
      "Iteration 2960, loss = 3.40653005\n",
      "Iteration 2961, loss = 3.40544863\n",
      "Iteration 2962, loss = 3.40436753\n",
      "Iteration 2963, loss = 3.40328676\n",
      "Iteration 2964, loss = 3.40220633\n",
      "Iteration 2965, loss = 3.40112622\n",
      "Iteration 2966, loss = 3.40004646\n",
      "Iteration 2967, loss = 3.39896703\n",
      "Iteration 2968, loss = 3.39788795\n",
      "Iteration 2969, loss = 3.39680921\n",
      "Iteration 2970, loss = 3.39573082\n",
      "Iteration 2971, loss = 3.39465278\n",
      "Iteration 2972, loss = 3.39357510\n",
      "Iteration 2973, loss = 3.39249776\n",
      "Iteration 2974, loss = 3.39142079\n",
      "Iteration 2975, loss = 3.39034418\n",
      "Iteration 2976, loss = 3.38926793\n",
      "Iteration 2977, loss = 3.38819205\n",
      "Iteration 2978, loss = 3.38711653\n",
      "Iteration 2979, loss = 3.38604139\n",
      "Iteration 2980, loss = 3.38496662\n",
      "Iteration 2981, loss = 3.38389223\n",
      "Iteration 2982, loss = 3.38281822\n",
      "Iteration 2983, loss = 3.38174458\n",
      "Iteration 2984, loss = 3.38067134\n",
      "Iteration 2985, loss = 3.37959848\n",
      "Iteration 2986, loss = 3.37852601\n",
      "Iteration 2987, loss = 3.37745393\n",
      "Iteration 2988, loss = 3.37638225\n",
      "Iteration 2989, loss = 3.37531096\n",
      "Iteration 2990, loss = 3.37424007\n",
      "Iteration 2991, loss = 3.37316959\n",
      "Iteration 2992, loss = 3.37209951\n",
      "Iteration 2993, loss = 3.37102984\n",
      "Iteration 2994, loss = 3.36996058\n",
      "Iteration 2995, loss = 3.36889173\n",
      "Iteration 2996, loss = 3.36782329\n",
      "Iteration 2997, loss = 3.36675527\n",
      "Iteration 2998, loss = 3.36568768\n",
      "Iteration 2999, loss = 3.36462050\n",
      "Iteration 3000, loss = 3.36355375\n",
      "Iteration 3001, loss = 3.36248742\n",
      "Iteration 3002, loss = 3.36142153\n",
      "Iteration 3003, loss = 3.36035607\n",
      "Iteration 3004, loss = 3.35929104\n",
      "Iteration 3005, loss = 3.35822644\n",
      "Iteration 3006, loss = 3.35716229\n",
      "Iteration 3007, loss = 3.35609858\n",
      "Iteration 3008, loss = 3.35503531\n",
      "Iteration 3009, loss = 3.35397249\n",
      "Iteration 3010, loss = 3.35291011\n",
      "Iteration 3011, loss = 3.35184819\n",
      "Iteration 3012, loss = 3.35078671\n",
      "Iteration 3013, loss = 3.34972570\n",
      "Iteration 3014, loss = 3.34866514\n",
      "Iteration 3015, loss = 3.34760504\n",
      "Iteration 3016, loss = 3.34654540\n",
      "Iteration 3017, loss = 3.34548622\n",
      "Iteration 3018, loss = 3.34442752\n",
      "Iteration 3019, loss = 3.34336927\n",
      "Iteration 3020, loss = 3.34231150\n",
      "Iteration 3021, loss = 3.34125421\n",
      "Iteration 3022, loss = 3.34019738\n",
      "Iteration 3023, loss = 3.33914104\n",
      "Iteration 3024, loss = 3.33808517\n",
      "Iteration 3025, loss = 3.33702978\n",
      "Iteration 3026, loss = 3.33597487\n",
      "Iteration 3027, loss = 3.33492045\n",
      "Iteration 3028, loss = 3.33386652\n",
      "Iteration 3029, loss = 3.33281308\n",
      "Iteration 3030, loss = 3.33176012\n",
      "Iteration 3031, loss = 3.33070766\n",
      "Iteration 3032, loss = 3.32965569\n",
      "Iteration 3033, loss = 3.32860422\n",
      "Iteration 3034, loss = 3.32755325\n",
      "Iteration 3035, loss = 3.32650278\n",
      "Iteration 3036, loss = 3.32545281\n",
      "Iteration 3037, loss = 3.32440334\n",
      "Iteration 3038, loss = 3.32335438\n",
      "Iteration 3039, loss = 3.32230593\n",
      "Iteration 3040, loss = 3.32125798\n",
      "Iteration 3041, loss = 3.32021055\n",
      "Iteration 3042, loss = 3.31916363\n",
      "Iteration 3043, loss = 3.31811722\n",
      "Iteration 3044, loss = 3.31707133\n",
      "Iteration 3045, loss = 3.31602596\n",
      "Iteration 3046, loss = 3.31498110\n",
      "Iteration 3047, loss = 3.31393677\n",
      "Iteration 3048, loss = 3.31289296\n",
      "Iteration 3049, loss = 3.31184967\n",
      "Iteration 3050, loss = 3.31080691\n",
      "Iteration 3051, loss = 3.30976467\n",
      "Iteration 3052, loss = 3.30872297\n",
      "Iteration 3053, loss = 3.30768179\n",
      "Iteration 3054, loss = 3.30664115\n",
      "Iteration 3055, loss = 3.30560103\n",
      "Iteration 3056, loss = 3.30456146\n",
      "Iteration 3057, loss = 3.30352271\n",
      "Iteration 3058, loss = 3.30248541\n",
      "Iteration 3059, loss = 3.30144874\n",
      "Iteration 3060, loss = 3.30041268\n",
      "Iteration 3061, loss = 3.29937724\n",
      "Iteration 3062, loss = 3.29834239\n",
      "Iteration 3063, loss = 3.29730814\n",
      "Iteration 3064, loss = 3.29627449\n",
      "Iteration 3065, loss = 3.29524143\n",
      "Iteration 3066, loss = 3.29420895\n",
      "Iteration 3067, loss = 3.29317707\n",
      "Iteration 3068, loss = 3.29214576\n",
      "Iteration 3069, loss = 3.29111503\n",
      "Iteration 3070, loss = 3.29008489\n",
      "Iteration 3071, loss = 3.28905532\n",
      "Iteration 3072, loss = 3.28802632\n",
      "Iteration 3073, loss = 3.28699791\n",
      "Iteration 3074, loss = 3.28597006\n",
      "Iteration 3075, loss = 3.28494279\n",
      "Iteration 3076, loss = 3.28391609\n",
      "Iteration 3077, loss = 3.28288997\n",
      "Iteration 3078, loss = 3.28186441\n",
      "Iteration 3079, loss = 3.28083943\n",
      "Iteration 3080, loss = 3.27981502\n",
      "Iteration 3081, loss = 3.27879118\n",
      "Iteration 3082, loss = 3.27776791\n",
      "Iteration 3083, loss = 3.27674521\n",
      "Iteration 3084, loss = 3.27572308\n",
      "Iteration 3085, loss = 3.27470153\n",
      "Iteration 3086, loss = 3.27368055\n",
      "Iteration 3087, loss = 3.27266014\n",
      "Iteration 3088, loss = 3.27164030\n",
      "Iteration 3089, loss = 3.27062103\n",
      "Iteration 3090, loss = 3.26960234\n",
      "Iteration 3091, loss = 3.26858422\n",
      "Iteration 3092, loss = 3.26756668\n",
      "Iteration 3093, loss = 3.26654970\n",
      "Iteration 3094, loss = 3.26553330\n",
      "Iteration 3095, loss = 3.26451748\n",
      "Iteration 3096, loss = 3.26350223\n",
      "Iteration 3097, loss = 3.26248755\n",
      "Iteration 3098, loss = 3.26147345\n",
      "Iteration 3099, loss = 3.26045992\n",
      "Iteration 3100, loss = 3.25944697\n",
      "Iteration 3101, loss = 3.25843459\n",
      "Iteration 3102, loss = 3.25742279\n",
      "Iteration 3103, loss = 3.25641157\n",
      "Iteration 3104, loss = 3.25540092\n",
      "Iteration 3105, loss = 3.25439085\n",
      "Iteration 3106, loss = 3.25338135\n",
      "Iteration 3107, loss = 3.25237243\n",
      "Iteration 3108, loss = 3.25136409\n",
      "Iteration 3109, loss = 3.25035633\n",
      "Iteration 3110, loss = 3.24934914\n",
      "Iteration 3111, loss = 3.24834254\n",
      "Iteration 3112, loss = 3.24733651\n",
      "Iteration 3113, loss = 3.24633105\n",
      "Iteration 3114, loss = 3.24532618\n",
      "Iteration 3115, loss = 3.24432188\n",
      "Iteration 3116, loss = 3.24331816\n",
      "Iteration 3117, loss = 3.24231502\n",
      "Iteration 3118, loss = 3.24131246\n",
      "Iteration 3119, loss = 3.24031048\n",
      "Iteration 3120, loss = 3.23930907\n",
      "Iteration 3121, loss = 3.23830824\n",
      "Iteration 3122, loss = 3.23730800\n",
      "Iteration 3123, loss = 3.23630832\n",
      "Iteration 3124, loss = 3.23530923\n",
      "Iteration 3125, loss = 3.23431072\n",
      "Iteration 3126, loss = 3.23331278\n",
      "Iteration 3127, loss = 3.23231543\n",
      "Iteration 3128, loss = 3.23131865\n",
      "Iteration 3129, loss = 3.23032245\n",
      "Iteration 3130, loss = 3.22932682\n",
      "Iteration 3131, loss = 3.22833178\n",
      "Iteration 3132, loss = 3.22733731\n",
      "Iteration 3133, loss = 3.22634342\n",
      "Iteration 3134, loss = 3.22535011\n",
      "Iteration 3135, loss = 3.22435737\n",
      "Iteration 3136, loss = 3.22336521\n",
      "Iteration 3137, loss = 3.22237363\n",
      "Iteration 3138, loss = 3.22138263\n",
      "Iteration 3139, loss = 3.22039220\n",
      "Iteration 3140, loss = 3.21940235\n",
      "Iteration 3141, loss = 3.21841307\n",
      "Iteration 3142, loss = 3.21742437\n",
      "Iteration 3143, loss = 3.21643625\n",
      "Iteration 3144, loss = 3.21544870\n",
      "Iteration 3145, loss = 3.21446172\n",
      "Iteration 3146, loss = 3.21347532\n",
      "Iteration 3147, loss = 3.21248950\n",
      "Iteration 3148, loss = 3.21150424\n",
      "Iteration 3149, loss = 3.21051957\n",
      "Iteration 3150, loss = 3.20953546\n",
      "Iteration 3151, loss = 3.20855193\n",
      "Iteration 3152, loss = 3.20756897\n",
      "Iteration 3153, loss = 3.20658658\n",
      "Iteration 3154, loss = 3.20560477\n",
      "Iteration 3155, loss = 3.20462353\n",
      "Iteration 3156, loss = 3.20364286\n",
      "Iteration 3157, loss = 3.20266276\n",
      "Iteration 3158, loss = 3.20168323\n",
      "Iteration 3159, loss = 3.20070427\n",
      "Iteration 3160, loss = 3.19972588\n",
      "Iteration 3161, loss = 3.19874806\n",
      "Iteration 3162, loss = 3.19777081\n",
      "Iteration 3163, loss = 3.19679412\n",
      "Iteration 3164, loss = 3.19581801\n",
      "Iteration 3165, loss = 3.19484246\n",
      "Iteration 3166, loss = 3.19386748\n",
      "Iteration 3167, loss = 3.19289307\n",
      "Iteration 3168, loss = 3.19191923\n",
      "Iteration 3169, loss = 3.19094595\n",
      "Iteration 3170, loss = 3.18997323\n",
      "Iteration 3171, loss = 3.18900108\n",
      "Iteration 3172, loss = 3.18802950\n",
      "Iteration 3173, loss = 3.18705848\n",
      "Iteration 3174, loss = 3.18608802\n",
      "Iteration 3175, loss = 3.18511813\n",
      "Iteration 3176, loss = 3.18414880\n",
      "Iteration 3177, loss = 3.18318003\n",
      "Iteration 3178, loss = 3.18221183\n",
      "Iteration 3179, loss = 3.18124419\n",
      "Iteration 3180, loss = 3.18027710\n",
      "Iteration 3181, loss = 3.17931058\n",
      "Iteration 3182, loss = 3.17834462\n",
      "Iteration 3183, loss = 3.17737922\n",
      "Iteration 3184, loss = 3.17641437\n",
      "Iteration 3185, loss = 3.17545009\n",
      "Iteration 3186, loss = 3.17448636\n",
      "Iteration 3187, loss = 3.17352319\n",
      "Iteration 3188, loss = 3.17256058\n",
      "Iteration 3189, loss = 3.17159852\n",
      "Iteration 3190, loss = 3.17063702\n",
      "Iteration 3191, loss = 3.16967608\n",
      "Iteration 3192, loss = 3.16871569\n",
      "Iteration 3193, loss = 3.16775586\n",
      "Iteration 3194, loss = 3.16679658\n",
      "Iteration 3195, loss = 3.16583786\n",
      "Iteration 3196, loss = 3.16487968\n",
      "Iteration 3197, loss = 3.16392207\n",
      "Iteration 3198, loss = 3.16296500\n",
      "Iteration 3199, loss = 3.16200848\n",
      "Iteration 3200, loss = 3.16105252\n",
      "Iteration 3201, loss = 3.16009711\n",
      "Iteration 3202, loss = 3.15914225\n",
      "Iteration 3203, loss = 3.15818793\n",
      "Iteration 3204, loss = 3.15723417\n",
      "Iteration 3205, loss = 3.15628096\n",
      "Iteration 3206, loss = 3.15532829\n",
      "Iteration 3207, loss = 3.15437618\n",
      "Iteration 3208, loss = 3.15342461\n",
      "Iteration 3209, loss = 3.15247359\n",
      "Iteration 3210, loss = 3.15152311\n",
      "Iteration 3211, loss = 3.15057318\n",
      "Iteration 3212, loss = 3.14962380\n",
      "Iteration 3213, loss = 3.14867496\n",
      "Iteration 3214, loss = 3.14772667\n",
      "Iteration 3215, loss = 3.14677892\n",
      "Iteration 3216, loss = 3.14583172\n",
      "Iteration 3217, loss = 3.14488506\n",
      "Iteration 3218, loss = 3.14393894\n",
      "Iteration 3219, loss = 3.14299337\n",
      "Iteration 3220, loss = 3.14204834\n",
      "Iteration 3221, loss = 3.14110385\n",
      "Iteration 3222, loss = 3.14015990\n",
      "Iteration 3223, loss = 3.13921649\n",
      "Iteration 3224, loss = 3.13827363\n",
      "Iteration 3225, loss = 3.13733130\n",
      "Iteration 3226, loss = 3.13638951\n",
      "Iteration 3227, loss = 3.13544827\n",
      "Iteration 3228, loss = 3.13450756\n",
      "Iteration 3229, loss = 3.13356739\n",
      "Iteration 3230, loss = 3.13262776\n",
      "Iteration 3231, loss = 3.13168867\n",
      "Iteration 3232, loss = 3.13075012\n",
      "Iteration 3233, loss = 3.12981210\n",
      "Iteration 3234, loss = 3.12887462\n",
      "Iteration 3235, loss = 3.12793767\n",
      "Iteration 3236, loss = 3.12700127\n",
      "Iteration 3237, loss = 3.12606539\n",
      "Iteration 3238, loss = 3.12513006\n",
      "Iteration 3239, loss = 3.12419526\n",
      "Iteration 3240, loss = 3.12326099\n",
      "Iteration 3241, loss = 3.12232726\n",
      "Iteration 3242, loss = 3.12139406\n",
      "Iteration 3243, loss = 3.12046139\n",
      "Iteration 3244, loss = 3.11952926\n",
      "Iteration 3245, loss = 3.11859767\n",
      "Iteration 3246, loss = 3.11766660\n",
      "Iteration 3247, loss = 3.11673607\n",
      "Iteration 3248, loss = 3.11580607\n",
      "Iteration 3249, loss = 3.11487660\n",
      "Iteration 3250, loss = 3.11394766\n",
      "Iteration 3251, loss = 3.11301926\n",
      "Iteration 3252, loss = 3.11209139\n",
      "Iteration 3253, loss = 3.11116405\n",
      "Iteration 3254, loss = 3.11023725\n",
      "Iteration 3255, loss = 3.10931098\n",
      "Iteration 3256, loss = 3.10838524\n",
      "Iteration 3257, loss = 3.10746003\n",
      "Iteration 3258, loss = 3.10653536\n",
      "Iteration 3259, loss = 3.10561121\n",
      "Iteration 3260, loss = 3.10468759\n",
      "Iteration 3261, loss = 3.10376450\n",
      "Iteration 3262, loss = 3.10284194\n",
      "Iteration 3263, loss = 3.10191991\n",
      "Iteration 3264, loss = 3.10099840\n",
      "Iteration 3265, loss = 3.10007743\n",
      "Iteration 3266, loss = 3.09915698\n",
      "Iteration 3267, loss = 3.09823706\n",
      "Iteration 3268, loss = 3.09731767\n",
      "Iteration 3269, loss = 3.09639881\n",
      "Iteration 3270, loss = 3.09548048\n",
      "Iteration 3271, loss = 3.09456267\n",
      "Iteration 3272, loss = 3.09364539\n",
      "Iteration 3273, loss = 3.09272863\n",
      "Iteration 3274, loss = 3.09181240\n",
      "Iteration 3275, loss = 3.09089670\n",
      "Iteration 3276, loss = 3.08998153\n",
      "Iteration 3277, loss = 3.08906688\n",
      "Iteration 3278, loss = 3.08815276\n",
      "Iteration 3279, loss = 3.08723916\n",
      "Iteration 3280, loss = 3.08632609\n",
      "Iteration 3281, loss = 3.08541355\n",
      "Iteration 3282, loss = 3.08450153\n",
      "Iteration 3283, loss = 3.08359004\n",
      "Iteration 3284, loss = 3.08267907\n",
      "Iteration 3285, loss = 3.08176863\n",
      "Iteration 3286, loss = 3.08085871\n",
      "Iteration 3287, loss = 3.07994932\n",
      "Iteration 3288, loss = 3.07904045\n",
      "Iteration 3289, loss = 3.07813211\n",
      "Iteration 3290, loss = 3.07722430\n",
      "Iteration 3291, loss = 3.07631701\n",
      "Iteration 3292, loss = 3.07541024\n",
      "Iteration 3293, loss = 3.07450400\n",
      "Iteration 3294, loss = 3.07359829\n",
      "Iteration 3295, loss = 3.07269310\n",
      "Iteration 3296, loss = 3.07178843\n",
      "Iteration 3297, loss = 3.07088429\n",
      "Iteration 3298, loss = 3.06998067\n",
      "Iteration 3299, loss = 3.06907758\n",
      "Iteration 3300, loss = 3.06817502\n",
      "Iteration 3301, loss = 3.06727298\n",
      "Iteration 3302, loss = 3.06637146\n",
      "Iteration 3303, loss = 3.06547047\n",
      "Iteration 3304, loss = 3.06457000\n",
      "Iteration 3305, loss = 3.06367006\n",
      "Iteration 3306, loss = 3.06277064\n",
      "Iteration 3307, loss = 3.06187175\n",
      "Iteration 3308, loss = 3.06097338\n",
      "Iteration 3309, loss = 3.06007554\n",
      "Iteration 3310, loss = 3.05917822\n",
      "Iteration 3311, loss = 3.05828143\n",
      "Iteration 3312, loss = 3.05738517\n",
      "Iteration 3313, loss = 3.05648942\n",
      "Iteration 3314, loss = 3.05559421\n",
      "Iteration 3315, loss = 3.05469952\n",
      "Iteration 3316, loss = 3.05380535\n",
      "Iteration 3317, loss = 3.05291171\n",
      "Iteration 3318, loss = 3.05201860\n",
      "Iteration 3319, loss = 3.05112601\n",
      "Iteration 3320, loss = 3.05023395\n",
      "Iteration 3321, loss = 3.04934241\n",
      "Iteration 3322, loss = 3.04845140\n",
      "Iteration 3323, loss = 3.04756092\n",
      "Iteration 3324, loss = 3.04667096\n",
      "Iteration 3325, loss = 3.04578153\n",
      "Iteration 3326, loss = 3.04489263\n",
      "Iteration 3327, loss = 3.04400425\n",
      "Iteration 3328, loss = 3.04311640\n",
      "Iteration 3329, loss = 3.04222907\n",
      "Iteration 3330, loss = 3.04134227\n",
      "Iteration 3331, loss = 3.04045600\n",
      "Iteration 3332, loss = 3.03957026\n",
      "Iteration 3333, loss = 3.03868504\n",
      "Iteration 3334, loss = 3.03780035\n",
      "Iteration 3335, loss = 3.03691619\n",
      "Iteration 3336, loss = 3.03603256\n",
      "Iteration 3337, loss = 3.03514945\n",
      "Iteration 3338, loss = 3.03426688\n",
      "Iteration 3339, loss = 3.03338483\n",
      "Iteration 3340, loss = 3.03250331\n",
      "Iteration 3341, loss = 3.03162232\n",
      "Iteration 3342, loss = 3.03074185\n",
      "Iteration 3343, loss = 3.02986192\n",
      "Iteration 3344, loss = 3.02898252\n",
      "Iteration 3345, loss = 3.02810364\n",
      "Iteration 3346, loss = 3.02722530\n",
      "Iteration 3347, loss = 3.02634748\n",
      "Iteration 3348, loss = 3.02547020\n",
      "Iteration 3349, loss = 3.02459344\n",
      "Iteration 3350, loss = 3.02371722\n",
      "Iteration 3351, loss = 3.02284152\n",
      "Iteration 3352, loss = 3.02196636\n",
      "Iteration 3353, loss = 3.02109173\n",
      "Iteration 3354, loss = 3.02021762\n",
      "Iteration 3355, loss = 3.01934405\n",
      "Iteration 3356, loss = 3.01847102\n",
      "Iteration 3357, loss = 3.01759851\n",
      "Iteration 3358, loss = 3.01672654\n",
      "Iteration 3359, loss = 3.01585509\n",
      "Iteration 3360, loss = 3.01498418\n",
      "Iteration 3361, loss = 3.01411381\n",
      "Iteration 3362, loss = 3.01324396\n",
      "Iteration 3363, loss = 3.01237465\n",
      "Iteration 3364, loss = 3.01150588\n",
      "Iteration 3365, loss = 3.01063763\n",
      "Iteration 3366, loss = 3.00976992\n",
      "Iteration 3367, loss = 3.00890275\n",
      "Iteration 3368, loss = 3.00803611\n",
      "Iteration 3369, loss = 3.00716333\n",
      "Iteration 3370, loss = 3.00617574\n",
      "Iteration 3371, loss = 3.00518869\n",
      "Iteration 3372, loss = 3.00420219\n",
      "Iteration 3373, loss = 3.00321624\n",
      "Iteration 3374, loss = 3.00223085\n",
      "Iteration 3375, loss = 3.00124600\n",
      "Iteration 3376, loss = 3.00026170\n",
      "Iteration 3377, loss = 2.99927795\n",
      "Iteration 3378, loss = 2.99829475\n",
      "Iteration 3379, loss = 2.99731210\n",
      "Iteration 3380, loss = 2.99633001\n",
      "Iteration 3381, loss = 2.99534846\n",
      "Iteration 3382, loss = 2.99436747\n",
      "Iteration 3383, loss = 2.99338703\n",
      "Iteration 3384, loss = 2.99239973\n",
      "Iteration 3385, loss = 2.99140915\n",
      "Iteration 3386, loss = 2.99041770\n",
      "Iteration 3387, loss = 2.98942552\n",
      "Iteration 3388, loss = 2.98843274\n",
      "Iteration 3389, loss = 2.98743949\n",
      "Iteration 3390, loss = 2.98644587\n",
      "Iteration 3391, loss = 2.98545198\n",
      "Iteration 3392, loss = 2.98445790\n",
      "Iteration 3393, loss = 2.98346373\n",
      "Iteration 3394, loss = 2.98246952\n",
      "Iteration 3395, loss = 2.98147534\n",
      "Iteration 3396, loss = 2.98048126\n",
      "Iteration 3397, loss = 2.97948732\n",
      "Iteration 3398, loss = 2.97849357\n",
      "Iteration 3399, loss = 2.97750006\n",
      "Iteration 3400, loss = 2.97650683\n",
      "Iteration 3401, loss = 2.97551391\n",
      "Iteration 3402, loss = 2.97452133\n",
      "Iteration 3403, loss = 2.97352913\n",
      "Iteration 3404, loss = 2.97253732\n",
      "Iteration 3405, loss = 2.97154593\n",
      "Iteration 3406, loss = 2.97055499\n",
      "Iteration 3407, loss = 2.96956451\n",
      "Iteration 3408, loss = 2.96857451\n",
      "Iteration 3409, loss = 2.96758501\n",
      "Iteration 3410, loss = 2.96659601\n",
      "Iteration 3411, loss = 2.96560754\n",
      "Iteration 3412, loss = 2.96461959\n",
      "Iteration 3413, loss = 2.96363220\n",
      "Iteration 3414, loss = 2.96264535\n",
      "Iteration 3415, loss = 2.96165906\n",
      "Iteration 3416, loss = 2.96067334\n",
      "Iteration 3417, loss = 2.95968820\n",
      "Iteration 3418, loss = 2.95870363\n",
      "Iteration 3419, loss = 2.95771965\n",
      "Iteration 3420, loss = 2.95673626\n",
      "Iteration 3421, loss = 2.95575347\n",
      "Iteration 3422, loss = 2.95477127\n",
      "Iteration 3423, loss = 2.95378968\n",
      "Iteration 3424, loss = 2.95280869\n",
      "Iteration 3425, loss = 2.95183025\n",
      "Iteration 3426, loss = 2.95087215\n",
      "Iteration 3427, loss = 2.94991579\n",
      "Iteration 3428, loss = 2.94896105\n",
      "Iteration 3429, loss = 2.94800783\n",
      "Iteration 3430, loss = 2.94705606\n",
      "Iteration 3431, loss = 2.94610563\n",
      "Iteration 3432, loss = 2.94515649\n",
      "Iteration 3433, loss = 2.94420856\n",
      "Iteration 3434, loss = 2.94326180\n",
      "Iteration 3435, loss = 2.94231613\n",
      "Iteration 3436, loss = 2.94137153\n",
      "Iteration 3437, loss = 2.94042793\n",
      "Iteration 3438, loss = 2.93948531\n",
      "Iteration 3439, loss = 2.93854363\n",
      "Iteration 3440, loss = 2.93760286\n",
      "Iteration 3441, loss = 2.93666296\n",
      "Iteration 3442, loss = 2.93572392\n",
      "Iteration 3443, loss = 2.93478571\n",
      "Iteration 3444, loss = 2.93384830\n",
      "Iteration 3445, loss = 2.93291169\n",
      "Iteration 3446, loss = 2.93197585\n",
      "Iteration 3447, loss = 2.93104077\n",
      "Iteration 3448, loss = 2.93010643\n",
      "Iteration 3449, loss = 2.92917282\n",
      "Iteration 3450, loss = 2.92823993\n",
      "Iteration 3451, loss = 2.92730775\n",
      "Iteration 3452, loss = 2.92637627\n",
      "Iteration 3453, loss = 2.92544548\n",
      "Iteration 3454, loss = 2.92451537\n",
      "Iteration 3455, loss = 2.92358595\n",
      "Iteration 3456, loss = 2.92265719\n",
      "Iteration 3457, loss = 2.92172910\n",
      "Iteration 3458, loss = 2.92080166\n",
      "Iteration 3459, loss = 2.91987488\n",
      "Iteration 3460, loss = 2.91894876\n",
      "Iteration 3461, loss = 2.91802328\n",
      "Iteration 3462, loss = 2.91709844\n",
      "Iteration 3463, loss = 2.91617424\n",
      "Iteration 3464, loss = 2.91525068\n",
      "Iteration 3465, loss = 2.91432776\n",
      "Iteration 3466, loss = 2.91340546\n",
      "Iteration 3467, loss = 2.91248380\n",
      "Iteration 3468, loss = 2.91156276\n",
      "Iteration 3469, loss = 2.91064235\n",
      "Iteration 3470, loss = 2.90972257\n",
      "Iteration 3471, loss = 2.90880341\n",
      "Iteration 3472, loss = 2.90788486\n",
      "Iteration 3473, loss = 2.90696694\n",
      "Iteration 3474, loss = 2.90604963\n",
      "Iteration 3475, loss = 2.90513295\n",
      "Iteration 3476, loss = 2.90421687\n",
      "Iteration 3477, loss = 2.90330142\n",
      "Iteration 3478, loss = 2.90238657\n",
      "Iteration 3479, loss = 2.90147234\n",
      "Iteration 3480, loss = 2.90055872\n",
      "Iteration 3481, loss = 2.89964571\n",
      "Iteration 3482, loss = 2.89873332\n",
      "Iteration 3483, loss = 2.89782153\n",
      "Iteration 3484, loss = 2.89691035\n",
      "Iteration 3485, loss = 2.89599978\n",
      "Iteration 3486, loss = 2.89508981\n",
      "Iteration 3487, loss = 2.89418046\n",
      "Iteration 3488, loss = 2.89327171\n",
      "Iteration 3489, loss = 2.89236356\n",
      "Iteration 3490, loss = 2.89145602\n",
      "Iteration 3491, loss = 2.89054908\n",
      "Iteration 3492, loss = 2.88964275\n",
      "Iteration 3493, loss = 2.88873702\n",
      "Iteration 3494, loss = 2.88783190\n",
      "Iteration 3495, loss = 2.88692737\n",
      "Iteration 3496, loss = 2.88602345\n",
      "Iteration 3497, loss = 2.88512013\n",
      "Iteration 3498, loss = 2.88421741\n",
      "Iteration 3499, loss = 2.88331529\n",
      "Iteration 3500, loss = 2.88241376\n",
      "Iteration 3501, loss = 2.88151284\n",
      "Iteration 3502, loss = 2.88061252\n",
      "Iteration 3503, loss = 2.87971279\n",
      "Iteration 3504, loss = 2.87881367\n",
      "Iteration 3505, loss = 2.87791513\n",
      "Iteration 3506, loss = 2.87701720\n",
      "Iteration 3507, loss = 2.87611986\n",
      "Iteration 3508, loss = 2.87522312\n",
      "Iteration 3509, loss = 2.87432697\n",
      "Iteration 3510, loss = 2.87343141\n",
      "Iteration 3511, loss = 2.87253645\n",
      "Iteration 3512, loss = 2.87164209\n",
      "Iteration 3513, loss = 2.87074831\n",
      "Iteration 3514, loss = 2.86985513\n",
      "Iteration 3515, loss = 2.86896254\n",
      "Iteration 3516, loss = 2.86807055\n",
      "Iteration 3517, loss = 2.86717914\n",
      "Iteration 3518, loss = 2.86628832\n",
      "Iteration 3519, loss = 2.86539810\n",
      "Iteration 3520, loss = 2.86450846\n",
      "Iteration 3521, loss = 2.86361941\n",
      "Iteration 3522, loss = 2.86273096\n",
      "Iteration 3523, loss = 2.86184574\n",
      "Iteration 3524, loss = 2.86096175\n",
      "Iteration 3525, loss = 2.86007846\n",
      "Iteration 3526, loss = 2.85919586\n",
      "Iteration 3527, loss = 2.85831393\n",
      "Iteration 3528, loss = 2.85743268\n",
      "Iteration 3529, loss = 2.85655207\n",
      "Iteration 3530, loss = 2.85567212\n",
      "Iteration 3531, loss = 2.85479282\n",
      "Iteration 3532, loss = 2.85391415\n",
      "Iteration 3533, loss = 2.85303611\n",
      "Iteration 3534, loss = 2.85215869\n",
      "Iteration 3535, loss = 2.85128190\n",
      "Iteration 3536, loss = 2.85040572\n",
      "Iteration 3537, loss = 2.84953015\n",
      "Iteration 3538, loss = 2.84865519\n",
      "Iteration 3539, loss = 2.84778084\n",
      "Iteration 3540, loss = 2.84690708\n",
      "Iteration 3541, loss = 2.84603393\n",
      "Iteration 3542, loss = 2.84516136\n",
      "Iteration 3543, loss = 2.84428939\n",
      "Iteration 3544, loss = 2.84341801\n",
      "Iteration 3545, loss = 2.84254722\n",
      "Iteration 3546, loss = 2.84167701\n",
      "Iteration 3547, loss = 2.84080739\n",
      "Iteration 3548, loss = 2.83993835\n",
      "Iteration 3549, loss = 2.83906989\n",
      "Iteration 3550, loss = 2.83820200\n",
      "Iteration 3551, loss = 2.83733470\n",
      "Iteration 3552, loss = 2.83646797\n",
      "Iteration 3553, loss = 2.83560181\n",
      "Iteration 3554, loss = 2.83473623\n",
      "Iteration 3555, loss = 2.83387121\n",
      "Iteration 3556, loss = 2.83300677\n",
      "Iteration 3557, loss = 2.83214290\n",
      "Iteration 3558, loss = 2.83127959\n",
      "Iteration 3559, loss = 2.83041685\n",
      "Iteration 3560, loss = 2.82955468\n",
      "Iteration 3561, loss = 2.82869307\n",
      "Iteration 3562, loss = 2.82783203\n",
      "Iteration 3563, loss = 2.82697154\n",
      "Iteration 3564, loss = 2.82611162\n",
      "Iteration 3565, loss = 2.82525226\n",
      "Iteration 3566, loss = 2.82439346\n",
      "Iteration 3567, loss = 2.82353522\n",
      "Iteration 3568, loss = 2.82267753\n",
      "Iteration 3569, loss = 2.82182040\n",
      "Iteration 3570, loss = 2.82096383\n",
      "Iteration 3571, loss = 2.82010781\n",
      "Iteration 3572, loss = 2.81925235\n",
      "Iteration 3573, loss = 2.81839744\n",
      "Iteration 3574, loss = 2.81754308\n",
      "Iteration 3575, loss = 2.81668927\n",
      "Iteration 3576, loss = 2.81583492\n",
      "Iteration 3577, loss = 2.81496830\n",
      "Iteration 3578, loss = 2.81410133\n",
      "Iteration 3579, loss = 2.81323410\n",
      "Iteration 3580, loss = 2.81236671\n",
      "Iteration 3581, loss = 2.81149921\n",
      "Iteration 3582, loss = 2.81063168\n",
      "Iteration 3583, loss = 2.80976418\n",
      "Iteration 3584, loss = 2.80889676\n",
      "Iteration 3585, loss = 2.80802947\n",
      "Iteration 3586, loss = 2.80716234\n",
      "Iteration 3587, loss = 2.80629543\n",
      "Iteration 3588, loss = 2.80542875\n",
      "Iteration 3589, loss = 2.80456235\n",
      "Iteration 3590, loss = 2.80369625\n",
      "Iteration 3591, loss = 2.80283047\n",
      "Iteration 3592, loss = 2.80196503\n",
      "Iteration 3593, loss = 2.80109992\n",
      "Iteration 3594, loss = 2.80023311\n",
      "Iteration 3595, loss = 2.79936640\n",
      "Iteration 3596, loss = 2.79849997\n",
      "Iteration 3597, loss = 2.79763386\n",
      "Iteration 3598, loss = 2.79676810\n",
      "Iteration 3599, loss = 2.79590269\n",
      "Iteration 3600, loss = 2.79503768\n",
      "Iteration 3601, loss = 2.79417307\n",
      "Iteration 3602, loss = 2.79330888\n",
      "Iteration 3603, loss = 2.79244513\n",
      "Iteration 3604, loss = 2.79158182\n",
      "Iteration 3605, loss = 2.79071898\n",
      "Iteration 3606, loss = 2.78985660\n",
      "Iteration 3607, loss = 2.78899471\n",
      "Iteration 3608, loss = 2.78813331\n",
      "Iteration 3609, loss = 2.78727240\n",
      "Iteration 3610, loss = 2.78641199\n",
      "Iteration 3611, loss = 2.78555209\n",
      "Iteration 3612, loss = 2.78469270\n",
      "Iteration 3613, loss = 2.78383382\n",
      "Iteration 3614, loss = 2.78297547\n",
      "Iteration 3615, loss = 2.78211764\n",
      "Iteration 3616, loss = 2.78126034\n",
      "Iteration 3617, loss = 2.78040356\n",
      "Iteration 3618, loss = 2.77954732\n",
      "Iteration 3619, loss = 2.77869161\n",
      "Iteration 3620, loss = 2.77783643\n",
      "Iteration 3621, loss = 2.77698178\n",
      "Iteration 3622, loss = 2.77612768\n",
      "Iteration 3623, loss = 2.77527411\n",
      "Iteration 3624, loss = 2.77442107\n",
      "Iteration 3625, loss = 2.77356858\n",
      "Iteration 3626, loss = 2.77271662\n",
      "Iteration 3627, loss = 2.77186520\n",
      "Iteration 3628, loss = 2.77101432\n",
      "Iteration 3629, loss = 2.77016397\n",
      "Iteration 3630, loss = 2.76931416\n",
      "Iteration 3631, loss = 2.76846489\n",
      "Iteration 3632, loss = 2.76761616\n",
      "Iteration 3633, loss = 2.76676796\n",
      "Iteration 3634, loss = 2.76592030\n",
      "Iteration 3635, loss = 2.76507317\n",
      "Iteration 3636, loss = 2.76422658\n",
      "Iteration 3637, loss = 2.76338052\n",
      "Iteration 3638, loss = 2.76253500\n",
      "Iteration 3639, loss = 2.76169000\n",
      "Iteration 3640, loss = 2.76084554\n",
      "Iteration 3641, loss = 2.76000160\n",
      "Iteration 3642, loss = 2.75915820\n",
      "Iteration 3643, loss = 2.75831532\n",
      "Iteration 3644, loss = 2.75747297\n",
      "Iteration 3645, loss = 2.75663115\n",
      "Iteration 3646, loss = 2.75578985\n",
      "Iteration 3647, loss = 2.75494907\n",
      "Iteration 3648, loss = 2.75410882\n",
      "Iteration 3649, loss = 2.75326909\n",
      "Iteration 3650, loss = 2.75242988\n",
      "Iteration 3651, loss = 2.75159119\n",
      "Iteration 3652, loss = 2.75075301\n",
      "Iteration 3653, loss = 2.74991536\n",
      "Iteration 3654, loss = 2.74907822\n",
      "Iteration 3655, loss = 2.74824159\n",
      "Iteration 3656, loss = 2.74740548\n",
      "Iteration 3657, loss = 2.74656988\n",
      "Iteration 3658, loss = 2.74573478\n",
      "Iteration 3659, loss = 2.74490020\n",
      "Iteration 3660, loss = 2.74406613\n",
      "Iteration 3661, loss = 2.74323257\n",
      "Iteration 3662, loss = 2.74239950\n",
      "Iteration 3663, loss = 2.74156695\n",
      "Iteration 3664, loss = 2.74073490\n",
      "Iteration 3665, loss = 2.73990334\n",
      "Iteration 3666, loss = 2.73907229\n",
      "Iteration 3667, loss = 2.73824174\n",
      "Iteration 3668, loss = 2.73741169\n",
      "Iteration 3669, loss = 2.73658213\n",
      "Iteration 3670, loss = 2.73575307\n",
      "Iteration 3671, loss = 2.73492450\n",
      "Iteration 3672, loss = 2.73409642\n",
      "Iteration 3673, loss = 2.73326883\n",
      "Iteration 3674, loss = 2.73244174\n",
      "Iteration 3675, loss = 2.73161513\n",
      "Iteration 3676, loss = 2.73078901\n",
      "Iteration 3677, loss = 2.72996337\n",
      "Iteration 3678, loss = 2.72913822\n",
      "Iteration 3679, loss = 2.72831355\n",
      "Iteration 3680, loss = 2.72748936\n",
      "Iteration 3681, loss = 2.72666565\n",
      "Iteration 3682, loss = 2.72584242\n",
      "Iteration 3683, loss = 2.72501966\n",
      "Iteration 3684, loss = 2.72419738\n",
      "Iteration 3685, loss = 2.72337558\n",
      "Iteration 3686, loss = 2.72255424\n",
      "Iteration 3687, loss = 2.72173338\n",
      "Iteration 3688, loss = 2.72091299\n",
      "Iteration 3689, loss = 2.72009307\n",
      "Iteration 3690, loss = 2.71927361\n",
      "Iteration 3691, loss = 2.71845462\n",
      "Iteration 3692, loss = 2.71763609\n",
      "Iteration 3693, loss = 2.71681803\n",
      "Iteration 3694, loss = 2.71600042\n",
      "Iteration 3695, loss = 2.71518328\n",
      "Iteration 3696, loss = 2.71436659\n",
      "Iteration 3697, loss = 2.71355037\n",
      "Iteration 3698, loss = 2.71273459\n",
      "Iteration 3699, loss = 2.71191927\n",
      "Iteration 3700, loss = 2.71110440\n",
      "Iteration 3701, loss = 2.71028999\n",
      "Iteration 3702, loss = 2.70947602\n",
      "Iteration 3703, loss = 2.70866250\n",
      "Iteration 3704, loss = 2.70784942\n",
      "Iteration 3705, loss = 2.70703680\n",
      "Iteration 3706, loss = 2.70622461\n",
      "Iteration 3707, loss = 2.70541287\n",
      "Iteration 3708, loss = 2.70460156\n",
      "Iteration 3709, loss = 2.70379070\n",
      "Iteration 3710, loss = 2.70298027\n",
      "Iteration 3711, loss = 2.70217028\n",
      "Iteration 3712, loss = 2.70136073\n",
      "Iteration 3713, loss = 2.70055161\n",
      "Iteration 3714, loss = 2.69974292\n",
      "Iteration 3715, loss = 2.69893466\n",
      "Iteration 3716, loss = 2.69812683\n",
      "Iteration 3717, loss = 2.69731942\n",
      "Iteration 3718, loss = 2.69651244\n",
      "Iteration 3719, loss = 2.69570589\n",
      "Iteration 3720, loss = 2.69489976\n",
      "Iteration 3721, loss = 2.69409405\n",
      "Iteration 3722, loss = 2.69328876\n",
      "Iteration 3723, loss = 2.69248389\n",
      "Iteration 3724, loss = 2.69167943\n",
      "Iteration 3725, loss = 2.69087539\n",
      "Iteration 3726, loss = 2.69007177\n",
      "Iteration 3727, loss = 2.68926855\n",
      "Iteration 3728, loss = 2.68846575\n",
      "Iteration 3729, loss = 2.68766336\n",
      "Iteration 3730, loss = 2.68686138\n",
      "Iteration 3731, loss = 2.68606233\n",
      "Iteration 3732, loss = 2.68526442\n",
      "Iteration 3733, loss = 2.68446708\n",
      "Iteration 3734, loss = 2.68367029\n",
      "Iteration 3735, loss = 2.68287402\n",
      "Iteration 3736, loss = 2.68207826\n",
      "Iteration 3737, loss = 2.68128300\n",
      "Iteration 3738, loss = 2.68048820\n",
      "Iteration 3739, loss = 2.67969386\n",
      "Iteration 3740, loss = 2.67889996\n",
      "Iteration 3741, loss = 2.67810650\n",
      "Iteration 3742, loss = 2.67731346\n",
      "Iteration 3743, loss = 2.67652320\n",
      "Iteration 3744, loss = 2.67575651\n",
      "Iteration 3745, loss = 2.67499054\n",
      "Iteration 3746, loss = 2.67422525\n",
      "Iteration 3747, loss = 2.67346061\n",
      "Iteration 3748, loss = 2.67269659\n",
      "Iteration 3749, loss = 2.67193314\n",
      "Iteration 3750, loss = 2.67117025\n",
      "Iteration 3751, loss = 2.67040790\n",
      "Iteration 3752, loss = 2.66964604\n",
      "Iteration 3753, loss = 2.66888467\n",
      "Iteration 3754, loss = 2.66812377\n",
      "Iteration 3755, loss = 2.66736331\n",
      "Iteration 3756, loss = 2.66660327\n",
      "Iteration 3757, loss = 2.66584365\n",
      "Iteration 3758, loss = 2.66508443\n",
      "Iteration 3759, loss = 2.66432558\n",
      "Iteration 3760, loss = 2.66356711\n",
      "Iteration 3761, loss = 2.66280900\n",
      "Iteration 3762, loss = 2.66205123\n",
      "Iteration 3763, loss = 2.66129380\n",
      "Iteration 3764, loss = 2.66053669\n",
      "Iteration 3765, loss = 2.65977991\n",
      "Iteration 3766, loss = 2.65902343\n",
      "Iteration 3767, loss = 2.65826726\n",
      "Iteration 3768, loss = 2.65751139\n",
      "Iteration 3769, loss = 2.65675580\n",
      "Iteration 3770, loss = 2.65600049\n",
      "Iteration 3771, loss = 2.65524547\n",
      "Iteration 3772, loss = 2.65449071\n",
      "Iteration 3773, loss = 2.65373623\n",
      "Iteration 3774, loss = 2.65298200\n",
      "Iteration 3775, loss = 2.65222804\n",
      "Iteration 3776, loss = 2.65147433\n",
      "Iteration 3777, loss = 2.65072087\n",
      "Iteration 3778, loss = 2.64996767\n",
      "Iteration 3779, loss = 2.64920812\n",
      "Iteration 3780, loss = 2.64844101\n",
      "Iteration 3781, loss = 2.64767312\n",
      "Iteration 3782, loss = 2.64690458\n",
      "Iteration 3783, loss = 2.64613546\n",
      "Iteration 3784, loss = 2.64536587\n",
      "Iteration 3785, loss = 2.64459587\n",
      "Iteration 3786, loss = 2.64382554\n",
      "Iteration 3787, loss = 2.64305495\n",
      "Iteration 3788, loss = 2.64228414\n",
      "Iteration 3789, loss = 2.64151318\n",
      "Iteration 3790, loss = 2.64074210\n",
      "Iteration 3791, loss = 2.63997094\n",
      "Iteration 3792, loss = 2.63919975\n",
      "Iteration 3793, loss = 2.63842855\n",
      "Iteration 3794, loss = 2.63765738\n",
      "Iteration 3795, loss = 2.63688626\n",
      "Iteration 3796, loss = 2.63611521\n",
      "Iteration 3797, loss = 2.63534427\n",
      "Iteration 3798, loss = 2.63457343\n",
      "Iteration 3799, loss = 2.63380274\n",
      "Iteration 3800, loss = 2.63303219\n",
      "Iteration 3801, loss = 2.63226180\n",
      "Iteration 3802, loss = 2.63149159\n",
      "Iteration 3803, loss = 2.63072157\n",
      "Iteration 3804, loss = 2.62995174\n",
      "Iteration 3805, loss = 2.62918211\n",
      "Iteration 3806, loss = 2.62841270\n",
      "Iteration 3807, loss = 2.62764351\n",
      "Iteration 3808, loss = 2.62687454\n",
      "Iteration 3809, loss = 2.62610581\n",
      "Iteration 3810, loss = 2.62533731\n",
      "Iteration 3811, loss = 2.62456906\n",
      "Iteration 3812, loss = 2.62380105\n",
      "Iteration 3813, loss = 2.62303329\n",
      "Iteration 3814, loss = 2.62226578\n",
      "Iteration 3815, loss = 2.62149853\n",
      "Iteration 3816, loss = 2.62073153\n",
      "Iteration 3817, loss = 2.61996480\n",
      "Iteration 3818, loss = 2.61919832\n",
      "Iteration 3819, loss = 2.61843211\n",
      "Iteration 3820, loss = 2.61766616\n",
      "Iteration 3821, loss = 2.61690049\n",
      "Iteration 3822, loss = 2.61613507\n",
      "Iteration 3823, loss = 2.61536993\n",
      "Iteration 3824, loss = 2.61460506\n",
      "Iteration 3825, loss = 2.61384045\n",
      "Iteration 3826, loss = 2.61307612\n",
      "Iteration 3827, loss = 2.61231205\n",
      "Iteration 3828, loss = 2.61154826\n",
      "Iteration 3829, loss = 2.61078474\n",
      "Iteration 3830, loss = 2.61002150\n",
      "Iteration 3831, loss = 2.60925852\n",
      "Iteration 3832, loss = 2.60849582\n",
      "Iteration 3833, loss = 2.60773339\n",
      "Iteration 3834, loss = 2.60697123\n",
      "Iteration 3835, loss = 2.60620935\n",
      "Iteration 3836, loss = 2.60544774\n",
      "Iteration 3837, loss = 2.60468640\n",
      "Iteration 3838, loss = 2.60392533\n",
      "Iteration 3839, loss = 2.60316454\n",
      "Iteration 3840, loss = 2.60240401\n",
      "Iteration 3841, loss = 2.60164376\n",
      "Iteration 3842, loss = 2.60088379\n",
      "Iteration 3843, loss = 2.60012408\n",
      "Iteration 3844, loss = 2.59936464\n",
      "Iteration 3845, loss = 2.59860548\n",
      "Iteration 3846, loss = 2.59784658\n",
      "Iteration 3847, loss = 2.59708796\n",
      "Iteration 3848, loss = 2.59632961\n",
      "Iteration 3849, loss = 2.59557153\n",
      "Iteration 3850, loss = 2.59481371\n",
      "Iteration 3851, loss = 2.59405619\n",
      "Iteration 3852, loss = 2.59329895\n",
      "Iteration 3853, loss = 2.59254197\n",
      "Iteration 3854, loss = 2.59178527\n",
      "Iteration 3855, loss = 2.59102883\n",
      "Iteration 3856, loss = 2.59027267\n",
      "Iteration 3857, loss = 2.58951677\n",
      "Iteration 3858, loss = 2.58876114\n",
      "Iteration 3859, loss = 2.58800578\n",
      "Iteration 3860, loss = 2.58725069\n",
      "Iteration 3861, loss = 2.58649586\n",
      "Iteration 3862, loss = 2.58574130\n",
      "Iteration 3863, loss = 2.58498701\n",
      "Iteration 3864, loss = 2.58423298\n",
      "Iteration 3865, loss = 2.58347921\n",
      "Iteration 3866, loss = 2.58272571\n",
      "Iteration 3867, loss = 2.58197247\n",
      "Iteration 3868, loss = 2.58121950\n",
      "Iteration 3869, loss = 2.58046679\n",
      "Iteration 3870, loss = 2.57971434\n",
      "Iteration 3871, loss = 2.57896215\n",
      "Iteration 3872, loss = 2.57821023\n",
      "Iteration 3873, loss = 2.57745856\n",
      "Iteration 3874, loss = 2.57670716\n",
      "Iteration 3875, loss = 2.57595602\n",
      "Iteration 3876, loss = 2.57520513\n",
      "Iteration 3877, loss = 2.57445372\n",
      "Iteration 3878, loss = 2.57369465\n",
      "Iteration 3879, loss = 2.57293519\n",
      "Iteration 3880, loss = 2.57217542\n",
      "Iteration 3881, loss = 2.57141540\n",
      "Iteration 3882, loss = 2.57065518\n",
      "Iteration 3883, loss = 2.56989483\n",
      "Iteration 3884, loss = 2.56913438\n",
      "Iteration 3885, loss = 2.56837389\n",
      "Iteration 3886, loss = 2.56761338\n",
      "Iteration 3887, loss = 2.56685290\n",
      "Iteration 3888, loss = 2.56609248\n",
      "Iteration 3889, loss = 2.56533214\n",
      "Iteration 3890, loss = 2.56457191\n",
      "Iteration 3891, loss = 2.56381180\n",
      "Iteration 3892, loss = 2.56305185\n",
      "Iteration 3893, loss = 2.56229207\n",
      "Iteration 3894, loss = 2.56153248\n",
      "Iteration 3895, loss = 2.56077308\n",
      "Iteration 3896, loss = 2.56001390\n",
      "Iteration 3897, loss = 2.55925494\n",
      "Iteration 3898, loss = 2.55849622\n",
      "Iteration 3899, loss = 2.55773775\n",
      "Iteration 3900, loss = 2.55697952\n",
      "Iteration 3901, loss = 2.55622156\n",
      "Iteration 3902, loss = 2.55546387\n",
      "Iteration 3903, loss = 2.55470645\n",
      "Iteration 3904, loss = 2.55394932\n",
      "Iteration 3905, loss = 2.55319246\n",
      "Iteration 3906, loss = 2.55243590\n",
      "Iteration 3907, loss = 2.55167963\n",
      "Iteration 3908, loss = 2.55092365\n",
      "Iteration 3909, loss = 2.55016797\n",
      "Iteration 3910, loss = 2.54941260\n",
      "Iteration 3911, loss = 2.54865753\n",
      "Iteration 3912, loss = 2.54790276\n",
      "Iteration 3913, loss = 2.54714831\n",
      "Iteration 3914, loss = 2.54639416\n",
      "Iteration 3915, loss = 2.54564032\n",
      "Iteration 3916, loss = 2.54488680\n",
      "Iteration 3917, loss = 2.54413359\n",
      "Iteration 3918, loss = 2.54338069\n",
      "Iteration 3919, loss = 2.54262811\n",
      "Iteration 3920, loss = 2.54187585\n",
      "Iteration 3921, loss = 2.54112390\n",
      "Iteration 3922, loss = 2.54037227\n",
      "Iteration 3923, loss = 2.53962095\n",
      "Iteration 3924, loss = 2.53886995\n",
      "Iteration 3925, loss = 2.53811927\n",
      "Iteration 3926, loss = 2.53736891\n",
      "Iteration 3927, loss = 2.53661886\n",
      "Iteration 3928, loss = 2.53586913\n",
      "Iteration 3929, loss = 2.53511972\n",
      "Iteration 3930, loss = 2.53437133\n",
      "Iteration 3931, loss = 2.53362519\n",
      "Iteration 3932, loss = 2.53287955\n",
      "Iteration 3933, loss = 2.53213439\n",
      "Iteration 3934, loss = 2.53138970\n",
      "Iteration 3935, loss = 2.53064545\n",
      "Iteration 3936, loss = 2.52990164\n",
      "Iteration 3937, loss = 2.52915825\n",
      "Iteration 3938, loss = 2.52841527\n",
      "Iteration 3939, loss = 2.52767269\n",
      "Iteration 3940, loss = 2.52693051\n",
      "Iteration 3941, loss = 2.52618871\n",
      "Iteration 3942, loss = 2.52544729\n",
      "Iteration 3943, loss = 2.52470624\n",
      "Iteration 3944, loss = 2.52396555\n",
      "Iteration 3945, loss = 2.52322522\n",
      "Iteration 3946, loss = 2.52248525\n",
      "Iteration 3947, loss = 2.52174562\n",
      "Iteration 3948, loss = 2.52100634\n",
      "Iteration 3949, loss = 2.52026740\n",
      "Iteration 3950, loss = 2.51952880\n",
      "Iteration 3951, loss = 2.51879053\n",
      "Iteration 3952, loss = 2.51805259\n",
      "Iteration 3953, loss = 2.51731498\n",
      "Iteration 3954, loss = 2.51657769\n",
      "Iteration 3955, loss = 2.51584074\n",
      "Iteration 3956, loss = 2.51510410\n",
      "Iteration 3957, loss = 2.51436778\n",
      "Iteration 3958, loss = 2.51363178\n",
      "Iteration 3959, loss = 2.51289610\n",
      "Iteration 3960, loss = 2.51216073\n",
      "Iteration 3961, loss = 2.51142567\n",
      "Iteration 3962, loss = 2.51069093\n",
      "Iteration 3963, loss = 2.50995650\n",
      "Iteration 3964, loss = 2.50922238\n",
      "Iteration 3965, loss = 2.50848856\n",
      "Iteration 3966, loss = 2.50775505\n",
      "Iteration 3967, loss = 2.50702185\n",
      "Iteration 3968, loss = 2.50628896\n",
      "Iteration 3969, loss = 2.50555637\n",
      "Iteration 3970, loss = 2.50482408\n",
      "Iteration 3971, loss = 2.50409210\n",
      "Iteration 3972, loss = 2.50336042\n",
      "Iteration 3973, loss = 2.50262904\n",
      "Iteration 3974, loss = 2.50189796\n",
      "Iteration 3975, loss = 2.50116718\n",
      "Iteration 3976, loss = 2.50043670\n",
      "Iteration 3977, loss = 2.49970652\n",
      "Iteration 3978, loss = 2.49897664\n",
      "Iteration 3979, loss = 2.49824705\n",
      "Iteration 3980, loss = 2.49751776\n",
      "Iteration 3981, loss = 2.49678877\n",
      "Iteration 3982, loss = 2.49606007\n",
      "Iteration 3983, loss = 2.49533166\n",
      "Iteration 3984, loss = 2.49460355\n",
      "Iteration 3985, loss = 2.49387574\n",
      "Iteration 3986, loss = 2.49314821\n",
      "Iteration 3987, loss = 2.49242098\n",
      "Iteration 3988, loss = 2.49169404\n",
      "Iteration 3989, loss = 2.49096740\n",
      "Iteration 3990, loss = 2.49024104\n",
      "Iteration 3991, loss = 2.48951497\n",
      "Iteration 3992, loss = 2.48878919\n",
      "Iteration 3993, loss = 2.48806371\n",
      "Iteration 3994, loss = 2.48733851\n",
      "Iteration 3995, loss = 2.48661359\n",
      "Iteration 3996, loss = 2.48588897\n",
      "Iteration 3997, loss = 2.48516463\n",
      "Iteration 3998, loss = 2.48444058\n",
      "Iteration 3999, loss = 2.48371682\n",
      "Iteration 4000, loss = 2.48299334\n",
      "Iteration 4001, loss = 2.48227013\n",
      "Iteration 4002, loss = 2.48154715\n",
      "Iteration 4003, loss = 2.48082445\n",
      "Iteration 4004, loss = 2.48010203\n",
      "Iteration 4005, loss = 2.47937989\n",
      "Iteration 4006, loss = 2.47865803\n",
      "Iteration 4007, loss = 2.47793645\n",
      "Iteration 4008, loss = 2.47721515\n",
      "Iteration 4009, loss = 2.47649412\n",
      "Iteration 4010, loss = 2.47577338\n",
      "Iteration 4011, loss = 2.47505291\n",
      "Iteration 4012, loss = 2.47433272\n",
      "Iteration 4013, loss = 2.47361281\n",
      "Iteration 4014, loss = 2.47289317\n",
      "Iteration 4015, loss = 2.47217381\n",
      "Iteration 4016, loss = 2.47145473\n",
      "Iteration 4017, loss = 2.47073592\n",
      "Iteration 4018, loss = 2.47001739\n",
      "Iteration 4019, loss = 2.46929913\n",
      "Iteration 4020, loss = 2.46858115\n",
      "Iteration 4021, loss = 2.46786344\n",
      "Iteration 4022, loss = 2.46714600\n",
      "Iteration 4023, loss = 2.46642884\n",
      "Iteration 4024, loss = 2.46571194\n",
      "Iteration 4025, loss = 2.46499532\n",
      "Iteration 4026, loss = 2.46427898\n",
      "Iteration 4027, loss = 2.46356290\n",
      "Iteration 4028, loss = 2.46284709\n",
      "Iteration 4029, loss = 2.46213155\n",
      "Iteration 4030, loss = 2.46141629\n",
      "Iteration 4031, loss = 2.46070129\n",
      "Iteration 4032, loss = 2.45998656\n",
      "Iteration 4033, loss = 2.45927210\n",
      "Iteration 4034, loss = 2.45855790\n",
      "Iteration 4035, loss = 2.45784397\n",
      "Iteration 4036, loss = 2.45713031\n",
      "Iteration 4037, loss = 2.45641691\n",
      "Iteration 4038, loss = 2.45570378\n",
      "Iteration 4039, loss = 2.45499092\n",
      "Iteration 4040, loss = 2.45427831\n",
      "Iteration 4041, loss = 2.45356598\n",
      "Iteration 4042, loss = 2.45285390\n",
      "Iteration 4043, loss = 2.45214209\n",
      "Iteration 4044, loss = 2.45143054\n",
      "Iteration 4045, loss = 2.45071925\n",
      "Iteration 4046, loss = 2.45000822\n",
      "Iteration 4047, loss = 2.44929745\n",
      "Iteration 4048, loss = 2.44858694\n",
      "Iteration 4049, loss = 2.44787669\n",
      "Iteration 4050, loss = 2.44716670\n",
      "Iteration 4051, loss = 2.44645697\n",
      "Iteration 4052, loss = 2.44574750\n",
      "Iteration 4053, loss = 2.44503828\n",
      "Iteration 4054, loss = 2.44432932\n",
      "Iteration 4055, loss = 2.44362061\n",
      "Iteration 4056, loss = 2.44291216\n",
      "Iteration 4057, loss = 2.44220397\n",
      "Iteration 4058, loss = 2.44149603\n",
      "Iteration 4059, loss = 2.44078835\n",
      "Iteration 4060, loss = 2.44008091\n",
      "Iteration 4061, loss = 2.43937373\n",
      "Iteration 4062, loss = 2.43866681\n",
      "Iteration 4063, loss = 2.43796013\n",
      "Iteration 4064, loss = 2.43725371\n",
      "Iteration 4065, loss = 2.43654754\n",
      "Iteration 4066, loss = 2.43584161\n",
      "Iteration 4067, loss = 2.43513594\n",
      "Iteration 4068, loss = 2.43443051\n",
      "Iteration 4069, loss = 2.43372534\n",
      "Iteration 4070, loss = 2.43302041\n",
      "Iteration 4071, loss = 2.43231573\n",
      "Iteration 4072, loss = 2.43161130\n",
      "Iteration 4073, loss = 2.43090711\n",
      "Iteration 4074, loss = 2.43020317\n",
      "Iteration 4075, loss = 2.42949948\n",
      "Iteration 4076, loss = 2.42879602\n",
      "Iteration 4077, loss = 2.42809282\n",
      "Iteration 4078, loss = 2.42738986\n",
      "Iteration 4079, loss = 2.42668714\n",
      "Iteration 4080, loss = 2.42598466\n",
      "Iteration 4081, loss = 2.42528243\n",
      "Iteration 4082, loss = 2.42458043\n",
      "Iteration 4083, loss = 2.42387868\n",
      "Iteration 4084, loss = 2.42317717\n",
      "Iteration 4085, loss = 2.42247590\n",
      "Iteration 4086, loss = 2.42177487\n",
      "Iteration 4087, loss = 2.42107408\n",
      "Iteration 4088, loss = 2.42037352\n",
      "Iteration 4089, loss = 2.41967321\n",
      "Iteration 4090, loss = 2.41897313\n",
      "Iteration 4091, loss = 2.41827329\n",
      "Iteration 4092, loss = 2.41757368\n",
      "Iteration 4093, loss = 2.41687431\n",
      "Iteration 4094, loss = 2.41617518\n",
      "Iteration 4095, loss = 2.41547628\n",
      "Iteration 4096, loss = 2.41477762\n",
      "Iteration 4097, loss = 2.41407919\n",
      "Iteration 4098, loss = 2.41338099\n",
      "Iteration 4099, loss = 2.41268303\n",
      "Iteration 4100, loss = 2.41198529\n",
      "Iteration 4101, loss = 2.41128779\n",
      "Iteration 4102, loss = 2.41059053\n",
      "Iteration 4103, loss = 2.40989349\n",
      "Iteration 4104, loss = 2.40919668\n",
      "Iteration 4105, loss = 2.40850010\n",
      "Iteration 4106, loss = 2.40780376\n",
      "Iteration 4107, loss = 2.40710764\n",
      "Iteration 4108, loss = 2.40641175\n",
      "Iteration 4109, loss = 2.40571608\n",
      "Iteration 4110, loss = 2.40502065\n",
      "Iteration 4111, loss = 2.40432544\n",
      "Iteration 4112, loss = 2.40363046\n",
      "Iteration 4113, loss = 2.40293570\n",
      "Iteration 4114, loss = 2.40224117\n",
      "Iteration 4115, loss = 2.40154686\n",
      "Iteration 4116, loss = 2.40085278\n",
      "Iteration 4117, loss = 2.40015892\n",
      "Iteration 4118, loss = 2.39946532\n",
      "Iteration 4119, loss = 2.39877194\n",
      "Iteration 4120, loss = 2.39807880\n",
      "Iteration 4121, loss = 2.39738587\n",
      "Iteration 4122, loss = 2.39669317\n",
      "Iteration 4123, loss = 2.39600069\n",
      "Iteration 4124, loss = 2.39530843\n",
      "Iteration 4125, loss = 2.39461640\n",
      "Iteration 4126, loss = 2.39392458\n",
      "Iteration 4127, loss = 2.39323298\n",
      "Iteration 4128, loss = 2.39254161\n",
      "Iteration 4129, loss = 2.39185045\n",
      "Iteration 4130, loss = 2.39115951\n",
      "Iteration 4131, loss = 2.39046879\n",
      "Iteration 4132, loss = 2.38977829\n",
      "Iteration 4133, loss = 2.38908801\n",
      "Iteration 4134, loss = 2.38839794\n",
      "Iteration 4135, loss = 2.38770808\n",
      "Iteration 4136, loss = 2.38701845\n",
      "Iteration 4137, loss = 2.38632902\n",
      "Iteration 4138, loss = 2.38563982\n",
      "Iteration 4139, loss = 2.38495082\n",
      "Iteration 4140, loss = 2.38426204\n",
      "Iteration 4141, loss = 2.38357347\n",
      "Iteration 4142, loss = 2.38288512\n",
      "Iteration 4143, loss = 2.38219698\n",
      "Iteration 4144, loss = 2.38150905\n",
      "Iteration 4145, loss = 2.38082133\n",
      "Iteration 4146, loss = 2.38013382\n",
      "Iteration 4147, loss = 2.37944652\n",
      "Iteration 4148, loss = 2.37875943\n",
      "Iteration 4149, loss = 2.37807255\n",
      "Iteration 4150, loss = 2.37738589\n",
      "Iteration 4151, loss = 2.37669943\n",
      "Iteration 4152, loss = 2.37601317\n",
      "Iteration 4153, loss = 2.37532713\n",
      "Iteration 4154, loss = 2.37464129\n",
      "Iteration 4155, loss = 2.37395566\n",
      "Iteration 4156, loss = 2.37327024\n",
      "Iteration 4157, loss = 2.37258502\n",
      "Iteration 4158, loss = 2.37190001\n",
      "Iteration 4159, loss = 2.37121521\n",
      "Iteration 4160, loss = 2.37053061\n",
      "Iteration 4161, loss = 2.36984621\n",
      "Iteration 4162, loss = 2.36916202\n",
      "Iteration 4163, loss = 2.36847803\n",
      "Iteration 4164, loss = 2.36779425\n",
      "Iteration 4165, loss = 2.36711065\n",
      "Iteration 4166, loss = 2.36642726\n",
      "Iteration 4167, loss = 2.36574407\n",
      "Iteration 4168, loss = 2.36506109\n",
      "Iteration 4169, loss = 2.36437830\n",
      "Iteration 4170, loss = 2.36369571\n",
      "Iteration 4171, loss = 2.36301333\n",
      "Iteration 4172, loss = 2.36233114\n",
      "Iteration 4173, loss = 2.36164916\n",
      "Iteration 4174, loss = 2.36096737\n",
      "Iteration 4175, loss = 2.36028578\n",
      "Iteration 4176, loss = 2.35960440\n",
      "Iteration 4177, loss = 2.35892321\n",
      "Iteration 4178, loss = 2.35824221\n",
      "Iteration 4179, loss = 2.35756142\n",
      "Iteration 4180, loss = 2.35688083\n",
      "Iteration 4181, loss = 2.35620043\n",
      "Iteration 4182, loss = 2.35552023\n",
      "Iteration 4183, loss = 2.35484022\n",
      "Iteration 4184, loss = 2.35416041\n",
      "Iteration 4185, loss = 2.35348080\n",
      "Iteration 4186, loss = 2.35280139\n",
      "Iteration 4187, loss = 2.35212216\n",
      "Iteration 4188, loss = 2.35144314\n",
      "Iteration 4189, loss = 2.35076431\n",
      "Iteration 4190, loss = 2.35008567\n",
      "Iteration 4191, loss = 2.34940723\n",
      "Iteration 4192, loss = 2.34872899\n",
      "Iteration 4193, loss = 2.34805093\n",
      "Iteration 4194, loss = 2.34737307\n",
      "Iteration 4195, loss = 2.34669541\n",
      "Iteration 4196, loss = 2.34601793\n",
      "Iteration 4197, loss = 2.34534065\n",
      "Iteration 4198, loss = 2.34466357\n",
      "Iteration 4199, loss = 2.34398667\n",
      "Iteration 4200, loss = 2.34330997\n",
      "Iteration 4201, loss = 2.34263346\n",
      "Iteration 4202, loss = 2.34195714\n",
      "Iteration 4203, loss = 2.34128101\n",
      "Iteration 4204, loss = 2.34060507\n",
      "Iteration 4205, loss = 2.33992932\n",
      "Iteration 4206, loss = 2.33925377\n",
      "Iteration 4207, loss = 2.33857840\n",
      "Iteration 4208, loss = 2.33790322\n",
      "Iteration 4209, loss = 2.33722824\n",
      "Iteration 4210, loss = 2.33655344\n",
      "Iteration 4211, loss = 2.33587883\n",
      "Iteration 4212, loss = 2.33520442\n",
      "Iteration 4213, loss = 2.33453019\n",
      "Iteration 4214, loss = 2.33385615\n",
      "Iteration 4215, loss = 2.33318230\n",
      "Iteration 4216, loss = 2.33250863\n",
      "Iteration 4217, loss = 2.33183516\n",
      "Iteration 4218, loss = 2.33116187\n",
      "Iteration 4219, loss = 2.33048877\n",
      "Iteration 4220, loss = 2.32981586\n",
      "Iteration 4221, loss = 2.32914313\n",
      "Iteration 4222, loss = 2.32847060\n",
      "Iteration 4223, loss = 2.32779825\n",
      "Iteration 4224, loss = 2.32712608\n",
      "Iteration 4225, loss = 2.32645411\n",
      "Iteration 4226, loss = 2.32578320\n",
      "Iteration 4227, loss = 2.32511383\n",
      "Iteration 4228, loss = 2.32444479\n",
      "Iteration 4229, loss = 2.32377608\n",
      "Iteration 4230, loss = 2.32310767\n",
      "Iteration 4231, loss = 2.32243955\n",
      "Iteration 4232, loss = 2.32177172\n",
      "Iteration 4233, loss = 2.32110416\n",
      "Iteration 4234, loss = 2.32043687\n",
      "Iteration 4235, loss = 2.31976983\n",
      "Iteration 4236, loss = 2.31910304\n",
      "Iteration 4237, loss = 2.31843649\n",
      "Iteration 4238, loss = 2.31777018\n",
      "Iteration 4239, loss = 2.31710410\n",
      "Iteration 4240, loss = 2.31643825\n",
      "Iteration 4241, loss = 2.31577262\n",
      "Iteration 4242, loss = 2.31510720\n",
      "Iteration 4243, loss = 2.31444200\n",
      "Iteration 4244, loss = 2.31377701\n",
      "Iteration 4245, loss = 2.31311223\n",
      "Iteration 4246, loss = 2.31244766\n",
      "Iteration 4247, loss = 2.31178328\n",
      "Iteration 4248, loss = 2.31111911\n",
      "Iteration 4249, loss = 2.31045514\n",
      "Iteration 4250, loss = 2.30979137\n",
      "Iteration 4251, loss = 2.30912779\n",
      "Iteration 4252, loss = 2.30846440\n",
      "Iteration 4253, loss = 2.30780121\n",
      "Iteration 4254, loss = 2.30713821\n",
      "Iteration 4255, loss = 2.30647540\n",
      "Iteration 4256, loss = 2.30581279\n",
      "Iteration 4257, loss = 2.30515036\n",
      "Iteration 4258, loss = 2.30448812\n",
      "Iteration 4259, loss = 2.30382607\n",
      "Iteration 4260, loss = 2.30316420\n",
      "Iteration 4261, loss = 2.30250252\n",
      "Iteration 4262, loss = 2.30184103\n",
      "Iteration 4263, loss = 2.30117972\n",
      "Iteration 4264, loss = 2.30052006\n",
      "Iteration 4265, loss = 2.29986095\n",
      "Iteration 4266, loss = 2.29920216\n",
      "Iteration 4267, loss = 2.29854365\n",
      "Iteration 4268, loss = 2.29788542\n",
      "Iteration 4269, loss = 2.29722747\n",
      "Iteration 4270, loss = 2.29656978\n",
      "Iteration 4271, loss = 2.29591234\n",
      "Iteration 4272, loss = 2.29525516\n",
      "Iteration 4273, loss = 2.29459821\n",
      "Iteration 4274, loss = 2.29394149\n",
      "Iteration 4275, loss = 2.29328501\n",
      "Iteration 4276, loss = 2.29262875\n",
      "Iteration 4277, loss = 2.29197272\n",
      "Iteration 4278, loss = 2.29131690\n",
      "Iteration 4279, loss = 2.29066129\n",
      "Iteration 4280, loss = 2.29000590\n",
      "Iteration 4281, loss = 2.28935071\n",
      "Iteration 4282, loss = 2.28869573\n",
      "Iteration 4283, loss = 2.28804095\n",
      "Iteration 4284, loss = 2.28738637\n",
      "Iteration 4285, loss = 2.28673199\n",
      "Iteration 4286, loss = 2.28607781\n",
      "Iteration 4287, loss = 2.28542382\n",
      "Iteration 4288, loss = 2.28477003\n",
      "Iteration 4289, loss = 2.28411643\n",
      "Iteration 4290, loss = 2.28346303\n",
      "Iteration 4291, loss = 2.28280982\n",
      "Iteration 4292, loss = 2.28215680\n",
      "Iteration 4293, loss = 2.28150397\n",
      "Iteration 4294, loss = 2.28085133\n",
      "Iteration 4295, loss = 2.28019888\n",
      "Iteration 4296, loss = 2.27954662\n",
      "Iteration 4297, loss = 2.27889454\n",
      "Iteration 4298, loss = 2.27824266\n",
      "Iteration 4299, loss = 2.27759096\n",
      "Iteration 4300, loss = 2.27693945\n",
      "Iteration 4301, loss = 2.27628812\n",
      "Iteration 4302, loss = 2.27563699\n",
      "Iteration 4303, loss = 2.27498604\n",
      "Iteration 4304, loss = 2.27433531\n",
      "Iteration 4305, loss = 2.27368482\n",
      "Iteration 4306, loss = 2.27303452\n",
      "Iteration 4307, loss = 2.27238442\n",
      "Iteration 4308, loss = 2.27173450\n",
      "Iteration 4309, loss = 2.27108478\n",
      "Iteration 4310, loss = 2.27043524\n",
      "Iteration 4311, loss = 2.26978589\n",
      "Iteration 4312, loss = 2.26913674\n",
      "Iteration 4313, loss = 2.26848777\n",
      "Iteration 4314, loss = 2.26783899\n",
      "Iteration 4315, loss = 2.26719040\n",
      "Iteration 4316, loss = 2.26654199\n",
      "Iteration 4317, loss = 2.26589378\n",
      "Iteration 4318, loss = 2.26524575\n",
      "Iteration 4319, loss = 2.26459792\n",
      "Iteration 4320, loss = 2.26395027\n",
      "Iteration 4321, loss = 2.26330281\n",
      "Iteration 4322, loss = 2.26265553\n",
      "Iteration 4323, loss = 2.26200845\n",
      "Iteration 4324, loss = 2.26136155\n",
      "Iteration 4325, loss = 2.26071485\n",
      "Iteration 4326, loss = 2.26006833\n",
      "Iteration 4327, loss = 2.25942200\n",
      "Iteration 4328, loss = 2.25877586\n",
      "Iteration 4329, loss = 2.25812991\n",
      "Iteration 4330, loss = 2.25748415\n",
      "Iteration 4331, loss = 2.25683858\n",
      "Iteration 4332, loss = 2.25619320\n",
      "Iteration 4333, loss = 2.25554800\n",
      "Iteration 4334, loss = 2.25490300\n",
      "Iteration 4335, loss = 2.25425819\n",
      "Iteration 4336, loss = 2.25361357\n",
      "Iteration 4337, loss = 2.25296914\n",
      "Iteration 4338, loss = 2.25232490\n",
      "Iteration 4339, loss = 2.25168085\n",
      "Iteration 4340, loss = 2.25103699\n",
      "Iteration 4341, loss = 2.25039332\n",
      "Iteration 4342, loss = 2.24974985\n",
      "Iteration 4343, loss = 2.24910657\n",
      "Iteration 4344, loss = 2.24846348\n",
      "Iteration 4345, loss = 2.24782058\n",
      "Iteration 4346, loss = 2.24717787\n",
      "Iteration 4347, loss = 2.24653888\n",
      "Iteration 4348, loss = 2.24590423\n",
      "Iteration 4349, loss = 2.24527019\n",
      "Iteration 4350, loss = 2.24463672\n",
      "Iteration 4351, loss = 2.24400378\n",
      "Iteration 4352, loss = 2.24337133\n",
      "Iteration 4353, loss = 2.24273933\n",
      "Iteration 4354, loss = 2.24210777\n",
      "Iteration 4355, loss = 2.24147660\n",
      "Iteration 4356, loss = 2.24084581\n",
      "Iteration 4357, loss = 2.24021538\n",
      "Iteration 4358, loss = 2.23958529\n",
      "Iteration 4359, loss = 2.23895551\n",
      "Iteration 4360, loss = 2.23832603\n",
      "Iteration 4361, loss = 2.23769685\n",
      "Iteration 4362, loss = 2.23706794\n",
      "Iteration 4363, loss = 2.23643929\n",
      "Iteration 4364, loss = 2.23581090\n",
      "Iteration 4365, loss = 2.23518275\n",
      "Iteration 4366, loss = 2.23455484\n",
      "Iteration 4367, loss = 2.23392717\n",
      "Iteration 4368, loss = 2.23329971\n",
      "Iteration 4369, loss = 2.23267248\n",
      "Iteration 4370, loss = 2.23204545\n",
      "Iteration 4371, loss = 2.23141864\n",
      "Iteration 4372, loss = 2.23079203\n",
      "Iteration 4373, loss = 2.23016562\n",
      "Iteration 4374, loss = 2.22953941\n",
      "Iteration 4375, loss = 2.22891340\n",
      "Iteration 4376, loss = 2.22828757\n",
      "Iteration 4377, loss = 2.22766194\n",
      "Iteration 4378, loss = 2.22703650\n",
      "Iteration 4379, loss = 2.22641125\n",
      "Iteration 4380, loss = 2.22578618\n",
      "Iteration 4381, loss = 2.22516129\n",
      "Iteration 4382, loss = 2.22453659\n",
      "Iteration 4383, loss = 2.22391208\n",
      "Iteration 4384, loss = 2.22328774\n",
      "Iteration 4385, loss = 2.22266359\n",
      "Iteration 4386, loss = 2.22203962\n",
      "Iteration 4387, loss = 2.22141583\n",
      "Iteration 4388, loss = 2.22079223\n",
      "Iteration 4389, loss = 2.22016880\n",
      "Iteration 4390, loss = 2.21954556\n",
      "Iteration 4391, loss = 2.21892249\n",
      "Iteration 4392, loss = 2.21829961\n",
      "Iteration 4393, loss = 2.21767691\n",
      "Iteration 4394, loss = 2.21705439\n",
      "Iteration 4395, loss = 2.21643206\n",
      "Iteration 4396, loss = 2.21580990\n",
      "Iteration 4397, loss = 2.21518793\n",
      "Iteration 4398, loss = 2.21456614\n",
      "Iteration 4399, loss = 2.21394454\n",
      "Iteration 4400, loss = 2.21332312\n",
      "Iteration 4401, loss = 2.21270188\n",
      "Iteration 4402, loss = 2.21208083\n",
      "Iteration 4403, loss = 2.21145997\n",
      "Iteration 4404, loss = 2.21083929\n",
      "Iteration 4405, loss = 2.21021880\n",
      "Iteration 4406, loss = 2.20959849\n",
      "Iteration 4407, loss = 2.20897838\n",
      "Iteration 4408, loss = 2.20835845\n",
      "Iteration 4409, loss = 2.20773871\n",
      "Iteration 4410, loss = 2.20712625\n",
      "Iteration 4411, loss = 2.20651862\n",
      "Iteration 4412, loss = 2.20591166\n",
      "Iteration 4413, loss = 2.20530530\n",
      "Iteration 4414, loss = 2.20469950\n",
      "Iteration 4415, loss = 2.20409421\n",
      "Iteration 4416, loss = 2.20348941\n",
      "Iteration 4417, loss = 2.20288504\n",
      "Iteration 4418, loss = 2.20228109\n",
      "Iteration 4419, loss = 2.20167752\n",
      "Iteration 4420, loss = 2.20107431\n",
      "Iteration 4421, loss = 2.20047144\n",
      "Iteration 4422, loss = 2.19986888\n",
      "Iteration 4423, loss = 2.19926662\n",
      "Iteration 4424, loss = 2.19866464\n",
      "Iteration 4425, loss = 2.19806292\n",
      "Iteration 4426, loss = 2.19746146\n",
      "Iteration 4427, loss = 2.19686024\n",
      "Iteration 4428, loss = 2.19625925\n",
      "Iteration 4429, loss = 2.19565848\n",
      "Iteration 4430, loss = 2.19505792\n",
      "Iteration 4431, loss = 2.19445756\n",
      "Iteration 4432, loss = 2.19385741\n",
      "Iteration 4433, loss = 2.19325744\n",
      "Iteration 4434, loss = 2.19265766\n",
      "Iteration 4435, loss = 2.19205806\n",
      "Iteration 4436, loss = 2.19145863\n",
      "Iteration 4437, loss = 2.19085938\n",
      "Iteration 4438, loss = 2.19026030\n",
      "Iteration 4439, loss = 2.18966139\n",
      "Iteration 4440, loss = 2.18906263\n",
      "Iteration 4441, loss = 2.18846404\n",
      "Iteration 4442, loss = 2.18786561\n",
      "Iteration 4443, loss = 2.18726733\n",
      "Iteration 4444, loss = 2.18666921\n",
      "Iteration 4445, loss = 2.18607125\n",
      "Iteration 4446, loss = 2.18547343\n",
      "Iteration 4447, loss = 2.18487577\n",
      "Iteration 4448, loss = 2.18427826\n",
      "Iteration 4449, loss = 2.18368090\n",
      "Iteration 4450, loss = 2.18308369\n",
      "Iteration 4451, loss = 2.18248662\n",
      "Iteration 4452, loss = 2.18188971\n",
      "Iteration 4453, loss = 2.18129294\n",
      "Iteration 4454, loss = 2.18069632\n",
      "Iteration 4455, loss = 2.18009985\n",
      "Iteration 4456, loss = 2.17950352\n",
      "Iteration 4457, loss = 2.17890735\n",
      "Iteration 4458, loss = 2.17831132\n",
      "Iteration 4459, loss = 2.17771543\n",
      "Iteration 4460, loss = 2.17711970\n",
      "Iteration 4461, loss = 2.17652411\n",
      "Iteration 4462, loss = 2.17592867\n",
      "Iteration 4463, loss = 2.17533337\n",
      "Iteration 4464, loss = 2.17473823\n",
      "Iteration 4465, loss = 2.17414323\n",
      "Iteration 4466, loss = 2.17354838\n",
      "Iteration 4467, loss = 2.17295368\n",
      "Iteration 4468, loss = 2.17235912\n",
      "Iteration 4469, loss = 2.17176472\n",
      "Iteration 4470, loss = 2.17117047\n",
      "Iteration 4471, loss = 2.17057636\n",
      "Iteration 4472, loss = 2.16998241\n",
      "Iteration 4473, loss = 2.16938860\n",
      "Iteration 4474, loss = 2.16879495\n",
      "Iteration 4475, loss = 2.16820145\n",
      "Iteration 4476, loss = 2.16760809\n",
      "Iteration 4477, loss = 2.16701489\n",
      "Iteration 4478, loss = 2.16642185\n",
      "Iteration 4479, loss = 2.16582895\n",
      "Iteration 4480, loss = 2.16523621\n",
      "Iteration 4481, loss = 2.16464362\n",
      "Iteration 4482, loss = 2.16405118\n",
      "Iteration 4483, loss = 2.16345890\n",
      "Iteration 4484, loss = 2.16286677\n",
      "Iteration 4485, loss = 2.16227480\n",
      "Iteration 4486, loss = 2.16168298\n",
      "Iteration 4487, loss = 2.16109132\n",
      "Iteration 4488, loss = 2.16049982\n",
      "Iteration 4489, loss = 2.15990847\n",
      "Iteration 4490, loss = 2.15931727\n",
      "Iteration 4491, loss = 2.15872624\n",
      "Iteration 4492, loss = 2.15813536\n",
      "Iteration 4493, loss = 2.15754464\n",
      "Iteration 4494, loss = 2.15695407\n",
      "Iteration 4495, loss = 2.15636367\n",
      "Iteration 4496, loss = 2.15577343\n",
      "Iteration 4497, loss = 2.15518334\n",
      "Iteration 4498, loss = 2.15459342\n",
      "Iteration 4499, loss = 2.15400365\n",
      "Iteration 4500, loss = 2.15341405\n",
      "Iteration 4501, loss = 2.15282460\n",
      "Iteration 4502, loss = 2.15223532\n",
      "Iteration 4503, loss = 2.15164620\n",
      "Iteration 4504, loss = 2.15105724\n",
      "Iteration 4505, loss = 2.15046844\n",
      "Iteration 4506, loss = 2.14987981\n",
      "Iteration 4507, loss = 2.14928789\n",
      "Iteration 4508, loss = 2.14868160\n",
      "Iteration 4509, loss = 2.14807262\n",
      "Iteration 4510, loss = 2.14746127\n",
      "Iteration 4511, loss = 2.14684780\n",
      "Iteration 4512, loss = 2.14623248\n",
      "Iteration 4513, loss = 2.14561550\n",
      "Iteration 4514, loss = 2.14499707\n",
      "Iteration 4515, loss = 2.14437736\n",
      "Iteration 4516, loss = 2.14375654\n",
      "Iteration 4517, loss = 2.14313473\n",
      "Iteration 4518, loss = 2.14251207\n",
      "Iteration 4519, loss = 2.14188868\n",
      "Iteration 4520, loss = 2.14126465\n",
      "Iteration 4521, loss = 2.14064007\n",
      "Iteration 4522, loss = 2.14001503\n",
      "Iteration 4523, loss = 2.13938961\n",
      "Iteration 4524, loss = 2.13876386\n",
      "Iteration 4525, loss = 2.13813786\n",
      "Iteration 4526, loss = 2.13751164\n",
      "Iteration 4527, loss = 2.13688527\n",
      "Iteration 4528, loss = 2.13625878\n",
      "Iteration 4529, loss = 2.13563220\n",
      "Iteration 4530, loss = 2.13500559\n",
      "Iteration 4531, loss = 2.13437896\n",
      "Iteration 4532, loss = 2.13375235\n",
      "Iteration 4533, loss = 2.13312578\n",
      "Iteration 4534, loss = 2.13249928\n",
      "Iteration 4535, loss = 2.13187286\n",
      "Iteration 4536, loss = 2.13124653\n",
      "Iteration 4537, loss = 2.13062033\n",
      "Iteration 4538, loss = 2.12999426\n",
      "Iteration 4539, loss = 2.12936833\n",
      "Iteration 4540, loss = 2.12874256\n",
      "Iteration 4541, loss = 2.12811696\n",
      "Iteration 4542, loss = 2.12749154\n",
      "Iteration 4543, loss = 2.12686630\n",
      "Iteration 4544, loss = 2.12624126\n",
      "Iteration 4545, loss = 2.12561641\n",
      "Iteration 4546, loss = 2.12499177\n",
      "Iteration 4547, loss = 2.12436734\n",
      "Iteration 4548, loss = 2.12374310\n",
      "Iteration 4549, loss = 2.12311907\n",
      "Iteration 4550, loss = 2.12249526\n",
      "Iteration 4551, loss = 2.12187167\n",
      "Iteration 4552, loss = 2.12124832\n",
      "Iteration 4553, loss = 2.12062519\n",
      "Iteration 4554, loss = 2.12000230\n",
      "Iteration 4555, loss = 2.11937965\n",
      "Iteration 4556, loss = 2.11875724\n",
      "Iteration 4557, loss = 2.11813507\n",
      "Iteration 4558, loss = 2.11751314\n",
      "Iteration 4559, loss = 2.11689145\n",
      "Iteration 4560, loss = 2.11627001\n",
      "Iteration 4561, loss = 2.11564882\n",
      "Iteration 4562, loss = 2.11502788\n",
      "Iteration 4563, loss = 2.11440718\n",
      "Iteration 4564, loss = 2.11378674\n",
      "Iteration 4565, loss = 2.11316654\n",
      "Iteration 4566, loss = 2.11254660\n",
      "Iteration 4567, loss = 2.11192691\n",
      "Iteration 4568, loss = 2.11130747\n",
      "Iteration 4569, loss = 2.11068828\n",
      "Iteration 4570, loss = 2.11006935\n",
      "Iteration 4571, loss = 2.10945067\n",
      "Iteration 4572, loss = 2.10883224\n",
      "Iteration 4573, loss = 2.10821407\n",
      "Iteration 4574, loss = 2.10759615\n",
      "Iteration 4575, loss = 2.10697848\n",
      "Iteration 4576, loss = 2.10636107\n",
      "Iteration 4577, loss = 2.10574391\n",
      "Iteration 4578, loss = 2.10512701\n",
      "Iteration 4579, loss = 2.10451037\n",
      "Iteration 4580, loss = 2.10389397\n",
      "Iteration 4581, loss = 2.10327783\n",
      "Iteration 4582, loss = 2.10266195\n",
      "Iteration 4583, loss = 2.10204632\n",
      "Iteration 4584, loss = 2.10143095\n",
      "Iteration 4585, loss = 2.10081583\n",
      "Iteration 4586, loss = 2.10020096\n",
      "Iteration 4587, loss = 2.09958635\n",
      "Iteration 4588, loss = 2.09897199\n",
      "Iteration 4589, loss = 2.09835788\n",
      "Iteration 4590, loss = 2.09774403\n",
      "Iteration 4591, loss = 2.09713044\n",
      "Iteration 4592, loss = 2.09651710\n",
      "Iteration 4593, loss = 2.09590401\n",
      "Iteration 4594, loss = 2.09529117\n",
      "Iteration 4595, loss = 2.09467859\n",
      "Iteration 4596, loss = 2.09406626\n",
      "Iteration 4597, loss = 2.09345419\n",
      "Iteration 4598, loss = 2.09284236\n",
      "Iteration 4599, loss = 2.09223080\n",
      "Iteration 4600, loss = 2.09161948\n",
      "Iteration 4601, loss = 2.09100842\n",
      "Iteration 4602, loss = 2.09039760\n",
      "Iteration 4603, loss = 2.08978705\n",
      "Iteration 4604, loss = 2.08917674\n",
      "Iteration 4605, loss = 2.08856668\n",
      "Iteration 4606, loss = 2.08795688\n",
      "Iteration 4607, loss = 2.08734733\n",
      "Iteration 4608, loss = 2.08673803\n",
      "Iteration 4609, loss = 2.08612899\n",
      "Iteration 4610, loss = 2.08552019\n",
      "Iteration 4611, loss = 2.08491164\n",
      "Iteration 4612, loss = 2.08430335\n",
      "Iteration 4613, loss = 2.08369531\n",
      "Iteration 4614, loss = 2.08308752\n",
      "Iteration 4615, loss = 2.08247997\n",
      "Iteration 4616, loss = 2.08187268\n",
      "Iteration 4617, loss = 2.08126564\n",
      "Iteration 4618, loss = 2.08065885\n",
      "Iteration 4619, loss = 2.08005231\n",
      "Iteration 4620, loss = 2.07944602\n",
      "Iteration 4621, loss = 2.07883998\n",
      "Iteration 4622, loss = 2.07823419\n",
      "Iteration 4623, loss = 2.07762864\n",
      "Iteration 4624, loss = 2.07702335\n",
      "Iteration 4625, loss = 2.07641831\n",
      "Iteration 4626, loss = 2.07581351\n",
      "Iteration 4627, loss = 2.07520896\n",
      "Iteration 4628, loss = 2.07460466\n",
      "Iteration 4629, loss = 2.07400061\n",
      "Iteration 4630, loss = 2.07339681\n",
      "Iteration 4631, loss = 2.07279326\n",
      "Iteration 4632, loss = 2.07218995\n",
      "Iteration 4633, loss = 2.07158689\n",
      "Iteration 4634, loss = 2.07098408\n",
      "Iteration 4635, loss = 2.07038152\n",
      "Iteration 4636, loss = 2.06977920\n",
      "Iteration 4637, loss = 2.06917713\n",
      "Iteration 4638, loss = 2.06857531\n",
      "Iteration 4639, loss = 2.06797373\n",
      "Iteration 4640, loss = 2.06737240\n",
      "Iteration 4641, loss = 2.06677131\n",
      "Iteration 4642, loss = 2.06617048\n",
      "Iteration 4643, loss = 2.06556988\n",
      "Iteration 4644, loss = 2.06496953\n",
      "Iteration 4645, loss = 2.06436943\n",
      "Iteration 4646, loss = 2.06376958\n",
      "Iteration 4647, loss = 2.06316996\n",
      "Iteration 4648, loss = 2.06257060\n",
      "Iteration 4649, loss = 2.06197147\n",
      "Iteration 4650, loss = 2.06137260\n",
      "Iteration 4651, loss = 2.06077396\n",
      "Iteration 4652, loss = 2.06017557\n",
      "Iteration 4653, loss = 2.05957743\n",
      "Iteration 4654, loss = 2.05897952\n",
      "Iteration 4655, loss = 2.05838186\n",
      "Iteration 4656, loss = 2.05778445\n",
      "Iteration 4657, loss = 2.05718727\n",
      "Iteration 4658, loss = 2.05659034\n",
      "Iteration 4659, loss = 2.05599365\n",
      "Iteration 4660, loss = 2.05539721\n",
      "Iteration 4661, loss = 2.05480100\n",
      "Iteration 4662, loss = 2.05420504\n",
      "Iteration 4663, loss = 2.05360932\n",
      "Iteration 4664, loss = 2.05301384\n",
      "Iteration 4665, loss = 2.05241860\n",
      "Iteration 4666, loss = 2.05182361\n",
      "Iteration 4667, loss = 2.05122885\n",
      "Iteration 4668, loss = 2.05063433\n",
      "Iteration 4669, loss = 2.05004006\n",
      "Iteration 4670, loss = 2.04944602\n",
      "Iteration 4671, loss = 2.04885223\n",
      "Iteration 4672, loss = 2.04825867\n",
      "Iteration 4673, loss = 2.04766535\n",
      "Iteration 4674, loss = 2.04707227\n",
      "Iteration 4675, loss = 2.04647943\n",
      "Iteration 4676, loss = 2.04588683\n",
      "Iteration 4677, loss = 2.04529447\n",
      "Iteration 4678, loss = 2.04470234\n",
      "Iteration 4679, loss = 2.04411046\n",
      "Iteration 4680, loss = 2.04351881\n",
      "Iteration 4681, loss = 2.04292740\n",
      "Iteration 4682, loss = 2.04233622\n",
      "Iteration 4683, loss = 2.04174528\n",
      "Iteration 4684, loss = 2.04115458\n",
      "Iteration 4685, loss = 2.04056411\n",
      "Iteration 4686, loss = 2.03997388\n",
      "Iteration 4687, loss = 2.03938389\n",
      "Iteration 4688, loss = 2.03879413\n",
      "Iteration 4689, loss = 2.03820461\n",
      "Iteration 4690, loss = 2.03761532\n",
      "Iteration 4691, loss = 2.03702627\n",
      "Iteration 4692, loss = 2.03643745\n",
      "Iteration 4693, loss = 2.03584886\n",
      "Iteration 4694, loss = 2.03526051\n",
      "Iteration 4695, loss = 2.03467239\n",
      "Iteration 4696, loss = 2.03408451\n",
      "Iteration 4697, loss = 2.03349685\n",
      "Iteration 4698, loss = 2.03290943\n",
      "Iteration 4699, loss = 2.03232225\n",
      "Iteration 4700, loss = 2.03173529\n",
      "Iteration 4701, loss = 2.03114857\n",
      "Iteration 4702, loss = 2.03056208\n",
      "Iteration 4703, loss = 2.02997582\n",
      "Iteration 4704, loss = 2.02938979\n",
      "Iteration 4705, loss = 2.02880399\n",
      "Iteration 4706, loss = 2.02821842\n",
      "Iteration 4707, loss = 2.02763308\n",
      "Iteration 4708, loss = 2.02704797\n",
      "Iteration 4709, loss = 2.02646309\n",
      "Iteration 4710, loss = 2.02587844\n",
      "Iteration 4711, loss = 2.02529402\n",
      "Iteration 4712, loss = 2.02470982\n",
      "Iteration 4713, loss = 2.02412586\n",
      "Iteration 4714, loss = 2.02354212\n",
      "Iteration 4715, loss = 2.02295861\n",
      "Iteration 4716, loss = 2.02237533\n",
      "Iteration 4717, loss = 2.02179227\n",
      "Iteration 4718, loss = 2.02120944\n",
      "Iteration 4719, loss = 2.02062684\n",
      "Iteration 4720, loss = 2.02004446\n",
      "Iteration 4721, loss = 2.01946231\n",
      "Iteration 4722, loss = 2.01888038\n",
      "Iteration 4723, loss = 2.01829868\n",
      "Iteration 4724, loss = 2.01771720\n",
      "Iteration 4725, loss = 2.01713595\n",
      "Iteration 4726, loss = 2.01655492\n",
      "Iteration 4727, loss = 2.01597411\n",
      "Iteration 4728, loss = 2.01539353\n",
      "Iteration 4729, loss = 2.01481317\n",
      "Iteration 4730, loss = 2.01423303\n",
      "Iteration 4731, loss = 2.01365311\n",
      "Iteration 4732, loss = 2.01307342\n",
      "Iteration 4733, loss = 2.01249395\n",
      "Iteration 4734, loss = 2.01191470\n",
      "Iteration 4735, loss = 2.01133567\n",
      "Iteration 4736, loss = 2.01075686\n",
      "Iteration 4737, loss = 2.01017827\n",
      "Iteration 4738, loss = 2.00959990\n",
      "Iteration 4739, loss = 2.00902175\n",
      "Iteration 4740, loss = 2.00844382\n",
      "Iteration 4741, loss = 2.00786610\n",
      "Iteration 4742, loss = 2.00728861\n",
      "Iteration 4743, loss = 2.00671133\n",
      "Iteration 4744, loss = 2.00613427\n",
      "Iteration 4745, loss = 2.00555743\n",
      "Iteration 4746, loss = 2.00498080\n",
      "Iteration 4747, loss = 2.00440440\n",
      "Iteration 4748, loss = 2.00382820\n",
      "Iteration 4749, loss = 2.00325223\n",
      "Iteration 4750, loss = 2.00267647\n",
      "Iteration 4751, loss = 2.00209940\n",
      "Iteration 4752, loss = 2.00152115\n",
      "Iteration 4753, loss = 2.00094186\n",
      "Iteration 4754, loss = 2.00036166\n",
      "Iteration 4755, loss = 1.99978067\n",
      "Iteration 4756, loss = 1.99919898\n",
      "Iteration 4757, loss = 1.99861669\n",
      "Iteration 4758, loss = 1.99803388\n",
      "Iteration 4759, loss = 1.99745064\n",
      "Iteration 4760, loss = 1.99686703\n",
      "Iteration 4761, loss = 1.99628312\n",
      "Iteration 4762, loss = 1.99569895\n",
      "Iteration 4763, loss = 1.99511458\n",
      "Iteration 4764, loss = 1.99453006\n",
      "Iteration 4765, loss = 1.99394543\n",
      "Iteration 4766, loss = 1.99336072\n",
      "Iteration 4767, loss = 1.99277597\n",
      "Iteration 4768, loss = 1.99219121\n",
      "Iteration 4769, loss = 1.99160646\n",
      "Iteration 4770, loss = 1.99102176\n",
      "Iteration 4771, loss = 1.99043712\n",
      "Iteration 4772, loss = 1.98985256\n",
      "Iteration 4773, loss = 1.98926810\n",
      "Iteration 4774, loss = 1.98868375\n",
      "Iteration 4775, loss = 1.98809954\n",
      "Iteration 4776, loss = 1.98751547\n",
      "Iteration 4777, loss = 1.98693155\n",
      "Iteration 4778, loss = 1.98634780\n",
      "Iteration 4779, loss = 1.98576422\n",
      "Iteration 4780, loss = 1.98518083\n",
      "Iteration 4781, loss = 1.98459762\n",
      "Iteration 4782, loss = 1.98401461\n",
      "Iteration 4783, loss = 1.98343180\n",
      "Iteration 4784, loss = 1.98284920\n",
      "Iteration 4785, loss = 1.98226681\n",
      "Iteration 4786, loss = 1.98168464\n",
      "Iteration 4787, loss = 1.98110268\n",
      "Iteration 4788, loss = 1.98052094\n",
      "Iteration 4789, loss = 1.97993952\n",
      "Iteration 4790, loss = 1.97935861\n",
      "Iteration 4791, loss = 1.97877794\n",
      "Iteration 4792, loss = 1.97819753\n",
      "Iteration 4793, loss = 1.97761736\n",
      "Iteration 4794, loss = 1.97703744\n",
      "Iteration 4795, loss = 1.97645776\n",
      "Iteration 4796, loss = 1.97587833\n",
      "Iteration 4797, loss = 1.97529915\n",
      "Iteration 4798, loss = 1.97472021\n",
      "Iteration 4799, loss = 1.97414151\n",
      "Iteration 4800, loss = 1.97356306\n",
      "Iteration 4801, loss = 1.97298485\n",
      "Iteration 4802, loss = 1.97240688\n",
      "Iteration 4803, loss = 1.97182916\n",
      "Iteration 4804, loss = 1.97125167\n",
      "Iteration 4805, loss = 1.97067443\n",
      "Iteration 4806, loss = 1.97009742\n",
      "Iteration 4807, loss = 1.96952065\n",
      "Iteration 4808, loss = 1.96894412\n",
      "Iteration 4809, loss = 1.96836782\n",
      "Iteration 4810, loss = 1.96779176\n",
      "Iteration 4811, loss = 1.96721593\n",
      "Iteration 4812, loss = 1.96664034\n",
      "Iteration 4813, loss = 1.96606497\n",
      "Iteration 4814, loss = 1.96548984\n",
      "Iteration 4815, loss = 1.96491494\n",
      "Iteration 4816, loss = 1.96434027\n",
      "Iteration 4817, loss = 1.96376583\n",
      "Iteration 4818, loss = 1.96320244\n",
      "Iteration 4819, loss = 1.96264882\n",
      "Iteration 4820, loss = 1.96209458\n",
      "Iteration 4821, loss = 1.96153981\n",
      "Iteration 4822, loss = 1.96098457\n",
      "Iteration 4823, loss = 1.96042893\n",
      "Iteration 4824, loss = 1.95987295\n",
      "Iteration 4825, loss = 1.95931668\n",
      "Iteration 4826, loss = 1.95876016\n",
      "Iteration 4827, loss = 1.95820345\n",
      "Iteration 4828, loss = 1.95764658\n",
      "Iteration 4829, loss = 1.95708959\n",
      "Iteration 4830, loss = 1.95653250\n",
      "Iteration 4831, loss = 1.95597534\n",
      "Iteration 4832, loss = 1.95541813\n",
      "Iteration 4833, loss = 1.95486091\n",
      "Iteration 4834, loss = 1.95430369\n",
      "Iteration 4835, loss = 1.95374648\n",
      "Iteration 4836, loss = 1.95318931\n",
      "Iteration 4837, loss = 1.95263218\n",
      "Iteration 4838, loss = 1.95207511\n",
      "Iteration 4839, loss = 1.95151812\n",
      "Iteration 4840, loss = 1.95096120\n",
      "Iteration 4841, loss = 1.95040438\n",
      "Iteration 4842, loss = 1.94984766\n",
      "Iteration 4843, loss = 1.94929104\n",
      "Iteration 4844, loss = 1.94873454\n",
      "Iteration 4845, loss = 1.94817815\n",
      "Iteration 4846, loss = 1.94762189\n",
      "Iteration 4847, loss = 1.94706576\n",
      "Iteration 4848, loss = 1.94650975\n",
      "Iteration 4849, loss = 1.94595389\n",
      "Iteration 4850, loss = 1.94539816\n",
      "Iteration 4851, loss = 1.94484258\n",
      "Iteration 4852, loss = 1.94428714\n",
      "Iteration 4853, loss = 1.94373184\n",
      "Iteration 4854, loss = 1.94317670\n",
      "Iteration 4855, loss = 1.94262170\n",
      "Iteration 4856, loss = 1.94206686\n",
      "Iteration 4857, loss = 1.94151216\n",
      "Iteration 4858, loss = 1.94095763\n",
      "Iteration 4859, loss = 1.94040324\n",
      "Iteration 4860, loss = 1.93984901\n",
      "Iteration 4861, loss = 1.93929494\n",
      "Iteration 4862, loss = 1.93874102\n",
      "Iteration 4863, loss = 1.93818725\n",
      "Iteration 4864, loss = 1.93763365\n",
      "Iteration 4865, loss = 1.93708020\n",
      "Iteration 4866, loss = 1.93652691\n",
      "Iteration 4867, loss = 1.93597377\n",
      "Iteration 4868, loss = 1.93542755\n",
      "Iteration 4869, loss = 1.93488778\n",
      "Iteration 4870, loss = 1.93434849\n",
      "Iteration 4871, loss = 1.93380964\n",
      "Iteration 4872, loss = 1.93327121\n",
      "Iteration 4873, loss = 1.93273315\n",
      "Iteration 4874, loss = 1.93219544\n",
      "Iteration 4875, loss = 1.93165807\n",
      "Iteration 4876, loss = 1.93112099\n",
      "Iteration 4877, loss = 1.93058420\n",
      "Iteration 4878, loss = 1.93004768\n",
      "Iteration 4879, loss = 1.92951141\n",
      "Iteration 4880, loss = 1.92897538\n",
      "Iteration 4881, loss = 1.92843956\n",
      "Iteration 4882, loss = 1.92790396\n",
      "Iteration 4883, loss = 1.92736855\n",
      "Iteration 4884, loss = 1.92683333\n",
      "Iteration 4885, loss = 1.92629829\n",
      "Iteration 4886, loss = 1.92576343\n",
      "Iteration 4887, loss = 1.92522873\n",
      "Iteration 4888, loss = 1.92469418\n",
      "Iteration 4889, loss = 1.92415979\n",
      "Iteration 4890, loss = 1.92362555\n",
      "Iteration 4891, loss = 1.92309145\n",
      "Iteration 4892, loss = 1.92255748\n",
      "Iteration 4893, loss = 1.92202365\n",
      "Iteration 4894, loss = 1.92148995\n",
      "Iteration 4895, loss = 1.92095638\n",
      "Iteration 4896, loss = 1.92042293\n",
      "Iteration 4897, loss = 1.91988960\n",
      "Iteration 4898, loss = 1.91935639\n",
      "Iteration 4899, loss = 1.91882330\n",
      "Iteration 4900, loss = 1.91829032\n",
      "Iteration 4901, loss = 1.91775746\n",
      "Iteration 4902, loss = 1.91722471\n",
      "Iteration 4903, loss = 1.91669207\n",
      "Iteration 4904, loss = 1.91615954\n",
      "Iteration 4905, loss = 1.91562712\n",
      "Iteration 4906, loss = 1.91509480\n",
      "Iteration 4907, loss = 1.91456259\n",
      "Iteration 4908, loss = 1.91403049\n",
      "Iteration 4909, loss = 1.91349849\n",
      "Iteration 4910, loss = 1.91296660\n",
      "Iteration 4911, loss = 1.91243481\n",
      "Iteration 4912, loss = 1.91190312\n",
      "Iteration 4913, loss = 1.91137154\n",
      "Iteration 4914, loss = 1.91084005\n",
      "Iteration 4915, loss = 1.91030867\n",
      "Iteration 4916, loss = 1.90977740\n",
      "Iteration 4917, loss = 1.90924622\n",
      "Iteration 4918, loss = 1.90871514\n",
      "Iteration 4919, loss = 1.90818417\n",
      "Iteration 4920, loss = 1.90765329\n",
      "Iteration 4921, loss = 1.90712252\n",
      "Iteration 4922, loss = 1.90659184\n",
      "Iteration 4923, loss = 1.90606127\n",
      "Iteration 4924, loss = 1.90553079\n",
      "Iteration 4925, loss = 1.90500042\n",
      "Iteration 4926, loss = 1.90447015\n",
      "Iteration 4927, loss = 1.90393997\n",
      "Iteration 4928, loss = 1.90340990\n",
      "Iteration 4929, loss = 1.90287992\n",
      "Iteration 4930, loss = 1.90235004\n",
      "Iteration 4931, loss = 1.90182027\n",
      "Iteration 4932, loss = 1.90129059\n",
      "Iteration 4933, loss = 1.90076101\n",
      "Iteration 4934, loss = 1.90023153\n",
      "Iteration 4935, loss = 1.89970215\n",
      "Iteration 4936, loss = 1.89917287\n",
      "Iteration 4937, loss = 1.89864369\n",
      "Iteration 4938, loss = 1.89811461\n",
      "Iteration 4939, loss = 1.89758563\n",
      "Iteration 4940, loss = 1.89705675\n",
      "Iteration 4941, loss = 1.89652796\n",
      "Iteration 4942, loss = 1.89599928\n",
      "Iteration 4943, loss = 1.89547069\n",
      "Iteration 4944, loss = 1.89494221\n",
      "Iteration 4945, loss = 1.89441382\n",
      "Iteration 4946, loss = 1.89388553\n",
      "Iteration 4947, loss = 1.89335735\n",
      "Iteration 4948, loss = 1.89282926\n",
      "Iteration 4949, loss = 1.89230127\n",
      "Iteration 4950, loss = 1.89177338\n",
      "Iteration 4951, loss = 1.89124559\n",
      "Iteration 4952, loss = 1.89071789\n",
      "Iteration 4953, loss = 1.89019030\n",
      "Iteration 4954, loss = 1.88966281\n",
      "Iteration 4955, loss = 1.88913542\n",
      "Iteration 4956, loss = 1.88860812\n",
      "Iteration 4957, loss = 1.88808093\n",
      "Iteration 4958, loss = 1.88755383\n",
      "Iteration 4959, loss = 1.88702683\n",
      "Iteration 4960, loss = 1.88649994\n",
      "Iteration 4961, loss = 1.88597314\n",
      "Iteration 4962, loss = 1.88544644\n",
      "Iteration 4963, loss = 1.88491984\n",
      "Iteration 4964, loss = 1.88439334\n",
      "Iteration 4965, loss = 1.88386694\n",
      "Iteration 4966, loss = 1.88334064\n",
      "Iteration 4967, loss = 1.88281444\n",
      "Iteration 4968, loss = 1.88228833\n",
      "Iteration 4969, loss = 1.88176233\n",
      "Iteration 4970, loss = 1.88123643\n",
      "Iteration 4971, loss = 1.88071062\n",
      "Iteration 4972, loss = 1.88018492\n",
      "Iteration 4973, loss = 1.87965931\n",
      "Iteration 4974, loss = 1.87913380\n",
      "Iteration 4975, loss = 1.87860839\n",
      "Iteration 4976, loss = 1.87808309\n",
      "Iteration 4977, loss = 1.87755788\n",
      "Iteration 4978, loss = 1.87703277\n",
      "Iteration 4979, loss = 1.87650776\n",
      "Iteration 4980, loss = 1.87598285\n",
      "Iteration 4981, loss = 1.87545803\n",
      "Iteration 4982, loss = 1.87493332\n",
      "Iteration 4983, loss = 1.87440871\n",
      "Iteration 4984, loss = 1.87388419\n",
      "Iteration 4985, loss = 1.87335978\n",
      "Iteration 4986, loss = 1.87283546\n",
      "Iteration 4987, loss = 1.87231125\n",
      "Iteration 4988, loss = 1.87178713\n",
      "Iteration 4989, loss = 1.87126311\n",
      "Iteration 4990, loss = 1.87073919\n",
      "Iteration 4991, loss = 1.87021537\n",
      "Iteration 4992, loss = 1.86969165\n",
      "Iteration 4993, loss = 1.86916803\n",
      "Iteration 4994, loss = 1.86864451\n",
      "Iteration 4995, loss = 1.86812108\n",
      "Iteration 4996, loss = 1.86759776\n",
      "Iteration 4997, loss = 1.86707453\n",
      "Iteration 4998, loss = 1.86655141\n",
      "Iteration 4999, loss = 1.86602838\n",
      "Iteration 5000, loss = 1.86550545\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (5000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-25 {\n",
       "  /* Definition of color scheme common for light and dark mode */\n",
       "  --sklearn-color-text: black;\n",
       "  --sklearn-color-line: gray;\n",
       "  /* Definition of color scheme for unfitted estimators */\n",
       "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
       "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
       "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
       "  --sklearn-color-unfitted-level-3: chocolate;\n",
       "  /* Definition of color scheme for fitted estimators */\n",
       "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
       "  --sklearn-color-fitted-level-1: #d4ebff;\n",
       "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
       "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
       "\n",
       "  /* Specific color for light theme */\n",
       "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
       "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-icon: #696969;\n",
       "\n",
       "  @media (prefers-color-scheme: dark) {\n",
       "    /* Redefinition of color scheme for dark theme */\n",
       "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
       "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-icon: #878787;\n",
       "  }\n",
       "}\n",
       "\n",
       "#sk-container-id-25 {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "#sk-container-id-25 pre {\n",
       "  padding: 0;\n",
       "}\n",
       "\n",
       "#sk-container-id-25 input.sk-hidden--visually {\n",
       "  border: 0;\n",
       "  clip: rect(1px 1px 1px 1px);\n",
       "  clip: rect(1px, 1px, 1px, 1px);\n",
       "  height: 1px;\n",
       "  margin: -1px;\n",
       "  overflow: hidden;\n",
       "  padding: 0;\n",
       "  position: absolute;\n",
       "  width: 1px;\n",
       "}\n",
       "\n",
       "#sk-container-id-25 div.sk-dashed-wrapped {\n",
       "  border: 1px dashed var(--sklearn-color-line);\n",
       "  margin: 0 0.4em 0.5em 0.4em;\n",
       "  box-sizing: border-box;\n",
       "  padding-bottom: 0.4em;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "#sk-container-id-25 div.sk-container {\n",
       "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
       "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
       "     so we also need the `!important` here to be able to override the\n",
       "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
       "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
       "  display: inline-block !important;\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-25 div.sk-text-repr-fallback {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       "div.sk-parallel-item,\n",
       "div.sk-serial,\n",
       "div.sk-item {\n",
       "  /* draw centered vertical line to link estimators */\n",
       "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
       "  background-size: 2px 100%;\n",
       "  background-repeat: no-repeat;\n",
       "  background-position: center center;\n",
       "}\n",
       "\n",
       "/* Parallel-specific style estimator block */\n",
       "\n",
       "#sk-container-id-25 div.sk-parallel-item::after {\n",
       "  content: \"\";\n",
       "  width: 100%;\n",
       "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
       "  flex-grow: 1;\n",
       "}\n",
       "\n",
       "#sk-container-id-25 div.sk-parallel {\n",
       "  display: flex;\n",
       "  align-items: stretch;\n",
       "  justify-content: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-25 div.sk-parallel-item {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       "\n",
       "#sk-container-id-25 div.sk-parallel-item:first-child::after {\n",
       "  align-self: flex-end;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-25 div.sk-parallel-item:last-child::after {\n",
       "  align-self: flex-start;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-25 div.sk-parallel-item:only-child::after {\n",
       "  width: 0;\n",
       "}\n",
       "\n",
       "/* Serial-specific style estimator block */\n",
       "\n",
       "#sk-container-id-25 div.sk-serial {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  align-items: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  padding-right: 1em;\n",
       "  padding-left: 1em;\n",
       "}\n",
       "\n",
       "\n",
       "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
       "clickable and can be expanded/collapsed.\n",
       "- Pipeline and ColumnTransformer use this feature and define the default style\n",
       "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
       "*/\n",
       "\n",
       "/* Pipeline and ColumnTransformer style (default) */\n",
       "\n",
       "#sk-container-id-25 div.sk-toggleable {\n",
       "  /* Default theme specific background. It is overwritten whether we have a\n",
       "  specific estimator or a Pipeline/ColumnTransformer */\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "/* Toggleable label */\n",
       "#sk-container-id-25 label.sk-toggleable__label {\n",
       "  cursor: pointer;\n",
       "  display: block;\n",
       "  width: 100%;\n",
       "  margin-bottom: 0;\n",
       "  padding: 0.5em;\n",
       "  box-sizing: border-box;\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "#sk-container-id-25 label.sk-toggleable__label-arrow:before {\n",
       "  /* Arrow on the left of the label */\n",
       "  content: \"▸\";\n",
       "  float: left;\n",
       "  margin-right: 0.25em;\n",
       "  color: var(--sklearn-color-icon);\n",
       "}\n",
       "\n",
       "#sk-container-id-25 label.sk-toggleable__label-arrow:hover:before {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "/* Toggleable content - dropdown */\n",
       "\n",
       "#sk-container-id-25 div.sk-toggleable__content {\n",
       "  max-height: 0;\n",
       "  max-width: 0;\n",
       "  overflow: hidden;\n",
       "  text-align: left;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-25 div.sk-toggleable__content.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-25 div.sk-toggleable__content pre {\n",
       "  margin: 0.2em;\n",
       "  border-radius: 0.25em;\n",
       "  color: var(--sklearn-color-text);\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-25 div.sk-toggleable__content.fitted pre {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-25 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
       "  /* Expand drop-down */\n",
       "  max-height: 200px;\n",
       "  max-width: 100%;\n",
       "  overflow: auto;\n",
       "}\n",
       "\n",
       "#sk-container-id-25 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
       "  content: \"▾\";\n",
       "}\n",
       "\n",
       "/* Pipeline/ColumnTransformer-specific style */\n",
       "\n",
       "#sk-container-id-25 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-25 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator-specific style */\n",
       "\n",
       "/* Colorize estimator box */\n",
       "#sk-container-id-25 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-25 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-25 div.sk-label label.sk-toggleable__label,\n",
       "#sk-container-id-25 div.sk-label label {\n",
       "  /* The background is the default theme color */\n",
       "  color: var(--sklearn-color-text-on-default-background);\n",
       "}\n",
       "\n",
       "/* On hover, darken the color of the background */\n",
       "#sk-container-id-25 div.sk-label:hover label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "/* Label box, darken color on hover, fitted */\n",
       "#sk-container-id-25 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator label */\n",
       "\n",
       "#sk-container-id-25 div.sk-label label {\n",
       "  font-family: monospace;\n",
       "  font-weight: bold;\n",
       "  display: inline-block;\n",
       "  line-height: 1.2em;\n",
       "}\n",
       "\n",
       "#sk-container-id-25 div.sk-label-container {\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "/* Estimator-specific */\n",
       "#sk-container-id-25 div.sk-estimator {\n",
       "  font-family: monospace;\n",
       "  border: 1px dotted var(--sklearn-color-border-box);\n",
       "  border-radius: 0.25em;\n",
       "  box-sizing: border-box;\n",
       "  margin-bottom: 0.5em;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-25 div.sk-estimator.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "/* on hover */\n",
       "#sk-container-id-25 div.sk-estimator:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-25 div.sk-estimator.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
       "\n",
       "/* Common style for \"i\" and \"?\" */\n",
       "\n",
       ".sk-estimator-doc-link,\n",
       "a:link.sk-estimator-doc-link,\n",
       "a:visited.sk-estimator-doc-link {\n",
       "  float: right;\n",
       "  font-size: smaller;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1em;\n",
       "  height: 1em;\n",
       "  width: 1em;\n",
       "  text-decoration: none !important;\n",
       "  margin-left: 1ex;\n",
       "  /* unfitted */\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted,\n",
       "a:link.sk-estimator-doc-link.fitted,\n",
       "a:visited.sk-estimator-doc-link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "/* Span, style for the box shown on hovering the info icon */\n",
       ".sk-estimator-doc-link span {\n",
       "  display: none;\n",
       "  z-index: 9999;\n",
       "  position: relative;\n",
       "  font-weight: normal;\n",
       "  right: .2ex;\n",
       "  padding: .5ex;\n",
       "  margin: .5ex;\n",
       "  width: min-content;\n",
       "  min-width: 20ex;\n",
       "  max-width: 50ex;\n",
       "  color: var(--sklearn-color-text);\n",
       "  box-shadow: 2pt 2pt 4pt #999;\n",
       "  /* unfitted */\n",
       "  background: var(--sklearn-color-unfitted-level-0);\n",
       "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted span {\n",
       "  /* fitted */\n",
       "  background: var(--sklearn-color-fitted-level-0);\n",
       "  border: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link:hover span {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
       "\n",
       "#sk-container-id-25 a.estimator_doc_link {\n",
       "  float: right;\n",
       "  font-size: 1rem;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1rem;\n",
       "  height: 1rem;\n",
       "  width: 1rem;\n",
       "  text-decoration: none;\n",
       "  /* unfitted */\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "}\n",
       "\n",
       "#sk-container-id-25 a.estimator_doc_link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "#sk-container-id-25 a.estimator_doc_link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "#sk-container-id-25 a.estimator_doc_link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "</style><div id=\"sk-container-id-25\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>MLPClassifier(hidden_layer_sizes=(50,), learning_rate_init=1e-08, max_iter=5000,\n",
       "              random_state=1, verbose=10)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-25\" type=\"checkbox\" checked><label for=\"sk-estimator-id-25\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;&nbsp;MLPClassifier<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.4/modules/generated/sklearn.neural_network.MLPClassifier.html\">?<span>Documentation for MLPClassifier</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></label><div class=\"sk-toggleable__content fitted\"><pre>MLPClassifier(hidden_layer_sizes=(50,), learning_rate_init=1e-08, max_iter=5000,\n",
       "              random_state=1, verbose=10)</pre></div> </div></div></div></div>"
      ],
      "text/plain": [
       "MLPClassifier(hidden_layer_sizes=(50,), learning_rate_init=1e-08, max_iter=5000,\n",
       "              random_state=1, verbose=10)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N = len(noms)\n",
    "\n",
    "# Les images doivent être aplatis pour l'entraînement\n",
    "visages = visages.reshape(N, -1)\n",
    "\n",
    "# Initialisation et entraînement du réseau de neurones\n",
    "mlp = MLPClassifier(hidden_layer_sizes=(50,), max_iter=5000, alpha=1e-4,\n",
    "                    solver='adam', verbose=10, random_state=1,\n",
    "                    learning_rate_init=0.00000001)\n",
    "\n",
    "mlp.fit(visages, noms)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5713d005",
   "metadata": {},
   "source": [
    "## Exécution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "03952133",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mLe noyau s’est bloqué lors de l’exécution du code dans une cellule active ou une cellule précédente. \n",
      "\u001b[1;31mVeuillez vérifier le code dans la ou les cellules pour identifier une cause possible de l’échec. \n",
      "\u001b[1;31mCliquez <a href='https://aka.ms/vscodeJupyterKernelCrash'>ici</a> pour plus d’informations. \n",
      "\u001b[1;31mPour plus d’informations, consultez Jupyter <a href='command:jupyter.viewOutput'>log</a>."
     ]
    }
   ],
   "source": [
    "cascade_visage = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "\n",
    "camera = cv2.VideoCapture(0) # 0 pour 'built-in' caméra, 1 pour caméra externe\n",
    "\n",
    "while True:\n",
    "    ret, trame = camera.read()\n",
    "    if ret:\n",
    "        gris = cv2.cvtColor(trame, cv2.COLOR_BGR2GRAY)\n",
    "        coordonnees_visage = cascade_visage.detectMultiScale(gris, 1.3, 5)\n",
    "\n",
    "        for (x, y, l, h) in coordonnees_visage:\n",
    "            visage = trame[y:y + h, x:x + l, :]\n",
    "            visage_redimensionne = cv2.resize(visage, (50, 50)).flatten().reshape(1, -1)\n",
    "            \n",
    "            texte = mlp.predict(visage_redimensionne)\n",
    "            \n",
    "            cv2.putText(trame, texte[0], (x, y-10), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 0), 2)\n",
    "            cv2.rectangle(trame, (x, y), (x + l, y + h), (0, 0, 255), 2)\n",
    "\n",
    "        cv2.imshow('Reconnaissance faciale en temps réel', trame)\n",
    "        \n",
    "        if cv2.waitKey(1) == 27: # Touche ESC pour quitter\n",
    "            break\n",
    "    else:\n",
    "        print(\"Erreur\")\n",
    "        break\n",
    "\n",
    "cv2.destroyAllWindows()\n",
    "camera.release()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d9dfd57",
   "metadata": {},
   "source": [
    "**Exemple d'exécution du système...**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c57dbfe2",
   "metadata": {},
   "source": [
    "<img src=\"moi.png\" width=\"400\" height=\"auto\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c00fbeb1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
